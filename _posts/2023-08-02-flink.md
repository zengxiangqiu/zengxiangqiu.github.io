---
title: "flink"
date:  2023-08-02 11:14:54 +0800
categories: [Data]
tags: [flink]
---



vue save config


jar get config and run ,flink sql api to build tables  end to end insert.

1. starrock
2. datastream source

restart k8s container ,bash kubectl command to restart

ETL 作业通常会周期性地触发，将数据从事务型数据库拷贝到分析型数据库或数据仓库。

数据管道是以持续流模式运行，而非周期性触发。


1. mysql table 如何 作为无界的输入源

  实现DynamicTable相关接口,比如[mysql cdc](https://github.com/ververica/flink-cdc-connectors/blob/54df7e93f0e688ac71ac2e5b6d9bea1e118fe42c/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/table/MySqlTableSource.java#L23)

2. 原生 flink connector jdbc 不支持 delete 操作,没有实现 upportsRowLevelDelete 接口,要cdc 才可以

3. 排除delete的情况,flink jdbc (mysql -> mssql) ,通过bull定时下发http request, select * from source where lastmoddate > 时间间隔 ,实现简单增量修改


[k8s install flink cluster ](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start/)

[DataStream Source](https://ververica.github.io/flink-cdc-connectors/master/content/connectors/mysql-cdc%28ZH%29.html#datastream-source)

[flink 中文文档](https://nightlies.apache.org/flink/flink-docs-release-1.17/zh/docs/dev/table/overview/)

`SHOW VARIABLES LIKE 'log_bin';`


[REST API ](https://nightlies.apache.org/flink/flink-docs-master/zh/docs/ops/rest_api/)





