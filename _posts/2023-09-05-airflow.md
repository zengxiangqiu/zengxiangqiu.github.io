---
title: "airflow"
date:  2023-09-05 11:17:04 +0800
categories: [Operation]
tags: [job]
---


set up database

https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#

## helm install

```sh
$ export NAMESPACE=airflow
$ kubectl create namespace $NAMESPACE

$ export RELEASE_NAME=airflow
$ helm install airflow apache-airflow/airflow --namespace airflow -f override.yaml

$ helm upgrade --install airflow apache-airflow/airflow --namespace airflow -f override.yaml
```

配置文件 override.yaml

```yaml
# Default airflow repository -- overridden by all the specific images below
defaultAirflowRepository: registry.xxx.com.cn/airflow-xxx

# Default airflow tag to deploy
defaultAirflowTag: "2.7.0"

postgresql:
  enabled: false
dags:
  gitSync:
    enabled: true
    repo: git@code.xxx.com.cn:xiangqiu.zeng/airflow-pags.git
    branch: main
    subPath: ""
    sshKeySecret: airflow-ssh-secret
extraSecrets:
  airflow-ssh-secret:
    data: |
      gitSshKey: ''
data:
  # Otherwise pass connection values in
  metadataConnection:
    user: airflow
    pass: airflow
    protocol: mysql
    host: 192.168.1.xxx
    port: 30673
    db: xxx_airflow
    sslmode: disable

images:
  gitSync:
    repository: registry.xxx.com.cn/git-sync
    tag: v3.6.3
    pullPolicy: IfNotPresent

workers:
  extraVolumes:
    - name: storage
      persistentVolumeClaim:
        claimName: nfs-airflow
  extraVolumeMounts:
    - name: storage
      mountPath: /opt/airflow/temp

webserverSecretKey: webserver-secret-key
webserverSecretKeySecretName: my-webserver-secret
```

### airflow image

```dockerfile
FROM apache/airflow:2.7.0
RUN pip config set global.index-url http://mirrors.aliyun.com/pypi/simple
RUN pip config set install.trusted-host mirrors.aliyun.com
RUN pip install "apache-airflow==${AIRFLOW_VERSION}" --no-cache-dir apache-airflow-providers-microsoft-mssql==3.4.2
```

### webserverSecretKey

[python online](https://www.online-python.com/#google_vignette)

```sh
$ python3 -c 'import secrets; print(secrets.token_hex(16))'
$ kubectl create secret generic my-webserver-secret --from-literal="webserver-secret-key='xxxxx')"
```

### git-sync

registry.k8s.io 无法访问，通过[hub-mirror](https://github.com/togettoyou/hub-mirror)提供issue触发action自动拉取image

```json
{
  "hub-mirror": [
    "registry.k8s.io/git-sync/git-sync:v3.6.3"
  ]
}
```

```sh
$ docker pull togettoyou/registry.k8s.io.git-sync.git-sync:v3.6.3
$ ssh-keygen -t rsa -b 4096 -C "xiangqiu.zeng@lesaunda.com.cn"
$ base64 airflow -w 0 > temp.txt #gitSshKey
```

访问 gitlab->设置个人资料->ssh密钥，粘贴复制 pk



###  worker volume

task 中将数据写入 `/opt/airflow/dags/temp` ，提示 readonly system

申请 nfs pv 再挂载

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-airflow
  namespace: airflow
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 100Gi
  storageClassName: nfs-gzk8s-storage
```

###  metadata database

```sql
CREATE DATABASE xxx_airflow CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
CREATE USER 'airflow' IDENTIFIED BY 'airflow';
GRANT ALL PRIVILEGES ON xxx_airflow.* TO 'airflow';
```

停用postgresql，避免创建container

```yaml
postgresql:
  enabled: false
```

### ingress

在dag中trigger，跳转location 没有带31198端口，待查

workaround

`kubectl  expose deploy airflow-webserver  --type=NodePort --name=airflow-webserver-external -n airflow`

```yaml
ingress:
  web:
    enabled: true
    ingressClassName: nginx
    path: "/airflow"
    pathType: Prefix


config:
  webserver:
    base_url: http://192.168.1.xxx:31198/airflow
```



##  provider

主要分hook和 operator ，operator 默认带入Dag default_args 存在的key-value，比如 mysql_conn_id，mssql_conn_id

hook get_records return tuple[],其中 decimal, timestamp 需要注意

decimal `CAST(FLOOR(INV.qty)+FLOOR(INV.qty_tran_out) AS SIGNED) `

timestamp `MAX( DATE_FORMAT(INV.modify_time,'%Y-%m-%d %H:%i:%s')) as 'f_date'`

###  mssql

> You can also use an external file to execute the SQL commands. Script folder must be at the same level as DAG.py file. This way you can easily maintain the SQL queries separated from the code.

sql 文件 与 py 文件放在同一folder

```python
  update_data = MsSqlOperator(
      task_id="update_data", database="xxxx",sql="inventory_xxxx_update.sql", dag=dag
  )
```

[templates-ref](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html)

upgrade airflow v2.7.3 之后 MsSqlOperator connect 报 `Adaptive server connection failed (DB-Lib error message 20002, severity 9)`

operator 使用 [pymssql](https://www.pymssql.org/en/stable/index.html) + [Freetds](https://www.freetds.org/userguide/index.html) + OBDC 访问mssql

freetds 配置文件 `/etc/freetds/freetds.conf` ，参考 [官方配置](https://www.freetds.org/userguide/freetdsconf.html) , github/stackoverflow 讨论是 tds version = 7.0 问题，进入worker 容器， 更改`export TDSVER=7.0`，未通过; freetds 自带命令行工具 `tsql -H [host] -p 1433 -o v -U [username] -D [database] -v `， freetds 配置 `tsql -C`

```
Compile-time settings (established with the "configure" script)
                            Version: freetds v1.2.3
             freetds.conf directory: /etc/freetds
     MS db-lib source compatibility: no
        Sybase binary compatibility: yes
                      Thread safety: yes
                      iconv library: yes
                        TDS version: auto
                              iODBC: no
                           unixodbc: yes
              SSPI "trusted" logins: no
                           Kerberos: yes
                            OpenSSL: no
                             GnuTLS: yes
                               MARS: yes
```

tsql 连接没有问题， 挂载 `/temp/test.py`,报同样错误

```python

import pymssql

conn = pymssql.connect(host="", username='', password='', database="")
```

配置 freetds 日志输出 `export TDSDUMP=/tmp/freetds.log`

比较tsql 和 test.py 输出，发现多了openssl, 测试库无配置证书，2.2.10的pymssql,--with-openssl=yes 所以无法连接，尝试将pymssql降版本 2.2.10 -> 2.2.8 ,`pip install pymssql==2.2.8`，测试通过



```sh
Starting log file for FreeTDS 1.4.3
login.c:1281:detected crypt flag 0
tls.c:1019:setting default openssl cipher to:HIGH:!SSLv2:!aNULL:-DH
....

```

```sh
log.c:168:Starting log file for FreeTDS 1.2.3
login.c:1281:detected crypt flag 0
```




##  jinja

[jinja2 if ](https://jinja.palletsprojects.com/en/2.10.x/templates/#if)

`-%}` 不换行
```sql
{% if modify_time -%}
WHERE INV.modify_time >= '{{ modify_time }}'
{% endif %}
```


[templating](https://airflow.apache.org/docs/apache-airflow/2.7.0/core-concepts/operators.html#concepts-jinja-templating)

mysql sql render

[Python: best practice and securest way to connect to MySQL and execute queries](https://stackoverflow.com/questions/7929364/python-best-practice-and-securest-way-to-connect-to-mysql-and-execute-queries/7929842#7929842)

传参format格式

[PEP 249 – Python Database API Specification v2.0](https://peps.python.org/pep-0249/)





##  variable

[Variable](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/models/variable.html#Variable)


##  helm install airflow

[Helm Chart for Apache Airflow](https://airflow.apache.org/docs/helm-chart/stable/index.html)


###  创建metadata数据库

```sql
CREATE DATABASE lesaunda_airflow CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
CREATE USER 'airflow' IDENTIFIED BY 'airflow';
GRANT ALL PRIVILEGES ON lesaunda_airflow.* TO 'airflow';
```

### ingress

```yaml
config:
  webserver:
    base_url: http://localhost:8080/airflow-dev
```


> op_kwargs (Optional[Mapping[str, Any]]): This is the dictionary we use to pass in user-defined key-value pairs to our python callable function
> PythonOperator 可以传参


> templates_dict (Optional[Dict[str, Any]]): This is the dictionary that airflow uses to pass the default variables as key-value pairs to our python callable function. E.g., the 'task_instance' or 'run_id' are a couple of variables that airflow provides us by default. Full list of default variables is here.
> task 可以通过`templates_dict`传参


> So technically, we just need to pass the additional key-value pairs we need through the op_kwargs parameter while using the PythonOperator. I think that's the intended design choice, but we could still use either to pass in our key-value pairs.
> Airflow takes care of the rest. The execute method in the PythonOperator merges the kwargs and templates_dict into a single dictionary, which we later unpack in the python_callable function, generally using either **kwargs or **context. Airflow code for this is here.


> Airflow context is only accessible from tasks in runtime, and TaskGroup is not a task, it's just a collection of tasks used to group the tasks in the UI.
> TaskGroup 只是显示上的一组task，不可以在运行时访问


1. baseOperator override 会重载并创建 new task
2. 从其他py file import func 再 decorate @task
3. operator.output 可以作为@task func 的参数
4. A DAG run is usually scheduled after its associated data interval has ended, to ensure the run is able to collect all the data within the time period. In other words, a run covering the data period of 2020-01-01 generally does not start to run until 2020-01-01 has ended, i.e. after 2020-01-02 00:00:00. 运行在间隔的结束，因为间隔期间会收集数据


[kubernete ingress workaround](https://github.com/apache/airflow/issues/28362)

[Templates reference](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html)

[FreeTDS](https://pymssql.readthedocs.io/en/latest/freetds.html)

[Stable API](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/delete_dag_run)
