[ { "title": "seatunnel", "url": "/posts/seatunnel/", "categories": "cdc", "tags": "cdc, myql", "date": "2024-08-06 09:54:30 +0800", "snippet": "seatunnelseatunnelseatunnel-docsourceexactly-once 如果数据源中的每条数据仅由源向下游发送一次，我们认为该source connector支持精确一次（exactly-once）。sinkexactly-once 当任意一条数据流入分布式系统时，如果系统在整个处理过程中仅准确处理任意一条数据一次，且处理结果正确，则认为系统满足精确一次一致性。seatunnel-webseatunnel-webcd seatunnel-webbuild.sh codeBugs seatunnel-ui$npm run build:prod 提示 $route 不存在,需修改 \\seatunnel-ui\\node_modules\\vue-router\\dist\\vue-router.d.tsvue3 style- declare module &#39;@vue&#39;+ declare module &#39;@vue/runtime-core&#39; download_datasource.sh not foundgit clone into window OS, file end of LRLF, can not be execute in linux.sed -i &#39;s/\\r//&#39; apache-seatunnel-web-1.0.0-SNAPSHOT/bin/seatunnel-backend-daemon.shsed -i &#39;s/\\r//&#39; apache-seatunnel-web-1.0.0-SNAPSHOT/bin/download_datasource.sh 2.3.3 java.util.LinkedHashMap cannot be cast to java.util.ArrayListhad been fixed in 2.3.5 2.3.5 依然需要将重新编译修改后的seatunnel-config-base.jar放入libs文件夹workaroundseatunnel-config-base&amp;lt;!-- just add this line to remove the inner class --&amp;gt;&amp;lt;exclude&amp;gt;com/typesafe/config/impl/ConfigParser$ParseContext.class&amp;lt;/exclude&amp;gt;env { parallelism = 1 job.mode = &quot;STREAMING&quot; checkpoint.interval = 10000}source { MySQL-CDC { base-url = &quot;jdbc:mysql://xxxx:3306/e3plus_stock_integrate&quot; username = &quot;root&quot; password = &quot;xxx&quot; table-names = [&quot;e3plus_stock_integrate.stk_stock_logic_account&quot;] }}sink { jdbc { driver = &quot;com.mysql.cj.jdbc.Driver&quot; url = &quot;jdbc:mysql://xxxx:3306/e3plus_stock_integrate?useUnicode=true&amp;amp;characterEncoding=UTF-8&amp;amp;rewriteBatchedStatements=true&quot; user = &quot;root&quot; password = &quot;xxx&quot; # generate_sink_sql = true # You need to configure both database and table database = e3plus_stock_integrate table = stk_stock_logic_account primary_keys = [&quot;id&quot;] # field_ide = UPPERCASE schema_save_mode = &quot;CREATE_SCHEMA_WHEN_NOT_EXIST&quot; data_save_mode=&quot;APPEND_DATA&quot; }}kubectl -n mysql exec xx -c mysql -- mysqldump --disable-keys --set-gtid-purged=OFF --single-transaction --no-data -h127.0.1.0 -uroot -p xx &amp;gt; xx.sql-- 排除不兼容的tables, 列名带# ，表名带 .SELECT * FROM information_schema.columns where table_schema = &#39;xxxx&#39; and (column_name like &#39;%#%&#39; OR table_name like &#39;%.%&#39;); snapshot splitreader 阶段暂停task，savepoint 保持offset，缓存中的queue是否存在？内存 seatunnel 退出的原因cluster miss -d, Ctrl+c 退出 恢复任务即使data_save_mode=drop_data，有checkpoint的情况下依然可以增量Incremental Snapshots in Debezium debezium{ include.schema.changes = false }" }, { "title": "Typescript", "url": "/posts/Typescript/", "categories": "language", "tags": "ts", "date": "2024-06-03 10:43:36 +0800", "snippet": "Finally, by setting the esModuleInterop option to true in tsconfig.json, you can import CommonJS modules using the syntax below (which is compliant with the ECMAScript specifications):// my-module.tsimport foo from “someCommonJsModule”; [TypeScript Organizing and Storing Types and Interfaces](https://www.becomebetterprogrammer.com/typescript-organizing-and-storing-types-and-interfaces/) " }, { "title": "filebeat", "url": "/posts/filebeat/", "categories": "ELK", "tags": "log", "date": "2024-05-24 10:46:43 +0800", "snippet": "The symlinks option can be useful if symlinks to the log files have additional metadata in the file name, and you want to process the metadata in Logstash. This is, for example, the case for Kubernetes log files.https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-container.htmlautodiscover# Kubernetes autodiscover provider supports hints in Pod annotations. To enable it just set hints.enabledfilebeat.autodiscover: providers: - type: kubernetes hints.enabled: trueconfigmap return bad format after edit,get it back by below commandkubectl -n kube-system get cm filebeat-config -o json | jq &#39;.data.&quot;filebeat.yml&quot;&#39; -r" }, { "title": "kubernetes mysql", "url": "/posts/kubernetes-mysql/", "categories": "kubernetes", "tags": "k8s", "date": "2024-03-13 16:33:32 +0800", "snippet": "配置[mysqld]default-time-zone = &quot;+08:00&quot;max_connections = 500SELECT @@global.time_zone;show variables like &quot;max_connections&quot;;set global max_connections = 200; mycnf: | [mysqld] max_connections=1000 default-time-zone=&quot;+08:00&quot; innodb_buffer_pool_size=51539607552 innodb_buffer_pool_instances=8 max_allowed_packet=524288000 net_write_timeout=3600 binlog_expire_logs_seconds=259200 innodb_log_buffer_size=1073741824 innodb_log_file_size=1073741824 innodb_flush_method=O_DIRECT innodb_io_capacity_max=8000 innodb_io_capacity=4000 replica_parallel_workers=40 log_bin=ON sql_mode=STRICT_TRANS_TABLES innodb_write_io_threads = 12 innodb_read_io_threads = 12集群# 进入containermysqlsh -uroot -p -h127.0.0.1STOP GROUP_REPLICA;START GROUP_REPLICA;dba.dropMetadataSchema()dba.configureInstance()dba.checkInstanceConfiguration()# 查看集群var cluster = dba.getCluster()cluster.status()cluster.describe()cluster.rescan()cluster.dissolve()cluster = dba.createCluster(&#39;devCluster&#39;)# option {recoveryMethod: &#39;clone&#39;} 当无法以增量的方式增加实例时（比如某些table没有主键）cluster.addInstance(&#39;root@localhost:3330&#39;)cluster.removeInstance(&#39;xxxx&#39;,{force:true})mysqlrouter --bootstrap root@localhost:3310 -d mysqlrouter # 重新引导router\\help# exit\\quitk8s 部署单个实例做持久化，新建 data，config pvc, 配置挂载在/etc/mysql/conf.d/mysqld.cnf- image: mysql:8.0.36 env: - name: MYSQL_ROOT_PASSWORD value: xxxx volumeMounts: - name: &quot;config&quot; mountPath: &quot;/etc/mysql/conf.d/mysqld.cnf&quot; subPath: &quot;mysql.conf.d&quot; - name: &quot;data&quot; mountPath: &quot;/var/lib/mysql&quot; nodeSelector: workload: production volumes: - name: &quot;config&quot; configMap: name: mysql-config - name: &quot;data&quot; persistentVolumeClaim: claimName: nfs-mysql[mysqld]innodb_log_buffer_size=16777216default-time-zone=&quot;+08:00&quot;log_bin=OFFinnodb_flush_method=O_DIRECT集群helm install mysql operatorhelm install mycluster mysql-operator/mysql-innodbcluster \\ --set credentials.root.user=&#39;root&#39; \\ --set credentials.root.password=&#39;sakila&#39; \\ --set credentials.root.host=&#39;%&#39; \\ --set serverInstances=3 \\ --set routerInstances=1 \\ --set tls.useSelfSigned=trueFAQ如何导出views导出view create scriptsmysql -pxxxx -u root -h127.0.0.1 --skip-column-names --batch -e &#39;select CONCAT(&quot;DROP TABLE IF EXISTS &quot;, TABLE_SCHEMA, &quot;.&quot;, TABLE_NAME, &quot;; CREATE OR REPLACE VIEW &quot;, TABLE_SCHEMA, &quot;.&quot;, TABLE_NAME, &quot; AS &quot;, VIEW_DEFINITION, &quot;; &quot;) table_name from information_schema.views&#39; &amp;gt; /tmp/views.sqlmysql router 断开所有连接 如果用户向仅剩一个成员的现有集群添加新实例，MySQL Router 将断开与集群的所有连接。发生这种情况的原因是新实例存在于组复制元数据中，但尚未存在于集群的元数据中。MySQL Router 认为没有法定人数并断开连接。在集群的元数据中显示新实例后，可以重新连接 mysql cluster 8.0.19 后元数据由1.x 升至 2.x ,需upgrade metadata 先，再部署高版本 router失去法定人数SELECT @@group_replication_local_address;set global group_replication_force_members=@@group_replication_local_address;SET GLOBAL group_replication_force_members=&quot;127.0.0.1:10000,127.0.0.1:10001&quot;; This procedure uses group_replication_force_members and should be considered a last resort remedy. It must be used with extreme care and only for overriding loss of quorum. If misused, it could create an artificial split-brain scenario or block the entire system altogether.问题 Error_code: 1032; handler error HA_ERR_KEY_NOT_FOUND集群中在SECONDARY只读节点强行SET global super_read_only = OFF ,修改表数据，如 truncate table，将导致group replication出现以上错误如何修复节点mysql将自动shutdown并无法重新加入集群，不断重启，应先移除集群 cluster.removeInstance ，节点mysql实例将不再discovery by cluster，此时可正常run，再重新加入，默认将以Incremental的 ReconverMethod 加入集群并恢复，可能部分table没有primary key，没有通过precheckconfig,所以 cluster.addInstance(‘xxxx:3306’,{recoveryMethod: ‘clone’}) 加入，先Drop table 再copy file. 报Unkown variable ，配置问题因pod没有销毁，pvc的内容将不变，即使改了k8s configmap 也无法同步本地config delete pod , mysql pod 中包含sidecar 和mysql， 即使 mysql 因错误重启，也无法同步configmap找到pvc，找到config，vim 进入改参考mysql router settingmysql router 8.0.33 bug about only one remaining member复制时统计复制时线程监控mysql集群组复制使用 Performance_schema 实现 MySQL 组复制监控中的可观察性view replication_status_fullMySQL 8 and Replication ObservabilityMGR最优化配置推荐MySQL 并行复制可以帮助我的从属服务器吗？Handling a Network Partition and Loss of Quorum MySQL any way to import a huge (32 GB) sql dump faster?MariaDB" }, { "title": "BusinessObject", "url": "/posts/BusinessObject/", "categories": "bo", "tags": "bo", "date": "2024-02-06 14:16:51 +0800", "snippet": "Merging Data From Multiple Queries" }, { "title": "zabbix", "url": "/posts/zabbix/", "categories": "op", "tags": "zabbix", "date": "2024-01-23 16:08:59 +0800", "snippet": "housekeeperhousekeeperdatabase-size根据趋势 计算 需要的 数据库大小zabbix_serverzabbix_server --versionzabbix_server -c /etc/zabbix/zabbix_server.conf -R housekeeper_execute # 手动启动housekeeperzabbix_agentd --versionsudo apt install zabbix_get # 4.0.50 ,3.x.x 版本 报 Check access restrictions in Zabbix agent configurationzabbix_get -s ip/hostname -p 10050 -k agent.ping # return 1 if oktrigger: expression {HOSTNAME:agent.ping.nodata(5m)}=1 # 表示 该主机评估时间内 agent.ping 没有响应item agent.ping update invterval: 1m # 表示每分钟执行一次agent.ping综合表示 每5min 检查 agent.ping(实际上执行了5次) 这五次是否有数据返回，因为 zabbix_get可以看到 ping 会return 1Action 由 1个或多个 trigger 满足条件时触发并向 user group 发送邮件，比如 trigger equal to disaster 时audit_log 查看 email发送snpmagentzabbix_get" }, { "title": "juniper", "url": "/posts/juniper/", "categories": "op", "tags": "juniper", "date": "2024-01-09 09:29:18 +0800", "snippet": "juniper run install file:///junos-srxsme-15.1X49-D75.5-domestic.tgz 之后报错Source media /dev/da0s1 does not exist,查看FreeBsd 挂载外部存储usb 的output，da0s1 代表 设备da0分区1, 因为usb device 没有分区，所以只有sim0，没有sim1，可以用fdisk分区再格式化Fat32，再放入tgz镜像文件。md0: Preloaded image &amp;lt;/isofs-install-srxsme&amp;gt; 20035584 bytes at 0x80f66068ad0: 7695MB &amp;lt;CF 8GB Ver7.02&amp;gt; at ata0-master WDMA2ad1: 476940MB &amp;lt;WD Blue SA510 2.5 500GB SSD 52015100&amp;gt; at ata1-master SATA300Kernel thread &quot;wkupdaemon&quot; (pid 49) exited prematurely.da0 at umass-sim0 bus 0 target 0 lun 0da0: &amp;lt;aigo U268 1100&amp;gt; Removable Direct Access SCSI-4 deviceda0: 40.000MB/s transfersda0: 30000MB (61440000 512 byte sectors: 255H 63S/T 3824C)Trying to mount root from cd9660:/dev/md0WARNING: preposterous time in file systemWARNING: clock 13335 days greater than file system timetty: not foundStarting JUNOS installation: Source Package: disk0:/junos-srxsme-15.1X49-D75.5-domestic.tgz Target Media : internal Product : srx550mERROR: Source media /dev/da0s1 does not existumass0: &amp;lt;STECH Simple Drive, class 0/0, rev 2.00/1.04, addr 3&amp;gt; on usbus0umass0: SCSI over Bulk-Only; quirks = 0x0100umass0:4:0:-1: Attached to scbus4da0 at umass-sim0 bus 0 scbus4 target 0 lun 0 # da = Direct Access , da0 = Direct Access node 0, da0sim0 = 设备da0分区0da0: &amp;lt;STECH Simple Drive 1.04&amp;gt; Fixed Direct Access SCSI-4 deviceda0: Serial Number WD-WXE508CAN263da0: 40.000MB/s transfersda0: 152627MB (312581808 512 byte sectors: 255H 63S/T 19457C)da0: quirks=0x2&amp;lt;NO_6_BYTE&amp;gt;$ sudo fdisk -l /dev/sdb #list partition tables$ sudo fdisk /dev/sdb$ command (m for help): g # create new empty gpt partition schema$ command: n # create a new partition$ Partition number (1-128, default 1):$ First sector (2048-500118158, default 2048):enter$ Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-500118158, default 500118158): +100G$ command: p # display partition table$ command: w # save$ sudo mkfs.ext4 -F /dev/sdb1 -v$ sudo sudo file -s /dev/sdb1 All USB flash drives used on SRX Series Firewalls must have the following features: USB 2.0 or later. Formatted with a FAT/FAT 32 or MS-DOS file system Generally speaking, FAT/NTFS are designed for Windows, Ext is used with Linux systems, and APFS/HFS are macOS file systems. Each of these address the logic of file structure differently which can result in issues.freebsd Device Configurationfdiskjuniper srx550m 最新os下载 mkfsjuniper os install from usb " }, { "title": "Eggjs", "url": "/posts/Eggjs/", "categories": "js", "tags": "eggjs", "date": "2023-12-26 15:40:26 +0800", "snippet": "sequelize-automatenpx sequelize-automate -t egg -h [hostname] -d [database] -u [username] -p [password] -P 3306 -e mysql -o model -c ..\\sequelize-automate.config.json--match, -m Match tables using given RegExp. [string] [default: null]–match 无效 ，要借助 json 文件 写入 tables{ &quot;tables&quot;:[&quot;xxxx&quot;]}sequelize-automate" }, { "title": "Python", "url": "/posts/Python/", "categories": "code", "tags": "python", "date": "2023-11-28 10:09:11 +0800", "snippet": "内置函数python ormPendulum python datetimepip install pymssql==2.2.8" }, { "title": "docsify", "url": "/posts/docsify/", "categories": "doc", "tags": "docsify", "date": "2023-10-30 14:50:15 +0800", "snippet": "https://docsify.js.org/http-serverdocsifydocsify-clinpm install -g docsify-clidocsify serve /docs 默认监听 3000 和 35279 ,后者服务于livereload,文件修改通过websocket通知前端更新。部署时可install其他反向代理服务器替代，如http-server自动生成sidebar$ npm install -g docsify-tools$ docsify-auto-sidebar -d .hub.docker 上 docsify-tools版本滞后,没有ignore _coverpage，download source_code, npm login 之后 publish 补丁npm publish @xxxx/docsify-tools --access publiclet ignores = /node_modules|^\\.|_sidebar|_navbar|_coverpage|_docsify/;git-sync当期望git pull 变更后自动gen sidebar ，git-sync 有 GIT_SYNC_EXECHOOK_COMMAND，run sh 文件 the command executed with the syncing repository as its working directory after syncing a new hash of the remote repository. it is subject to the sync time out and will extend period between syncs. (doesn’t support the command arguments)考虑 k8s git-sync@3.6.3 没有nodejs环境，重新build image,替换debian update源，安装nvm，再install nodejs，其他package` –no-cache –progress plain` 可debug看到详细报错或输出RUN wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash timeout自行下载 install.sh 文件， ADD install.sh /tmp/nvm 复制入镜像FROM registry.xxxx.com.cn/git-sync:v3.6.3USER rootRUN sed -i &#39;s/deb.debian.org/mirrors.aliyun.com/g&#39; /etc/apt/sources.list &amp;amp;&amp;amp; \\ sed -i &#39;s/security.debian.org/mirrors.aliyun.com/g&#39; /etc/apt/sources.list &amp;amp;&amp;amp; \\ apt-get update -yENV NODE_VERSION=21.1.0RUN apt-get install -y curl bashENV NVM_DIR /tmp/nvmRUN file=&quot;$(ls -1)&quot; &amp;amp;&amp;amp; echo $fileRUN file=&quot;$(ls -1)&quot; &amp;amp;&amp;amp; echo $fileRUN mkdir -p /tmp/nvm# 复制install.sh 进去ADD install.sh /tmp/nvmWORKDIR /tmp/nvmRUN file=&quot;$(ls -1)&quot; &amp;amp;&amp;amp; echo $fileRUN bash /tmp/nvm/install.shRUN . &quot;$NVM_DIR/nvm.sh&quot; &amp;amp;&amp;amp; nvm install ${NODE_VERSION}RUN . &quot;$NVM_DIR/nvm.sh&quot; &amp;amp;&amp;amp; nvm use v${NODE_VERSION}RUN . &quot;$NVM_DIR/nvm.sh&quot; &amp;amp;&amp;amp; nvm alias default v${NODE_VERSION}ENV NODE_PATH $NVM_DIR/versions/node/v$NODE_VERSION/lib/node_modulesENV PATH $NVM_DIR/versions/node/v$NODE_VERSION/bin:$PATHRUN npm --versionRUN npm config set proxy nullRUN npm config set https-proxy nullRUN npm config set registry http://registry.npmjs.org/# RUN npm install -g http-server@latest --verboseRUN npm install -g @zengxq/docsify-tools --registry=https://registry.npm.taobao.org --verboseRUN file=&quot;$(ls -1)&quot; &amp;amp;&amp;amp; echo $fileauto-sidebar.sh#!/bin/bashdocsify-auto-sidebar -d .# debugdocsify serve . --port=3000git-sync exechooksync 后执行指定sh 文件#!/bin/shcd .. &amp;amp;&amp;amp;git config --global core.quotepath off &amp;amp;&amp;amp;git checkout -f main &amp;amp;&amp;amp;git pull &amp;amp;&amp;amp;git ls-tree -r --name-only HEAD | while read filename; do echo &quot;{ \\&quot;view\\&quot;:\\&quot;$filename\\&quot;,\\&quot;date\\&quot;:\\&quot;$(git log -1 --format=&quot;%ci&quot; -- $filename)\\&quot; },&quot;done | perl -pe &#39;BEGIN{print &quot;[&quot;}; END{print &quot;]\\n&quot;}&#39; | perl -pe &#39;s/},\\n/},/&#39;| perl -pe &#39;s/},]/}]/&#39;| tee -i &#39;update-last.json&#39;node js file如果 install -g package , root 下没有 packages.json 文件npm install -g chokidar &amp;amp;&amp;amp; \\export NODE_PATH=$(npm root --quiet -g) &amp;amp;&amp;amp; \\node /docs/git-sync/updateModTime.js参考docsify 官网docsify-themeabledocsify 插件" }, { "title": "anki", "url": "/posts/anki/", "categories": "learn", "tags": "anki", "date": "2023-10-13 15:50:40 +0800", "snippet": "POST 需申请 Hypothesis Token，Postman 选 Bearer Token GetAnnotations and store id in redis addNotes with local host set cronjob参考Hypothesis APIoffcial websiteanki-connect githubanki-connect offcial websiteanki doc" }, { "title": "baison", "url": "/posts/baison/", "categories": "develop", "tags": "baison", "date": "2023-09-07 16:47:22 +0800", "snippet": "跨公司买卖[LS.HD.0001] 亿才商业（上海）有限公司 [CZH] 华东仓 -&amp;gt;[LS.JV.0001] 信蝶商业（杭州）有限公司 [HZ1] 杭州仓核算 [LS.HB.0003] 昶信贸易（天津）有限公司 直运单 ZYD2024022600029 （华东仓 -&amp;gt; 杭州仓） 出库单 CKD202402260038 (由直运单执行出库，华东仓出库，2对) 协同订单 XTDD2024022600002 （[CZH] 华东仓 -&amp;gt; [JZW] 杭州仓） 销售单 XSD24022600005 （[LS.HD.0001] 亿才商业（上海）有限公司 卖给 [LS.HB.0003] 中转-昶信贸易） 采购单 CGD24022600003 （[LS.HB.0003] 昶信贸易（天津）有限公司 从 [LS.HD.0001] 亿才商业（上海）有限公司 采购）（入库bill_no CGD24022600003） 销售单 XSD24022600006 （[LS.HB.0003] 中转-昶信贸易 卖给 [LS.JV.0001] 信蝶商业（杭州）有限公司） 采购单 CGD24022600004 （[LS.JV.0001] 信蝶商业（杭州）有限公司 从 [LS.HB.0003] 昶信贸易 采购）（入库，2对） 入库单 RKD202402260020 ([JZW] 杭州仓 入库 ，来源直运单，mq-job 生成提交审核并推送jd，成功后回写库存数量) 收货验收差异单 发货验收差异单 YSCYD24022600001 （发多收少，发了3对，收了2对） XTDD2024030800004 协同订单 RKD202403080011 入库单 CGD24030800009 采购单 （收1对） 备注： 跨结算公司会协同生成两对销采，以昶信中转标准采购 采购申请单 DPO240305002 (订货会，计划从供应商[YX1] 广州市易翔鞋业有限公司采购 3对) 采购订单 XPO24030500003 （申请单执行，头单， 要 3对） 采购单 CGD24030500002 （计划入[CXWH] 昶信总仓 ，采购订单执行，要 1对） 入库单 RKD202403050002 采购单 CGD24030500003 （计划入[CXWH] 昶信总仓 ，采购订单执行，执行数 3对，可累计超过采购订单，但单次上限是采购订单的数量） 入库单 RKD202403050003 不同核算公司采购 DPO240227006 （核算 [LS.HB.0003] 昶信贸易（天津）有限公司，从供应商采购） XPO24022700004 （收货核算 [LS.HN1.0015] 利信达商业（中国）有限公司 [ESG] 电商仓） ZYD2024022700158 （采购订单执行，生成SCM直运经销，[LS.HB.0003] 中转-昶信贸易 发出， [ESG] 电商仓 收） 协同订单 销售单 （[LS.HB.0003]昶信贸易（天津）有限公司 卖给 [LS.HN1.0015] 利信达商业（中国）有限公司） 采购单 （[LS.HN1.0015] 利信达商业（中国）有限公司 从 [LS.HB.0003]昶信贸易（天津）有限公司 采购 ） RKD202402270096 （直运单入库，入库bill_no ZYD2024022700158） " }, { "title": "airflow", "url": "/posts/airflow/", "categories": "Operation", "tags": "job", "date": "2023-09-05 11:17:04 +0800", "snippet": "You should use a different secret key for every instance you run, as this key is used to sign session cookies and perform other security related functions!set up databasehttps://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#helm install$ export NAMESPACE=airflow$ kubectl create namespace $NAMESPACE$ export RELEASE_NAME=airflow$ helm install airflow apache-airflow/airflow --namespace airflow -f override.yaml$ helm upgrade --install airflow apache-airflow/airflow --namespace airflow -f override.yamlairflow users create \\ --username admin \\ --password xxxx \\ --firstname Peter \\ --lastname Parker \\ --role Admin \\ --email spiderman@superhero.org配置文件 override.yaml# Default airflow repository -- overridden by all the specific images belowdefaultAirflowRepository: registry.xxx.com.cn/airflow-xxx# Default airflow tag to deploydefaultAirflowTag: &quot;2.7.0&quot;postgresql: enabled: falsedags: gitSync: enabled: true repo: git@code.xxx.com.cn:xiangqiu.zeng/airflow-pags.git branch: main subPath: &quot;&quot; sshKeySecret: airflow-ssh-secretextraSecrets: airflow-ssh-secret: data: | gitSshKey: &#39;&#39;data: # Otherwise pass connection values in metadataConnection: user: airflow pass: airflow protocol: mysql host: mysql.airflow port: 3306 db: xxx_airflow sslmode: disableimages: gitSync: repository: registry.xxx.com.cn/git-sync tag: v3.6.3 pullPolicy: IfNotPresentworkers: extraVolumes: - name: storage persistentVolumeClaim: claimName: nfs-airflow extraVolumeMounts: - name: storage mountPath: /opt/airflow/tempwebserverSecretKey: webserver-secret-keywebserverSecretKeySecretName: my-webserver-secretairflow imageFROM apache/airflow:2.7.0RUN pip config set global.index-url http://mirrors.aliyun.com/pypi/simpleRUN pip config set install.trusted-host mirrors.aliyun.comRUN pip install &quot;apache-airflow==${AIRFLOW_VERSION}&quot; --no-cache-dir apache-airflow-providers-microsoft-mssql==3.4.2webserverSecretKeypython online$ python3 -c &#39;import secrets; print(secrets.token_hex(16))&#39;$ kubectl create secret generic my-webserver-secret --from-literal=&quot;webserver-secret-key=&#39;xxxxx&#39;)&quot;git-syncregistry.k8s.io 无法访问，通过hub-mirror提供issue触发action自动拉取image{ &quot;hub-mirror&quot;: [ &quot;registry.k8s.io/git-sync/git-sync:v3.6.3&quot; ]}$ docker pull togettoyou/registry.k8s.io.git-sync.git-sync:v3.6.3$ ssh-keygen -t rsa -b 4096 -C &quot;xiangqiu.zeng@lesaunda.com.cn&quot;$ base64 airflow -w 0 &amp;gt; temp.txt #gitSshKey访问 gitlab-&amp;gt;设置个人资料-&amp;gt;ssh密钥，粘贴复制 pkworker volumetask 中将数据写入 /opt/airflow/dags/temp ，提示 readonly system申请 nfs pv 再挂载apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-airflow namespace: airflowspec: accessModes: - ReadWriteMany resources: requests: storage: 100Gi storageClassName: nfs-gzk8s-storagemetadata databaseCREATE DATABASE xxx_airflow CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;CREATE USER &#39;airflow&#39; IDENTIFIED BY &#39;airflow&#39;;GRANT ALL PRIVILEGES ON xxx_airflow.* TO &#39;airflow&#39;;停用postgresql，避免创建containerpostgresql: enabled: falseingress在dag中trigger，跳转location 没有带31198端口，待查workaroundkubectl expose deploy airflow-webserver --type=NodePort --name=airflow-webserver-external -n airflowingress: web: enabled: true ingressClassName: nginx path: &quot;/airflow&quot; pathType: Prefixconfig: webserver: base_url: http://192.168.1.xxx:31198/airflowwebserver: defaultUser: enabled: true role: Admin username: admin email: admin@example.com firstName: admin lastName: user password: admin新建 user进入webserver pod,会提示输入pwdairflow users create \\ --username admin \\ --firstname admin \\ --lastname user \\ --role Admin \\ --email admin@example.comprovider主要分hook和 operator ，operator 默认带入Dag default_args 存在的key-value，比如 mysql_conn_id，mssql_conn_idhook get_records return tuple[],其中 decimal, timestamp 需要注意decimal CAST(FLOOR(INV.qty)+FLOOR(INV.qty_tran_out) AS SIGNED) timestamp MAX( DATE_FORMAT(INV.modify_time,&#39;%Y-%m-%d %H:%i:%s&#39;)) as &#39;f_date&#39;mssql You can also use an external file to execute the SQL commands. Script folder must be at the same level as DAG.py file. This way you can easily maintain the SQL queries separated from the code.sql 文件 与 py 文件放在同一folder update_data = MsSqlOperator( task_id=&quot;update_data&quot;, database=&quot;xxxx&quot;,sql=&quot;inventory_xxxx_update.sql&quot;, dag=dag )templates-refupgrade airflow v2.7.3 之后 MsSqlOperator connect 报 Adaptive server connection failed (DB-Lib error message 20002, severity 9)operator 使用 pymssql + Freetds + OBDC 访问mssqlfreetds 配置文件 /etc/freetds/freetds.conf ，参考 官方配置 , github/stackoverflow 讨论是 tds version = 7.0 问题，进入worker 容器， 更改export TDSVER=7.0，未通过; freetds 自带命令行工具 tsql -H [host] -p 1433 -o v -U [username] -D [database] -v ， freetds 配置 tsql -CCompile-time settings (established with the &quot;configure&quot; script) Version: freetds v1.2.3 freetds.conf directory: /etc/freetds MS db-lib source compatibility: no Sybase binary compatibility: yes Thread safety: yes iconv library: yes TDS version: auto iODBC: no unixodbc: yes SSPI &quot;trusted&quot; logins: no Kerberos: yes OpenSSL: no GnuTLS: yes MARS: yestsql 连接没有问题， 挂载 /temp/test.py,报同样错误import pymssqlconn = pymssql.connect(host=&quot;&quot;, username=&#39;&#39;, password=&#39;&#39;, database=&quot;&quot;)配置 freetds 日志输出 export TDSDUMP=/tmp/freetds.log比较tsql 和 test.py 输出，发现多了openssl, 测试库无配置证书，2.2.10的pymssql,–with-openssl=yes 所以无法连接，尝试将pymssql降版本 2.2.10 -&amp;gt; 2.2.8 ,pip install pymssql==2.2.8，测试通过Starting log file for FreeTDS 1.4.3login.c:1281:detected crypt flag 0tls.c:1019:setting default openssl cipher to:HIGH:!SSLv2:!aNULL:-DH....log.c:168:Starting log file for FreeTDS 1.2.3login.c:1281:detected crypt flag 0kubectl get pods -n airflow -o jsonpath=&quot;{.items[*].spec[&#39;initContainers&#39;, &#39;containers&#39;][*].image}&quot; |\\tr -s &#39;[[:space:]]&#39; &#39;\\n&#39; |\\sort |\\uniq -cjinjajinja2 if -%} 不换行templatingmysql sql renderPython: best practice and securest way to connect to MySQL and execute queries传参format格式PEP 249 – Python Database API Specification v2.0variableVariablehelm install airflowHelm Chart for Apache AirflowMysql作为metadata数据库创建metadata数据库1.CREATE DATABASE lesaunda_airflow CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;CREATE USER &#39;airflow&#39; IDENTIFIED BY &#39;airflow&#39;;GRANT ALL PRIVILEGES ON lesaunda_airflow.* TO &#39;airflow&#39;;ingressconfig: webserver: base_url: http://localhost:8080/airflow-dev op_kwargs (Optional[Mapping[str, Any]]): This is the dictionary we use to pass in user-defined key-value pairs to our python callable function PythonOperator 可以传参 templates_dict (Optional[Dict[str, Any]]): This is the dictionary that airflow uses to pass the default variables as key-value pairs to our python callable function. E.g., the ‘task_instance’ or ‘run_id’ are a couple of variables that airflow provides us by default. Full list of default variables is here. task 可以通过templates_dict传参 So technically, we just need to pass the additional key-value pairs we need through the op_kwargs parameter while using the PythonOperator. I think that’s the intended design choice, but we could still use either to pass in our key-value pairs.Airflow takes care of the rest. The execute method in the PythonOperator merges the kwargs and templates_dict into a single dictionary, which we later unpack in the python_callable function, generally using either **kwargs or **context. Airflow code for this is here. Airflow context is only accessible from tasks in runtime, and TaskGroup is not a task, it’s just a collection of tasks used to group the tasks in the UI.TaskGroup 只是显示上的一组task，不可以在运行时访问 baseOperator override 会重载并创建 new task 从其他py file import func 再 decorate @task operator.output 可以作为@task func 的参数 A DAG run is usually scheduled after its associated data interval has ended, to ensure the run is able to collect all the data within the time period. In other words, a run covering the data period of 2020-01-01 generally does not start to run until 2020-01-01 has ended, i.e. after 2020-01-02 00:00:00. 运行在间隔的结束，因为间隔期间会收集数据kubernete ingress workaroundTemplates referenceFreeTDSStable APIariflow exampleuseful demotemplate_fields: Sequence[str] = (&quot;path&quot;,) 如果 path (eg: /tmp//my_file) 嵌套了jinja的模板，需要注册template_fields,由jinja解析后再传参" }, { "title": "Java with VSCode", "url": "/posts/Java-with-VSCode/", "categories": "language", "tags": "vscode", "date": "2023-08-08 10:47:21 +0800", "snippet": "download java jdk适用于 Windows 的 Java 下载download mavenDownloading Apache Maven 3.9.4修改镜像库地址配置文件 C:\\Program Files\\Maven\\apache-maven-3.9.3\\conf\\settings.xml&amp;lt;mirrors&amp;gt; &amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;aliyunmaven&amp;lt;/id&amp;gt; &amp;lt;mirrorOf&amp;gt;*&amp;lt;/mirrorOf&amp;gt; &amp;lt;name&amp;gt;阿里云公共仓库&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;https://maven.aliyun.com/repository/public&amp;lt;/url&amp;gt; &amp;lt;/mirror&amp;gt; &amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;aliyunmaven&amp;lt;/id&amp;gt; &amp;lt;mirrorOf&amp;gt;*&amp;lt;/mirrorOf&amp;gt; &amp;lt;name&amp;gt;阿里云谷歌仓库&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;https://maven.aliyun.com/repository/google&amp;lt;/url&amp;gt; &amp;lt;/mirror&amp;gt; &amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;aliyunmaven&amp;lt;/id&amp;gt; &amp;lt;mirrorOf&amp;gt;*&amp;lt;/mirrorOf&amp;gt; &amp;lt;name&amp;gt;阿里云阿帕奇仓库&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;https://maven.aliyun.com/repository/apache-snapshots&amp;lt;/url&amp;gt; &amp;lt;/mirror&amp;gt; &amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;aliyunmaven&amp;lt;/id&amp;gt; &amp;lt;mirrorOf&amp;gt;*&amp;lt;/mirrorOf&amp;gt; &amp;lt;name&amp;gt;阿里云spring仓库&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;https://maven.aliyun.com/repository/spring&amp;lt;/url&amp;gt; &amp;lt;/mirror&amp;gt; &amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;aliyunmaven&amp;lt;/id&amp;gt; &amp;lt;mirrorOf&amp;gt;*&amp;lt;/mirrorOf&amp;gt; &amp;lt;name&amp;gt;阿里云spring插件仓库&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;https://maven.aliyun.com/repository/spring-plugin&amp;lt;/url&amp;gt; &amp;lt;/mirror&amp;gt;&amp;lt;/mirrors&amp;gt;编辑环境变量How do I set or change the PATH system variable?How to Set the PATH Variable in WindowsJAVA_HOME=C:\\Program Files\\Java\\jre-1.8MAVEN_HOME=C:\\Program Files\\Maven\\apache-maven-3.9.3# Path%MAVEN_HOME%\\bin%JAVA_HOME%\\bin变量生效setx PATH &quot;%MAVEN_HOME%\\bin;%PATH%&quot;echo %PATH%# 查看maven版本mvn -vjava -versionVSCode Install extension Extension Pack for java， 修改vscode用户设置（JSON）&quot;java.configuration.runtimes&quot;: [ { &quot;name&quot;: &quot;JavaSE-1.8&quot;, &quot;path&quot;: &quot;C:\\\\Program Files\\\\Java\\\\jre-1.8&quot; }],&quot;maven.executable.path&quot;:&quot;C:\\\\Program Files\\\\Maven\\\\apache-maven-3.9.3\\\\bin\\\\mvn&quot;,&quot;maven.terminal.customEnv&quot;: [ { &quot;environmentVariable&quot;: &quot;JAVA_HOME&quot;, &quot;value&quot;: &quot;C:\\\\Program Files\\\\Java\\\\jre-1.8&quot; }],&quot;java.project.referencedLibraries&quot;: [ &quot;library/**/*.jar&quot;],&quot;java.configuration.maven.userSettings&quot;:&quot;C:\\\\Program Files\\\\Maven\\\\apache-maven-3.9.3\\\\conf\\\\settings.xml&quot;从vscode maven 组件 新建项目,根据项目需要下载package，maven repository,国内无法访问，需要代理&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.flink&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;flink-table-api-java-bridge&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.17.1&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;" }, { "title": "flink", "url": "/posts/flink/", "categories": "Data", "tags": "flink", "date": "2023-08-02 11:14:54 +0800", "snippet": "vue save configjar get config and run ,flink sql api to build tables end to end insert. starrock datastream sourcerestart k8s container ,bash kubectl command to restartETL 作业通常会周期性地触发，将数据从事务型数据库拷贝到分析型数据库或数据仓库。数据管道是以持续流模式运行，而非周期性触发。 mysql table 如何 作为无界的输入源实现DynamicTable相关接口,比如mysql cdc 原生 flink connector jdbc 不支持 delete 操作,没有实现 upportsRowLevelDelete 接口,要cdc 才可以 排除delete的情况,flink jdbc (mysql -&amp;gt; mssql) ,通过bull定时下发http request, select * from source where lastmoddate &amp;gt; 时间间隔 ,实现简单增量修改 k8s install flink cluster DataStream Sourceflink 中文文档SHOW VARIABLES LIKE &#39;log_bin&#39;;REST API " }, { "title": "LSD k8s cluster", "url": "/posts/LSD-k8s-cluster/", "categories": "k8s", "tags": "k8s", "date": "2023-07-31 10:47:54 +0800", "snippet": " components nfs-provisioner es ingress-nginx flannel filebeat dashboard jv-staff lsd-staff lsd-sync lsd-job-manager lsd-inventory lsd-sysinfo rabbitmq redis redisinsight mysql-cluster " }, { "title": "Excel VBA", "url": "/posts/Excel-VBA/", "categories": "tools", "tags": "excel", "date": "2023-07-25 19:06:21 +0800", "snippet": " 数字带引号csv格式，点击数据-分列再比较How to List Sheet Name in Excel (5 Methods + VBA)Workbook 3 Methods to Create a List of Hyperlinks to All Worksheets in an Excel WorkbookAtl +F11 进入 VB Script 窗口，选中 this workbook,Sub CreateMenuOfHyperlinksToAllWorksheets() Dim objSheet As Worksheet ActiveWorkbook.Sheets.Add(Before:=Worksheets(1)).Name = &quot;Sheet Menu&quot; Range(&quot;A1&quot;).Select For Each objSheet In ActiveWorkbook.Worksheets If ActiveSheet.Name &amp;lt;&amp;gt; objSheet.Name Then ActiveCell.Hyperlinks.Add Anchor:=Selection, Address:=&quot;&quot;, SubAddress:= &quot;&#39;&quot; &amp;amp; objSheet.Name &amp;amp; &quot;&#39;&quot; &amp;amp; &quot;!A1&quot;, TextToDisplay:=objSheet.Name ActiveCell.Offset(1, 0).Select ActiveCell.EntireColumn.AutoFit End If Next objSheet With ActiveSheet .Rows(1).Insert .Cells(1, 1) = &quot;MENU&quot; .Cells(1, 1).Font.Bold = True .Cells(1, 1).Font.Size = 14 .Cells(1, 1).Columns.AutoFit End WithEnd SubCtrl + 左下角三角&amp;lt; 再选中第一个sheet" }, { "title": "Bull", "url": "/posts/Bull/", "categories": "scheduler", "tags": "js", "date": "2023-07-03 17:37:58 +0800", "snippet": "Auto-discover queues There is no a way to auto-discover queues, because you need to provide the connection &amp;amp; the specific connection….You can add queues dynamically, with addQueue method that createBullBoard returns.Deadly0const {setQueues} = createBullBoard({queues: [], serverAdapter});client.KEYS(`${config.BULL_PREFIX}:*`, (err, keys) =&amp;gt; {...setQueues(queueList);" }, { "title": "ClickHouse", "url": "/posts/ClickHouse/", "categories": "database", "tags": "clickhouse", "date": "2023-06-28 11:18:13 +0800", "snippet": "https://github.com/ClickHouse/ClickHouse/blob/master/LICENSEConnect ClickHouse下载ClickHouse 驱动下载MDAC打开ODBC管理数据源（64位），选择ClickHouse驱动（UniCode）添加demo，url=https://play.clickhouse.com:443/,username=explorer或者play，参考官网打开 ODBCTest（Unicode,amd64），conn-&amp;gt;full connect-&amp;gt;选择之前添加的数据源，select &#39;1&#39;,Ctrl+E, SuccessCode=0, Ctrl+R，Get all data;阿里云 ClickHouse通过HTTPS协议连接ClickHouse" }, { "title": "Expressjs", "url": "/posts/Expressjs/", "categories": "api", "tags": "nodejs, expressjs", "date": "2023-06-25 14:50:52 +0800", "snippet": "Routerexpressjs router 可以设计为模块，挂载在 app 上，它也被称之为mini-approuter 定义 routerouter.get(&#39;/&#39;, (req, res) =&amp;gt; { res.send(&#39;Birds home page&#39;)})router 挂载 appapp.use(&#39;/birds&#39;, birds)" }, { "title": "Eslint", "url": "/posts/Eslint/", "categories": "code", "tags": "eslint", "date": "2023-06-20 16:28:09 +0800", "snippet": "pluginsThe eslint-plugin- prefix can be omitted from the plugin name{ &quot;plugins&quot;: [ &quot;plugin1&quot;, &quot;eslint-plugin-plugin2&quot; ]}shared configBegin with eslint-config-, such as eslint-config-myconfig.{ &quot;extends&quot;: &quot;myconfig&quot;}eslint 9.0.0+ 不再支持json confighttps://eslint.org/docs/latest/use/configure/plugins" }, { "title": "Kubernetes Role", "url": "/posts/Kubernetes-Role/", "categories": "kubernetes", "tags": "role, rolebinding, serviceaccount", "date": "2023-06-06 09:55:36 +0800", "snippet": "role对单一namespace资源授权ClusterRole 除Role之外扩展node，endpoint,其他namespacerolebinding subjects(user, serviceaccount, group) roleRef(role,ClusterRole)" }, { "title": "nodejs", "url": "/posts/nodejs/", "categories": "language", "tags": "node", "date": "2023-06-02 14:12:14 +0800", "snippet": "vscode F1 toggle auto attach select “仅带标识 –inspect”打开新的terminal，npm run scriptnodemonwindow 10 stuck at ‘restarting due to changes’,和window env 配置有关，可能需要rebootknexjsinsert 返回listwhereILike 默认 collate SQL_Latin1_General_CP1_CI_AS likebuild image 时可sed 替换RUN sed -i &#39;s/SQL_Latin1_General_CP1_CI_AS/Chinese_PRC_CI_AS/gp&#39; node_modules/knex/lib/dialects/mssql/query/mssql-querycompiler.jsnodejs 退出 需销毁资源process.on(&#39;SIGINT&#39;, () =&amp;gt; { logger.debug(&#39;SIGINT signal received: closing HTTP server&#39;); connection.destroy().then(() =&amp;gt; { process.exit(0); });});transaction在多个promise task中并行多个knexjs transaction 提示 Requests can only be made in the LoggedIn state, not the SentClientRequest state,这是由组件 tedious 报的 socket exception， 同一个connection无法并行执行多个事务。/* 单一事务 */connection.transaction(async (trx) =&amp;gt; { // get multiple promise operation (eg: insert into header first ,and detail) // await Promise.all(subtasks)})batchInest提示 The incoming request has too many parameters. The server supports a maximum of 2100 parameters. Reduce the number of parameters and resend the request.const chunkSize = 200;const chunkArray = [];for (let i = 0; i &amp;lt; machine.apps.length; i += chunkSize) { /* 切割 */ const chunk = machine.apps.slice(i, i + chunkSize); const t = trx(&#39;tables&#39;) .insert(chunk) .then(() =&amp;gt; { logger.info( `${extractId} ${machineName} had been extracted completely` ); return machineId; }); chunkArray.push(t); // do whatever}Promise.all(chunkArray).then((ids) =&amp;gt; &#39;1&#39;);knex-transactionshttps://stackify.com/node-js-error-handling/https://www.mankier.com/debugnpm install -g debugwindowset DEBUG=*,express.router* &amp;amp;&amp;amp; node --inspect index.js或者install dotenv, .env 文件配置DEBUG=express.router*https://www.digitalocean.com/community/tutorials/nodejs-cron-jobs-by-examplesyarn add @bull-board/ui@file:../../packages/uijs debug onlineplaycodenpm proxyset env http_proxy = http://xxxx:port重启vscode生效snippetsconst arr = &quot;&quot;.split(&#39;,&#39;).filter(Boolean) // []packageschokidar监听文件或目录变更const watcher = chokidar.watch(&#39;file, dir, glob, or array&#39;, { ignored: /(^|[\\/\\\\])\\../, // ignore dotfiles persistent: true});npm install errornpm install amqplib # stucknpm install amqplib # 输出日志查看日志 fail to removing log file ....查看 path user/&amp;lt;username&amp;gt;/local/npm-cached/_logs 发现指定的log file 无法删除，提示无权限打开任务管理器，资源监视器，关联的句柄 搜索 这个文件发现 有个 node 进程 占用了，可能是后台vscode run command 没有释放admin run cmd , taskkill /F /PID &amp;lt;PID&amp;gt; kill 进程express-fileuploadkubernetes nginxproxy-body-size : 15mimport fileUpload from &#39;express-fileupload&#39;;app.use( fileUpload({ limits: { fileSize: 15 * 1024 * 1024, }, }));如果两者不对等，zip文件将被切割，出现end of central directory record signature not found" }, { "title": "Mongo", "url": "/posts/Mongo/", "categories": "database", "tags": "mongo", "date": "2023-05-19 16:13:38 +0800", "snippet": "connect to mongo instancemongo -h 127.0.0.1 -port 27017list dbsdb.adminCommand( { listDatabases: 1 } )切换dbuse graylog查看dbsizedb.stats()查看replica ，not running with –replSetrs.status()Shut down firstsystemctl status mongod.service /usr/bin/mongod -f /etc/mongod.confsystemctl stop mongod.servicemongod –shutdownrs.initiate()replica set 方式runmongod –port 27017 –dbpath /var/lib/mongo –replSet rs0 –bind_ip localhost –config /etc/mongod.conf加入replica setrs.add( { host: “192.168.1.191:31787” } )kubectl expose deployment/kubernetes-bootcamp –type=”NodePort”mongod –port 27017 –dbpath /data/db –replSet rs0 –bind_ip localhost,192.168.20.95 –config /etc/mongod.confmongod –port 27017 –dbpath /var/lib/mongo –replSet rs0 –bind_ip localhost,192.168.20.95 –config /etc/mongod.confmongo commandConvert a Standalone to a Replica SetAdd Members to a Replica Set" }, { "title": "kubernetes volume", "url": "/posts/kubernetes-volume/", "categories": "kubernetes", "tags": "volume", "date": "2023-05-17 10:28:14 +0800", "snippet": "emptyDiremptyDir用于节点临时存储，pod消失，volume内容消失pv持久卷分静态制备和动态制备静态制备pv 创建，status available pending, 等待绑定，pvc创建绑定，已申领未绑定， pod 创建，persistentVolumeClaim 中 绑定对应的pvc进行访问创建-申领-绑定动态制备创建StorageClass 存储类, 创建serviceaccount, rbac , role, rolebinding等，部署创建provisioner pod（serviceaccount, storageclass）, 通过storageclass 创建pvc, 再部署其他需要动态制备持久卷的容器apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-elasticspec: accessModes: - ReadWriteMany storageClassName: &quot;nfs-gzk8s-storage&quot; # 动态制备，对应storagclassname ,静态对应控制符串 resources: requests: storage: 500Gi # volumeName: foo-pv # 对应之前静态制备的pv卷name，动态制时需注释掉！other deploymentvolumes: - name: storage persistentVolumeClaim: claimName: nfs-elastic访问模式ReadWriteOnce 卷可以被一个节点以读写方式挂载。 ReadWriteOnce 访问模式也允许运行在同一节点上的多个 Pod 访问卷。ReadWriteMany 卷可以被多个节点以读写方式挂载容量单位是 Ki (kibi)、Mi (mebi)、Gi (gibi)、 Ti (tebi)、 Pi (pebi)、 Ei (exbi)回收策略 Retain – 手动回收 Recycle – 基本擦除 (rm -rf /thevolume/*) Delete – 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除user 删除 PVC 不会立刻移除，会等待关联的pod退出， 同理，删除PV要求没有bound的PVC，顺序上是先terminal pod，delete pvc ,自动 delete pvnfs install serveryum -y install rpcbind nfs-utilssystemctl enable --now nfs-server.servicesystemctl status rpcbind# 服务未开启，则开启# 创建pathmkdir -m 0755 -p /data/nfs# 默认只有50G 挂载在root下df -hT /data/nfs# 修改为home下，容量大mount -t auto /dev/mapper/centos-home /data/nfs# 设置权限echo &quot;/data/nfs *(rw,sync,no_root_squash)&quot; &amp;gt;&amp;gt; /etc/exports# 生效exports -rv# 必要时关闭防火墙systemctl stop firewalld.service# 查看nfs 公布的pathshowmount -e client在客户端测试nfs 用于测试server，非必需storageclass 会指定provisioner(制备器)，比如nfs需要外部制备器（docker.io/dyrnq/nfs-subdir-external-provisioner:v4.0.2）参考Configure NFS as Kubernetes Persistent Volume StorageKubernetes PVC Guide: Basics, Tutorials and Troubleshooting Tipskubernetes exampleslinux 分区pvc capacity just a label" }, { "title": "kubernetes nfs", "url": "/posts/kubernetes-nfs/", "categories": "k8s", "tags": "k8s, nfs", "date": "2023-05-16 14:29:02 +0800", "snippet": "在所有节点安装nfs clientyum install -y nfs-utilsnfsstat -s -cUsing nfsstat and nfsiostat to troubleshoot NFS performance issues on Linux" }, { "title": "CentOS", "url": "/posts/CentOS/", "categories": "system", "tags": "centos", "date": "2023-03-27 14:20:53 +0800", "snippet": "ntp client 同步时间vim /etc/chrony.conf# 修改pool [ntp server ip] iburstsystemctl restart chronyd防火墙service firewalld stopsystemctl enable firewalldfirewall-cmd --query-port=9300/tcp # 查端口是否开放firewall-cmd --add-port=9300/tcp --permanent # 防火墙永久性允许指定端口firewall-cmd --reload Firewalld stopped, but firewall-cmd shows “running”?参考 这里# 无意中键入命令，发现k8s proxy 无法正常运行， 即使是type = nodeport的服务$ firewalld$ sudo systemctl stop firewalld$ sudo systemctl disable firewalld$ sudo systemctl mark --now firewalld# 以上命令均无法关闭防火墙$ systemctl status firewalld.serviceinactive$ sudo firewall-cmd --staterunning# 查看pid$ ps -ax | grep firewalld3014 ? Ssl 0:07 /usr/bin/python2 -Es /usr/sbin/firewalld# kill processs$ kill 3014$ sudo firewall-cmd --statenot runningulimitulimit -Hn # 查看进程可打开的最大句柄数ulimit -n 655350 # 设置cat /proc/1518/limits # 查看# 如未能生效，可尝试一下方法vim /lib/systemd/system/elasticsearch.service # 修改LimitNOFILE=655350systemctl daemon-reload #重启守护进程ps -edaf | grep mongo | grep -v grep 找到piddnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm参考 linux系统配置了limits.conf为何不生效？CentOS7配置阿里云镜像源（超详细过程）epel 修改mirror清华大学EPEL 软件仓库镜像使用帮助$ sudo yum clean all &amp;amp;&amp;amp; yum makecache南科大镜像库tmp 自动清理$ systemctl status systemd-tmpfiles*$ cat /usr/lib/tmpfiles.d/tmp.conf/etc/apt/sources.list 备份，替换du -h /data/nfs/ | sort -hr | head -n 10# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb http://security.ubuntu.com/ubuntu/ xenial-security main restricted universe multiverse# deb-src http://security.ubuntu.com/ubuntu/ xenial-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse# # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse" }, { "title": "linux", "url": "/posts/linux/", "categories": "system", "tags": "linux", "date": "2023-03-01 11:22:33 +0800", "snippet": "分区扩容umount 时可能出现正在使用，执行以下命令# 备份xfsdump -f /home.xfsdump /home# 卸载umount /home# 减容lvreduce -L 50G /dev/mapper/centos-home# 扩容# 生效xfs_growfs /dev/mapper/centos-root# 格式化mkfs.xfs -f /dev/mapper/centos-home# 挂载mount /home# 恢复xfsrestore -f /home.xfsdump /homeyum install -y psmiscsudo fuser -km /home# 找大文件du -h /home | sort -hr | head -n 10#查看系统版本cat /etc/os-releaseln (original file path) (new file path)du -hc --max-depth=0 &amp;lt;directory&amp;gt;When changes are made to one filename, the other reflects those changes.# 覆盖写入$ cat &amp;gt; file.txt# 追加写入$ cat &amp;gt;&amp;gt; file.txt# here documnet$ cat &amp;gt; file.txt &amp;lt;&amp;lt; EOF&amp;gt; 内容&amp;gt; EOFiptableslsof -nP -iTCP -sTCP:LISTENhttps://phoenixnap.com/kb/wp-content/uploads/2021/04/iptables-diagram.pnghttps://phoenixnap.com/kb/iptables-tutorial-linux-firewall` sudo tcpdump -nn -A -s1500 -l -i eno1 port 31198`iptables -Liptables -Sfdisklvm (Logical Volume Manager) 扩容eg: zabbix 服务器提示 /lib/var/mysql too full, VM + 200G 到 sda$ sudo cd /var/lib/mysql # 提示 cd not found ，因为cd （change directory）不是程序,所以要切换用户$ sudo -i$ df -hs # disk file system usage/dev/mapper/xxx 786G 564G 189G 75% /$ du -hs /var/lib/mysql # 递归显示 disk file usage560G /var/lib/mysql$ lsblk # 查看块设备（list block devices）大小和挂载， /dev/sda 有200G未使用$ fdisk -l # 显示硬盘分区table$ fdisk /dev/sdatype n (new one partition)type 4 (number of partition)type p (primary type)enter ...type w (save)type p (printf)$ reboot$ lsblk # 新增了/dev/sda3 200G$ pvcreate /dev/sda3 # 初始化物理卷，提供给LVM扩容使用$ vgs # 查看卷组 VG #PV #LV #SN Attr VSize VFree GZ-NWMon2-vg 2 2 0 wz--n- 580g 0$ vgextend GZ-NWMon2-vg /dev/sda3 # 扩充卷组$ vgs # 查看卷组 VG #PV #LV #SN Attr VSize VFree GZ-NWMon2-vg 2 2 0 wz--n- 580g 200g$ lvextend -l +100%FREE /dev/mapper/GZ--NWMon2--vg-root # 扩充逻辑卷$ resize2fs -p /dev/mapper/GZ--NWMon2--vg-root # 扩充文件系统$ df -hs # 已生效$ ps -Flww -p THE_PID #查看pid 信息$ ls | grep -P &quot;^OFF.*&quot; | xargs -d&quot;\\n&quot; rm # 删除正则匹配到的文件$ sar -u 5 #Show CPU Utilization$ iostattop$ top -O # 显示field$ top -o -%MEM # 按内存使用降序排列$ top | grep mysqld # 查看指定processtimedatectlsudo timedatectl set-timezone Asia/Shanghaisudo timedatectl set-ntp truecurlcurl -w @- -o /dev/null -s &quot;$@&quot; &amp;lt;&amp;lt;&#39;EOF&#39; time_namelookup: %{time_namelookup}\\n time_connect: %{time_connect}\\n time_appconnect: %{time_appconnect}\\n time_pretransfer: %{time_pretransfer}\\n time_redirect: %{time_redirect}\\n time_starttransfer: %{time_starttransfer}\\n ----------\\n time_total: %{time_total}\\nEOFcurl timersync# 复制文件进度rsync -ah --progress /root/apache-seatunnel-2.3.6-SNAPSHOT-bin.tar.gz ./dmidecode # 查看硬件信息参考Linux Directory Structure Explained for BeginnersCentOS根目录存储容量调整大小How to Install and Enable Bash Auto Completion in CentOS/RHELHow to Get the Size of a Directory in Linuxsed commandHow to Increase space on Linux vmware" }, { "title": "Vuejs", "url": "/posts/Vuejs/", "categories": "category", "tags": "tag", "date": "2023-01-17 12:07:59 +0800", "snippet": "HTML 标签和属性名称是不分大小写的，所以浏览器会把任何大写的字符解释为小写。这意味着当你使用 DOM 内的模板时，无论是 PascalCase 形式的组件名称、camelCase 形式的 prop 名称还是 v-on 的事件名称，都需要转换为相应等价的 kebab-case (短横线连字符) 形式。use ES6 to refer to “ECMAScript 2015 Language” (arrow functions, template strings, Promises)vuejs history model 情况下，刷新页面，出现404 nginx 问题 参考 kubernetes nginxviteCreate Vue Projectnpm create vite@latest my-vue-app -- --template vue@babel/preset-env for compiling ES2015+ syntaxhttps://github.com/node-fetch/node-fetch/issues/1289require和import的区别谈一谈对Thinkjs3进行业务逻辑测试的方法ES6 VS ES2015 this.$nextTick(() =&amp;gt; { setTimeout(() =&amp;gt; { let lastColEl = document.querySelector(&#39;.el-table__header colgroup col:last-child&#39;) if (lastColEl) { // 最后一列的宽度加上滚动条的宽度 lastColEl.width = Number(lastColEl.width) + 20 // 6为滚动条宽度 } }, 1000) })vetur 内置 Linting vue/essentialformatters &quot;vetur.format.defaultFormatter.js&quot;: &quot;prettier&quot;, &quot;vetur.format.defaultFormatter.html&quot;: &quot;js-beautify-html&quot;, &quot;[vue]&quot;: { &quot;editor.defaultFormatter&quot;: &quot;octref.vetur&quot;, &quot;editor.formatOnSave&quot;: true, },vetureslint-plugin-vuevue vscode snippets" }, { "title": "qmall", "url": "/posts/qmall/", "categories": "category", "tags": "tag", "date": "2023-01-13 10:02:40 +0800", "snippet": "商品类目：（1个接口）规格类型：必定是选择“多规格”（3个接口）规格名：对应 specInfoList 下的 specId 与 specName，可以通过 查询规格列表 API 获取。规格值：对应 specInfoList.skuSpecValueList 下的 specValueId 与 specValueName。可以通过 查询规格值列表 API 获取。规格图片尺码推荐商品详情模板：“轮播推荐”（1个接口上传图片）配送方式：只选择“商家配送”，这里显示创建好的运费模板（1个接口）兼容requirejs &amp;amp; commonjs 语法node –experimental-modules my-app.mjsnodejs12异步堆栈node temp.js –async-stack-tracesweimob APIorder list searchYou don’t need Babel with Nodenode版本支持的语法mysql work with json在线转换timestampHow to use ESM on the web and in Node.jsOfficial Document查询订单详情 weimob_shop/order/detail/get查询订单列表 weimob_shop/order/list/searchaxoisOfficial apinode-mysqlUsing MySQL with Node.js and the mysql JavaScript ClientINSERT INTO tablename SET assignment_listdate as int 毫秒级 bigint(13) 秒级： int(13) 日期： int(11)const lastModTime = Date.now().valueOf() / 1000MySQL 语法金蝶对账流程nginx部署vue 创建node镜像，再build，最后创建nginx镜像，copy from node 放入/usr/share/nginx/html 单独将vue-cli-service build生成的dist文件夹放入/usr/share/nginx/html参考 How to Serve a Vue App with nginx in Dockervue formatshift + alt + F 未能格式化 vue 文件，查看output， 提示Cannot find module &#39;./parser-html.js&#39;执行 npm install prettier@2.8.3 --save-devvue create projectvue-cli-service build 提示 Cannot read properties of undefined (reading &#39;tap&#39;) ，安装vue-template-compiler@2.7.14提示 ERR_OSSL_EVP_UNSUPPORTED ,script: SET NODE_OPTIONS=--openssl-legacy-provider &amp;amp;&amp;amp; vue-cli-service build参考 vue-cli Official Doc修改文件权限Linux File Permission Tutorial: How to Check and Change Permissionschmod 644 filechown owner filechgrp group file6 = 4(read)+2(write)+0(excute),其他参考公式think-logger3 只保留两天日志查 runtime/production.json 正常,底层用log4j，进入docker container 查看log4j版本 cat /node_modules/log4j/package.json，发现version&amp;gt;6.0.0,对比正式env 4.5.1查看log4j datefile ,相比之前的版本多了 numBackups - integer (default 1) - the number of old files that matches the pattern to keep (excluding the hot file).所以只保留两天日志查看npm install 策略 使用npm的语义控制版本以及 将所有 Node.js 依赖包更新到最新版本，发现^3.0.0 npm install 会自动打补丁，但正式env不会up到3.2.14，仍在3.2.11，查看DOCKERFILE,参考Build your Node image,发现只copy 了package.json ，无锁定版本，最后修改DOCKERFILE COPY [&quot;package.json&quot;, &quot;package-lock.json*&quot;, &quot;./&quot;]cnpm 源js arraysplice() 方法通过删除或替换 splice(start, deleteCount, item1)slice() 方法返回一个新的数组对象,浅拷贝 slice(start, end)reduce() 累加器let sum = [0, 1, 2, 3].reduce(function (previousValue, currentValue) { return previousValue + currentValue}, 0)join() join(separator) 连成字符串const actions = Object.keys(think.app.controllers.admin).map((x) =&amp;gt; { return { [x]: think.app.controllers.admin[x] .toString() .match(/(\\b.*Action)/g) .reduce((accumulator, value) =&amp;gt; { return { ...accumulator, [value.replace(&quot;Action&quot;, &quot;&quot;) + &quot;:any&quot;]: [&quot;*&quot;], }; }, {}), };});常见问题 vue project env 不生效放在根目录How to collect, customize, and centralize Node.js logs如何查看日志文件tail -10 app.log | grep -E &#39;regex.*&#39;less can search like vi/vim /?, can scroll backmoregrepview like vi open in readonly-modelel-ui 清空datepicker ,自定义@change即可knexjs whereLike throw &quot;COLLATION &#39;utf8_bin&#39; is not valid for CHARACTER SET &#39;utf8mb4&#39;&quot; 原因是whereLike default COLLATION, 改用 whereILike(case-insensitive), knex(&#39;users&#39;).whereILike(&#39;email&#39;, &#39;%mail%&#39;)whereLike(statement, collation = &#39;utf8_bin&#39;) { return `${this._columnClause(statement)} ${this._not( statement, &#39;like &#39; )}${this._valueClause(statement)} COLLATE ${collation}`; }查看 whereLike does not work with the MySQL utf8mb4 character set docker volume create redis-vol,qmall-bill-vol,qmall-ui-voldocker run \\ --network=qmall \\ -h qmall-redis \\ --name=qmall-redis \\ --expose 6379 \\ --restart=always \\ --mount source=redis-vol,target=/data \\ -d \\ redis/redis-stack-server:latestdocker run \\ -d \\ --network=qmall \\ -h qmall-bill-api \\ --name=qmall-bill-api \\ --expose 3000 \\ --restart=always \\ --mount source=qmall-bill-vol,target=/backend/static/certificate \\ --mount source=qmall-bill-settlements,target=/backend/static/settlements \\ --mount source=qmall-bill-log,target=/backend/logs \\ qmall-bill-api:1.0.6api rebuild 后需要restart ui, 因为ui nginx upstream 找不到 api hostnamedocker run \\ -d \\ --network=qmall \\ -p 8155:8155 \\ -v /usr/local/Source/nshop/custom/nginx.conf:/etc/nginx/nginx.conf \\ -v /etc/localtime:/etc/localtime:ro \\ -v /usr/local/Source/manage/html:/usr/share/nginx/html \\ --mount source=qmall-ui,target=/var/log/nginx \\ --restart=always \\ --name qmall-ui \\ nginx评估 goods 表 + filed delivery_time 对发货的影响前端 router-view 是否 需要 keep-aliveERP fastcgi 是一种协议 php-fpm 是 cgi的进程管理器 nginx 启用 ngxhttp_fastcgi_module 通过tcp socket方式 转发到管理器php:5.6-fpmdocker run --network=qmall -v /root/projects/crm/s1:/var/www/html -v /root/www.conf:/usr/local/etc/php-fpm.d/www.conf -v /root/up_php.ini:/usr/local/etc/php/conf.d/up_php.ini -d php:7.3-fpmdocker run --network=qmall -v /root/codes:/var/www/html -v /root/www.conf:/usr/local/etc/php-fpm.d/www.conf -v /root/up_php.ini:/usr/local/etc/php/conf.d/up_php.ini -d php:5.6-fpmdocker pull codeigniter/nginx-php-fpm:13根目录 /var/lib/nginx/html修改 application/config/database.config hostname pwd database# imagedocker run -d -p 8555:80 --network=qmall -v /root/default.conf:/etc/nginx/conf.d/default.conf -v /root/codes:/var/lib/nginx/html codeigniter/nginx-php-fp:13enable mysql extensionup_php.iniextension=mysqliextension=pdo_mysqlextension=pdo_odbcextension=mysqliextension=pdo_mysqlextension=pdo_odbcRUN docker-php-ext-install pdo pdo_mysqlRUN docker-php-ext-install mysqli &amp;amp;&amp;amp; docker-php-ext-enable mysqlienable gb extensionFROM php:8.1.0-fpm# ... other instructions ...# setup GD extensionRUN apt-get update &amp;amp;&amp;amp; \\apt-get install -y libfreetype6-dev libjpeg62-turbo-dev libpng-dev &amp;amp;&amp;amp; \\docker-php-ext-configure gd --with-freetype=/usr/include/ --with-jpeg=/usr/include/ &amp;amp;&amp;amp; \\docker-php-ext-install gd#7.4+docker-php-ext-configure gd --with-freetype --with-jpeg#7.3docker-php-ext-configure gd --with-png-dir=/usr/include/ --with-jpeg-dir=/usr/include/ --with-freetype-dir=/usr/include/docker-php-ext-configure gd --with-webp=/usr/include/webp --with-jpeg=/usr/include --with-freetype=/usr/include/freetype2/ --with-jpeg-dir=/usr/include/;# ... other instructions ...nginx.conf server { listen 80; server_name 47.xxx.xx.xxx; root /var/www/html; location ~ .*\\.php { #try_files $uri =404; root /var/www/html; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass 172.18.0.12:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; #fastcgi_param SCRIPT_FILENAME $fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_path_info; fastcgi_read_timeout 300; fastcgi_send_timeout 300; fastcgi_connect_timeout 300; fastcgi_buffers 16 32k; fastcgi_buffer_size 64k; fastcgi_busy_buffers_size 64k; } location / { root /var/www/html; index index.html index.htm; } }php md5 online后端 数据库#order_goods 新增字段 delivery_time rights_update_time #admin 新增字段 - role servicegetOrderInfos 定时任务新增更新 delivery_time，rights_update_timebase.js 新增验证 token exp前端 fetch.js 新增判断接口返回status = 401时， remove token， 重定向至登陆界面 修复侧边面板定位打开的route#其他新增 对账 api及UI creat tabledocker run -itd –expose 8360 -v /etc/localtime:/etc/localtime:ro -v /usr/local/Source/nshop/logs:/backend/logs –restart=always –network=qmall –name qmall_api_104 qmall_api:1.0.4docker volume create redis-voldocker run –network=qmall -h qmall-redis –name=qmall-redis –expose 6379 –restart=always –mount source=redis-vol,target=/data -d redis/redis-stack-server:latestdocker volume create qmall-bill-logdocker volume create qmall-bill-voldocker build -t qmall-bill-init:1.0.0 .docker run -it –network=qmall qmall-bill-init:1.0.0docker build -t qmall_api:1.0.4 .docker run -d –network=qmall -h qmall-bill-api –name=qmall-bill-api_1.0.0 –expose 3000 –restart=always –mount source=qmall-bill-vol,target=/backend/static/certificate –mount source=qmall-bill-log,target=/backend/logs qmall-bill-api:1.0.0 location /api/bills { proxy_pass http://qmall-bill-api_1.0.0:3000/bills; } location /api/suppliers { proxy_pass http://qmall-bill-api_1.0.0:3000/suppliers; }vim /usr/local/Source/nshop/custom/nginx.confwget https://raw.staticdn.net/grafana/loki/v2.8.0/cmd/loki/loki-local-config.yaml -O loki-config.yamldocker run –name loki -d -v $(pwd):/mnt/config -p 3100:3100 grafana/loki:2.8.0 -config.file=/mnt/config/loki-config.yamlwget https://raw.staticdn.net/grafana/loki/v2.8.0/clients/cmd/promtail/promtail-docker-config.yaml -O promtail-config.yamldocker run –name promtail -d -v $(pwd):/mnt/config -v /var/log:/var/log –link loki grafana/promtail:2.8.0 -config.file=/mnt/config/promtail-config.yamlscrape_configs: job_name: systemstatic_configs: targets: localhostlabels: job: varlogs path: /var/log/*log job_name: fontendstatic_configs: targets: localhostlabels: job: fontend path: /var/lib/docker/volumes/projects_front-logs/_data/*log docker run –name promtail -d -v $(pwd)/promtail-config.yaml:/mnt/config/promtail-config.yaml -v /var/log:/var/log -v /var/lib/docker/volumes/projects_front-logs/_data/qmall-access.log:/var/log/qmall-access.log –link loki grafana/promtail:2.8.0 -config.file=/mnt/config/promtail-config.yaml1，（直接关联DV系统数据）实际收入实际成本实际盈利2，（当月对账系统结算汇总）结算收入结算成本结算盈利以上数据都需要附带明细表明细大类：0，订单日期1，订单号2，商品名称3，规格4，数量5，实际收入6，成本7，盈利vendor/topthink/think-mongo/src/Query.php收入概况预下载账单，再get billitem from redis, set unicost group by goods, set netincome into redis by datemetabasedocker run -d -p 3000:3000 \\ -e &quot;MB_DB_TYPE=mysql&quot; \\ -e &quot;MB_DB_DBNAME=metabase&quot; \\ -e &quot;MB_DB_PORT=3306&quot; \\ -e &quot;MB_DB_USER=root&quot; \\ -e &quot;MB_DB_PASS=Lesaunda&quot; \\ -e &quot;MB_DB_HOST=172.21.14.207&quot; \\ --name metabase metabase/metabase" }, { "title": "javascript", "url": "/posts/javascript/", "categories": "code", "tags": "js", "date": "2023-01-03 10:35:22 +0800", "snippet": "使用 var 声明的变量将在任何代码执行前被创建，这被称为变量提升。这些变量的初始值为 undefined。let can be updated but not re-declared.var var declarations are globally scoped or function/locally scoped. var variables can be re-declared and updated Hoisting of varlet let can be updated but not re-declared.Const const declarations are block scoped const cannot be updated or re-declared问题 ERROR in build.js from UglifyJs Unexpected token: punc (()参考 ERROR in build.js from UglifyJs Unexpected token: punc (()" }, { "title": " MySQL", "url": "/posts/MySQL/", "categories": "database", "tags": "mysql", "date": "2022-12-30 15:53:20 +0800", "snippet": "Command# connectmysql --user=root --password nshop -h 127.0.0.1# backupmysqldump -u root -p nshop -h 127.0.0.1 --column-statistics=0 --create-options -v &amp;gt; nshop_20230220.sql# mysql-cli 无法输入中文,系统使用的是POSIX字符集，POSIX字符集是不支持中文的，而C.UTF-8是支持中文的crictl exec -it xxx env LANG=C.UTF-8 /bin/bash-- 清理 binlogPURGE BINARY LOGS BEFORE &#39;2008-12-15 10:06:06&#39;;SELECT * FROM performance_schema.replication_group_members;SELECT * FROM performance_schema.replication_applier_status;SELECT * FROM performance_schema.replication_group_member_stats;SELECT * FROM performance_schema.replication_group_communication_information;SELECT * FROM performance_schema.replication_connection_status;SELECT * FROM performance_schema.replication_applier_status;SELECT @@group_replication_flow_control_applier_threshold;SELECT group_replication_get_write_concurrency();SELECT @@replica_parallel_workers;show variables like &#39;%group%&#39;;-- 监控组复制SELECT MEMBER_ID AS id, COUNT_TRANSACTIONS_IN_QUEUE AS trx_tobe_verified, COUNT_TRANSACTIONS_REMOTE_IN_APPLIER_QUEUE AS trx_tobe_applied, COUNT_TRANSACTIONS_CHECKED AS trx_chkd, COUNT_TRANSACTIONS_REMOTE_APPLIED AS trx_done, COUNT_TRANSACTIONS_LOCAL_PROPOSED AS proposed FROM performance_schema.replication_group_member_stats;SELECT RECEIVED_TRANSACTION_SET FROM performance_schema.replication_connection_status WHERE channel_name = &#39;group_replication_applier&#39; UNION ALL SELECT variable_value FROM performance_schema.global_variables WHERE variable_name = &#39;gtid_executed&#39;;SELECT MEMBER_HOST, COUNT_TRANSACTIONS_IN_QUEUE TRX_LOCAL_Q, COUNT_TRANSACTIONS_REMOTE_IN_APPLIER_QUEUE TRX_APPLY_QFROM performance_schema.replication_group_member_stats t1 JOIN performance_schema.replication_group_members t2 ON t2.MEMBER_ID = t1.MEMBER_ID;SHOW REPLICAS;SHOW BINARY LOGS;SELECT @@innodb_log_buffer_size;SET GLOBAL innodb_log_buffer_size = 1073741824;SELECT @@key_buffer_size ;show variables like &#39;innodb_thread_concurrency&#39;;show variables like &#39;log_%&#39;;show variables like &#39;%expire%&#39;;SET GLOBAL binlog_expire_logs_seconds = 259200;SELECT @@innodb_flush_log_at_trx_commit;SET GLOBAL innodb_redo_log_capacity = 1073741824;SELECT NAME, PROCESSLIST_INFO FROM performance_schema.threads WHERE NAME=&quot;thread/group_rpl/THD_transaction_monitor&quot;;SELECT event_name, work_completed, work_estimated FROM performance_schema.events_stages_current WHERE event_name LIKE &quot;%stage/group_rpl%&quot;;SELECT group_replication_get_write_concurrency();SELECT * FROM performance_schema.replication_applier_status_by_coordinator;SELECT * FROM performance_schema.replication_applier_status_by_worker;SET GLOBAL group_replication_poll_spin_loops= 10000;SET GLOBAL innodb_io_capacity_max=8000;SET GLOBAL innodb_io_capacity=4000;SET GLOBAL replica_parallel_workers=40;select CHANNEL_NAME,SSL_ALLOWED,SSL_VERIFY_SERVER_CERTIFICATE,CONNECTION_RETRY_INTERVAL,CONNECTION_RETRY_COUNT,HEARTBEAT_INTERVAL,COMPRESSION_ALGORITHM,GTID_ONLY from performance_schema.replication_connection_configuration;select CHANNEL_NAME,SOURCE_UUID,SERVICE_STATE,RECEIVED_TRANSACTION_SET,LAST_QUEUED_TRANSACTION,LAST_QUEUED_TRANSACTION_ORIGINAL_COMMIT_TIMESTAMP,LAST_QUEUED_TRANSACTION_IMMEDIATE_COMMIT_TIMESTAMP,LAST_QUEUED_TRANSACTION_START_QUEUE_TIMESTAMP,LAST_QUEUED_TRANSACTION_END_QUEUE_TIMESTAMP,QUEUEING_TRANSACTION from performance_schema.replication_connection_status;select CHANNEL_NAME,THREAD_ID,SERVICE_STATE,LAST_PROCESSED_TRANSACTION,LAST_PROCESSED_TRANSACTION_ORIGINAL_COMMIT_TIMESTAMP,LAST_PROCESSED_TRANSACTION_IMMEDIATE_COMMIT_TIMESTAMP,PROCESSING_TRANSACTION from performance_schema.replication_applier_status_by_coordinator;select CHANNEL_NAME,WORKER_ID,THREAD_ID,SERVICE_STATE,LAST_APPLIED_TRANSACTION,APPLYING_TRANSACTION from performance_schema.replication_applier_status_by_worker;select MEMBER_ID,COUNT_TRANSACTIONS_IN_QUEUE from performance_schema.replication_group_member_stats;select * from performance_schema.replication_applier_global_filters;SELECT MEMBER_HOST,MEMBER_STATE,MEMBER_ROLE FROM performance_schema.replication_group_members;select * from sys.replication_lag;select * from replication_status;select * from sys.replication_status_full;SELECT table_schema &quot;DB Name&quot;, ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) &quot;DB Size in MB&quot;FROM information_schema.tableswhere table_schema = &#39;lesaunda_airflow&#39;GROUP BY table_schema; Data Manipulation Language (DML) Data Query Language (DQL) Data Definition Language (DDL)DDLCREATE DATABASE flycrm CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;备份和恢复# 查看可用$ yum repolist all | grep mysql# install$ sudo yum install mysql-community-server# dump$ mysqldump --column-statistics=0 -u root -h 192.168.0.72 -p nshop nideshop_order_goods --verbose &amp;gt; nideshop_order_goods_20230712.sql$ mysqldump -h 39.108.xxx.20 -u e3plus --port 3306 -p --all-databases --no-data --single-transaction --skip-lock-tables --verbose | gzip &amp;gt; my_backup.sql.gz$ mysqldump --column-statistics=0 -h 127.0.0.1 -u root -p nshop | gzip &amp;gt; nshop_20231227.sql.gz# 解压$ gzip -d my_backup.sql.gz# restore, mysql client 5.x 连接 MySQL 8.x 可能出现 Authentication plugin &#39;caching_sha2_password&#39; cannot be loaded, upgrade$ mysql -h 172.21.14.207 -uroot --port 3307 -p nshop_20231227 &amp;lt; my_backup.sqleg$ mysqldump --column-statistics=0 -h 127.0.0.1 -u root -p nshop | gzip &amp;gt; nshop_20231227.sql.gz# if want to ignore errors about unkown database when create view , should add --force# --all-databases$ mysqldump -h 127.0.0.1 --column-statistics=0 -uroot -p nshop nideshop_order nideshop_order_goods &amp;gt; nshop_20240115_test.sql# ssh local machine$ scp root@110.41.xx.xxx:/root/nshop_20231227.sql.gz /root# gzip$ gzip -d nshop_20231227.sql.gz$ docker exec -it qmall-db /bin/sh$ CREATE DATABASE nshop_20231227 CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;$ exit$ mysql -h 172.21.14.207 -uroot -p nshop_20231227 &amp;lt; nshop_20231227.sql# window$ pscp root@110.41.xxx.xxx:/root/nshop_20231227.sql.gz &quot;E:\\SQL Script\\zeefu&quot;Traditionally, an EXISTS subquery starts with SELECT *, but it could begin with SELECT 5 or SELECT column1 or anything at all. MySQL ignores the SELECT list in such a subquery, so it makes no difference.开启 log_bin mysql-8.0.33 配置文件path/etc/my.cnf[mysqld]log_bin = /var/log/mysql-bin.logSTRICT_TRANS_TABLES includes the effect of the ERROR_FOR_DIVISION_BY_ZERO, NO_ZERO_DATE, and NO_ZERO_IN_DATEtimestampadd(MINUTE,-5,NOW()) VARCHAR(M) - Length + 1 bytes if column values require 0 – 255 bytesVARCHAR(4) ‘ab’ require 3BytemssqlFor multibyte encoding character sets, the storage size is still n bytes + 2 bytes but the number of characters that can be stored may be smaller than n.How Many Bytes Per Character in SQL Server: a Completely Complete GuideChinese_PRC_CI_ASVARCHAR(80)Chinese-PRC, case-insensitive, accent-sensitive, kanatype-insensitive, width-insensitive排序规则名称由两部份构成，前半部份是指本排序规则所支持的字符集。 如： chinese_prc_cs_ai_ws 前半部份：指unicode字符集，chinese_prc_指针对大陆简体字unicode的排序规则。 排序规则的后半部份即后缀 含义： _bin 二进制排序 _ci(cs) 是否区分大小写，ci不区分，cs区分 _ai(as) 是否区分重音，ai不区分，as区分　　　 _ki(ks) 是否区分假名类型,ki不区分，ks区分　 _wi(ws) 是否区分宽度 wi不区分，ws区分修改collateALTER TABLE nideshop_rights MODIFY orderNo varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;alter table nideshop_rights convert to character set utf8mb4 collate utf8mb4_general_ci;SELECT table_name AS `Table`, round(((data_length + index_length) / 1024 / 1024), 2) `Size in MB`FROM information_schema.TABLESWHERE table_schema = &quot;zabbix&quot; AND table_name = &quot;history_unit&quot;;导出到文件SHOW VARIABLES LIKE &quot;secure_file_priv&quot;;SELECT id, first_name, last_nameFROM customerINTO OUTFILE &#39;/Users/BB/customer.txt&#39;;DATALENGTH ，This function returns the number of bytes used to represent any expression.mysql SUBSTR function The forms with a len argument return a substring len characters long from string str, starting at position pos.If it’s mostly East Asian script (such as Korean, Chinese, and Japanese), each character requires 3 bytes with UTF-8 and 2 bytes with UTF-16. Using UTF-16 provides storage benefits.What is the difference between utf8mb4 and utf8 charsets in MySQL? However, MySQL’s encoding called “utf8” (alias of “utf8mb3”) only stores a maximum of three bytes per code point.So if you want your column to support storing characters lying outside the BMP (and you usually want to), such as emoji, use “utf8mb4”.pri key vs uni key mysql wb shortcutHow to take backup of a single table in a MySQL database?How to dump a remote database using mysqldumpmysql Client OptionsA Quick Guide to Using the MySQL Yum RepositoryUse mysqldump to Backup and Restore a MySQL Database常见问题 副本时区与master不同 Otherwise, statements depending on the local time on the source are not replicated properly, such as statements that use the NOW() or FROM_UNIXTIME() functions.Replication and Time ZonesClustered and Secondary IndexesGenerated Invisible Primary Keys配置innodb_adaptive_hash_index_parts If there are numerous threads waiting on rw-latches created in btr0sea.c, consider increasing the number of adaptive hash index partitions or disabling the adaptive hash index.innodb_log_buffer_size The size in bytes of the buffer that InnoDB uses to write to the log files on disk. The default is 16MB. A large log buffer enables large transactions to run without the need to write the log to disk before the transactions commit.innodb_redo_log_capacity Increase the size of your redo log files. When InnoDB has written redo log files full, it must write the modified contents of the buffer pool to disk in a checkpoint. Small redo log files cause many unnecessary disk writes.wait_timeout The number of seconds the server waits for activity on a noninteractive connection before closing it.innodb_write_io_threads The number of I/O threads for write operations in InnoDB.ALTER INSTANCE DISABLE INNODB REDO_LOG You can disable redo logging using the ALTER INSTANCE DISABLE INNODB REDO_LOG statement. This functionality is intended for loading data into a new MySQL instance. Disabling redo logging speeds up data loading by avoiding redo log writes and doublewrite buffering.Intention LocksAn intention exclusive lock (IX) indicates that a transaction intends to set an exclusive lock on individual rows in a table.Before a transaction can acquire an exclusive lock on a row in a table, it must first acquire an IX lock on the table.shared lockA kind of lock that allows other transactions to read the locked object, and to also acquire other shared locks on it, but not to write to it. The opposite of exclusive lock.innodb_io_capacitySet the innodb_io_capacity parameter to the approximate number of I/O operations that the system can perform per second. Ideally, keep the setting as low as possible, but not so low that background activities slow down. If the setting is too high, data is removed from the buffer pool and insert buffer too quickly for caching to provide a significant benefit.https://docs.oracle.com/javase/6/docs/api/java/sql/Connection.html#isValid%28int%29–log-bin[=base_name]disable-log-binSET GLOBAL TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;SHOW INNODB STATUS walk-throughA Comprehensive (and Animated) Guide to InnoDB LockingConfiguring InnoDB I/O CapacityTransaction Isolation LevelsInnoDB LockingOptimizing InnoDB Redo LoggingOptimizing INSERT StatementsOptimizing InnoDB Transaction ManagementConfiguring InnoDB Buffer Pool SizeMySQL Performance Tuning GuideUsing NFS with MySQLNetAppPrepare MySQL for a Safe Shutdown" }, { "title": " ElasticSearch", "url": "/posts/ElasticSearch/", "categories": "elastic", "tags": "elastic", "date": "2022-12-13 14:54:29 +0800", "snippet": "概念 我们使用的术语 对象 和 文档 是可以互相替换的shard = hash(routing) % number_of_primary_shardsrouting 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置在 hits 数组中每个结果包含文档的 _index 、 _type 、 _id ，加上 _source 字段。这意味着我们可以直接从返回的搜索结果中使用整个文档。_shards 部分告诉我们在查询中参与分片的总数一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。同时你可以使用 from 和 size 参数来分页：索引是由段（Segment）组成的，段存储在硬盘（Disk）文件中，段不是实时更新的，这意味着，段在写入磁盘后，就不再被更新curl -X GET &#39;http://localhost:9200/lsd-_0/_search?q=filebeat_fields_source:nginx&amp;amp;pretty&#39;Lucene 的段是分别存储到单个文件中的。因为段是不可变的，这些文件也都不会变化，这是对缓存友好的，同时操作系统也会把这些段文件缓存起来，以便更快的访问。# 硬盘df -hlsblk一个 Elasticsearch 集群可以 包含多个 索引 ，相应的每个索引可以包含多个 类型 。 这些不同的类型存储着多个 文档 ，每个文档又有 多个 属性Elasticsearch 默认按照相关性得分排序索引 —— 保存相关数据的地方。 索引实际上是指向一个或者多个物理 分片 的 逻辑命名空间 。倒排索引包含一个有序列表，列表包含所有文档出现过的不重复个体，或称为 词项 ，对于每一个词项，包含了它所有曾出现过文档的列表一个 Lucene 索引 我们在 Elasticsearch 称作 分片 。 一个 Elasticsearch 索引 是分片的集合。 当 Elasticsearch 在索引中搜索的时候， 他发送查询到每一个属于索引的分片(Lucene 索引)，然后像 执行分布式检索 提到的那样，合并每个分片的结果到一个全局的结果集。文档更新和删除也会导致大量的合并数，因为它们会产生最终需要被合并的段 碎片映射, 就像数据库中的 schema ，描述了文档可能具有的字段或 属性 、每个字段的数据类型—比如 string, integer 或 date —以及 Lucene 是如何索引和存储这些字段的。docker run -d -p 9201:9200 -p 9301:9300 --name=es02 -e bootstrap.memory_lock=true -e ES_JAVA_OPTS=&quot;-Xms2g -Xmx2g&quot; -v /var/lib/es02:/var/lib/elasticsearch -v /var/log/es02:/var/log/elasticsearch -v /root/es02.yml:/usr/share/elasticsearch/config/elasticsearch.yml --cap-add=IPC_LOCK --ulimit nofile=65536:65536 --ulimit memlock=-1:-1 docker.elastic.co/elasticsearch/elasticsearch:6.7.2docker run -d -p 9200:9200 -p 9300:9300 -e bootstrap.memory_lock=true -e ES_JAVA_OPTS=&quot;-Xms2g -Xmx2g&quot; -v /var/lib/elasticsearch:/var/lib/elasticsearch -v /var/log/elasticsearch:/var/log/elasticsearch -v /root/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml --cap-add=IPC_LOCK --ulimit nofile=262144:262144 --ulimit memlock=-1:-1 docker.elastic.co/elasticsearch/elasticsearch:6.7.2 transport.bind_host, which the node should bind in order to listen for incoming transport connectionstransport.publish_host, which the node can be contacted by other nodes写入 index， es 写入 translog 和 in-memory, _refresh 将缓存区写入 lucene Segment(内存), _flush 将 Segment 写入 disk。POST /_cluster/reroute?retry_failed=true 重启重试，5 次失败后会停止POST /_flush/synced 自动分配POST /_cluster/allocation/explain?pretty 分配解析POST /_cluster/settings 开启自动分配{ &quot;persistent&quot;: { &quot;cluster.routing.allocation.enable&quot;: &quot;all&quot; }}kopfdocker run --name=kopf \\ --hostname=kopf \\ -p 80:80 \\ --env=KOPF_SERVER_NAME=grafana.dev \\ --env=KOPF_ES_SERVERS=192.168.20.95:9200 \\ --restart=always \\ lmenezes/elasticsearch-kopfcerebro适用于 es 2.0+ 之后# create brige networkdocker network create es# run cerebrodocker run -d --network=es --name=cerebro -e http.port=80 -p 80:80 lmenezes/cerebro脑裂split-brain(master_eligible_nodes / 2) + 1set 2， 3个有资格成为master的node，当其中一个master node 宕机， 剩余两个node将选举出新的master。vmtest docker run 2 master， graylog-srv 也是 master-node， 当vmtest宕机，cluster 将无法run，因为没有足够的master-eligiable node（至少2）考虑将 minimum_master_nodes =1, vmtest es01 作为 master_eligible_node ,es02 只作为 ingest，vmtest 暂不store data。iptables failedinstall k8s 后 docker container exit 之后无法restart&#39;python-nftables&#39; failed: internal:0:0-0: Error: Could not process rule: No such file or directorysystemctl restart docker 无法重启reboot ,OKdocker run ，没有日志写入file config/log4j2.properties in the docker image only enables console logging. Docker makes container’s console logs available via docker logs.即使container stop, 也可以查看 docker logs参考这里runlikeKubernetes-elasticsearch-nfs 集群部署How to Delete Elasticsearch Unassigned Shards in 4 Easy Stepsautodiscover" }, { "title": "kubernetes install", "url": "/posts/kubernetes-install/", "categories": "kubernetes", "tags": "kubernetes", "date": "2022-11-07 11:59:04 +0800", "snippet": "kubectl常见命令#集群健康kubectl get cs#创建deployment 以及servicekubectl create deployment demo --image=httpd --port=80kubectl expose deploy [deployName] --name=xxx --type=NodePort#services Endpointskubectl get svc,ep#重启depolymentkubectl rollout restart deployment nginx-deployment#标签，日志输出kubectl logs -l app=lsd-inventory -n lesaunda#以机器可读的方式列举隶属于某 Job 的全部 Podpods=$(kubectl get pods --selector=job-name=pi --output=jsonpath=&#39;{.items[*].metadata.name}&#39;)echo $podskubectl logs $pods# scale deploymentkubectl scale deployment/xxxx --replicas=5 -n lesaunda# find out default namespacekubectl config view --minify --output &#39;jsonpath={..namespace}&#39;; echo# set default namespacekubectl config set-context --current --namespace=&amp;lt;NAME&amp;gt;# restart daemonsetkubectl rollout restart daemonset filebeat -n kube-systemkubectl rollout status ds/filebeat -n kube-systemkubectl create configmap xx_config --from-file=.env# copy configmapkubectl get configmap &amp;lt;secret-name&amp;gt; --namespace=&amp;lt;source-namespace&amp;gt; -o yaml \\ | sed &#39;s/namespace: &amp;lt;from-namespace&amp;gt;/namespace: &amp;lt;to-namespace&amp;gt;/&#39; \\ | kubectl create -f -# Let’s check for the expiration date of a specific certificateopenssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text | grep &#39;Not After&#39;find /etc/kubernetes/pki/ -type f -name &quot;*.crt&quot; -print | grep -v &#39;ca.crt$&#39; | xargs -L 1 -t -i bash -c &#39;openssl x509 -noout -text -in {} | grep &quot;Not After&quot;&#39;kubeadm certs check-expirationkubeadm certs renew all# After renewing the certificates, we need to restart the control plane components, including the API server, controller manager, and scheduler, to use the new certificates.$ sudo systemctl restart kubelet$ kubectl -n seatunnel cp ./xxx.jar seatunnel-0:/opt/seatunnel/lib -c seatunnel# remove all evicted podskubectl get pod -n airflow| grep Evicted | awk &#39;{print $1}&#39; | xargs kubectl delete pod -n airflowyaml block hostAliases```yaml hostAliases: ip: “192.168.1.10” hostnames: “foo.local” “bar.local”``` localhost:8080 was refused docker ps | grep kube-apiservermkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 参考The connection to the server localhost:8080 was refused - did you specify the right host or port? kubeadmkubeadm init# centossudo yum install kubelet-1.25.3 kubeadm-1.25.3 kubectl-1.25.3 --nogpgcheck -y# ubuntusudo apt-get updatesudo apt-get install -y apt-transport-https ca-certificatescurl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpgcurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key addsudo apt install -y kubelet=1.25.3-00 kubeadm=1.25.3-00 kubectl=1.25.3-00为了保证 kubelet 正常工作，你必须禁用交换分区,关闭 selinux 与防火墙,配置 kubernetes /etc/modules-load.d/k8s.confkubeadm init \\--kubernetes-version=v1.25.3 \\--image-repository=registry.aliyuncs.com/google_containers \\--pod-network-cidr=10.244.0.0/16 \\--control-plane-endpoint=kube-apiserver \\--cri-socket unix:///run/containerd/containerd.sock \\--v=5kubeadm init 之前存在给定的证书和私钥对，kubeadm 将不会重写它们Kubeadm 对集群所有的节点，使用相同的 KubeletConfiguration执行 init、join 和 upgrade 等子命令会促使 kubeadm 将 KubeletConfiguration 写入到文件 /var/lib/kubelet/config.yaml 中， 继而把它传递给本地节点的 kubelet。kubeadm certs check-expirationkubeadm 将 kubelet 配置为自动更新证书kubeadm join# 生成加入集群命令kubeadm token create --print-join-command# 加入集群kubeadm join kube-apiserver:6443 \\--token i7guj6.at1q41k4bdgi0gbg \\--discovery-token-ca-cert-hash sha256:fc2a4732767762ea293ff2c6da67162bcc27e9850fc4919e769b0215c300d856 \\--cri-socket unix:///run/containerd/containerd.sock \\--v=8# ubuntu 16.04 kubernetes v1.25.3 需要 install cri-dockerd --cri-socket /run/cri-dockerd.sock# 将控制平面证书（ca, etcd, api....）上传到集群，并下载解密keykubeadm init phase upload-certs --upload-certs --v=5# -certificate-key 下载并解密kubeadm join kube-apiserver:6443 \\--token 0q5r70.53fggw68nta4697x \\--discovery-token-ca-cert-hash sha256:fc2a4732767762ea293ff2c6da67162bcc27e9850fc4919e769b0215c300d856 \\--certificate-key 27f45c69543808830fb2c59c2dd110a7e010766dd1e5a1ea9a956ef58a194bd0 \\--cri-socket unix:///run/containerd/containerd.sock \\--control-plane \\--v=8问题汇总 pull image kubeadm config images listkubeadm config images pull --cri-socket=unix:///run/containerd/containerd.sock --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.25.3 cri sandbox image pause pod 所有容器的父容器 kubelet call runsandbox 和 linux namespace 以及 cgroup 申领 network namespace等相关资源配额,pause start 之后将一直阻塞,remove pause first and remove containers in it. cri 比如 containerd 以哪个版本的pause作为sandbox可在/etc/containerd/config.toml中设置sandbox_image, default registry.k8s.io/pause systemctl status kubelet 状态 Actived，正常，如 InActivce, systemctl enable kubelet journalctl -xeu kubectl --no-pager 查看日志，发现failed to pull image [registry.k8s.io/pause:3.6 -o pause.tar] 拉取，tag，import crictl imagesdocker pull registry.aliyuncs.com/google_containers/pause:3.6docker tag registry.aliyuncs.com/google_containers/pause:3.6 registry.k8s.io/pause:3.6docker save registry.k8s.io/pause:3.6 -o pause.tarctr -n k8s.io images import pause.tarctr -n k8s.io i tag registry.aliyuncs.com/google_containers/pause:3.6 registry.k8s.io/pause:3.6 failed to reserve sandbox name apiserver 访问 container runtime interface 失败 vim /etc/containerd/config.toml# disabled_plugins = [&quot;cri&quot;]systemctl daemon-reloadsystemctl restart containerd kubelet invalid slice name kubelet configuration –cgroup-driver string Default: cgroupfs 查看 /var/lib/kubelet/config.yaml cgroupDriver: systemd 查看 /etc/containerd/config.toml 或 containerd config default systemd_cgroup = false 改为true, restart containerd 如使用 docker + cri-dockerd 配置cgroupfs，查看/etc/docker/daemon.json { &quot;exec-opts&quot;: [&quot;native.cgroupdriver=cgroupfs&quot;], &quot;insecure-registries&quot; : [&quot;registry.xxxx.com.cn&quot;]} restart docker,kubelet alter node Role kubectl get no master -o wide --show-labels kubectl get nodes kubectl label no ubuntucache-srv node-role.kubernetes.io/control-plane= 参考k8s 中节点 node 的 ROLES 值是 none /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1 执行echo &quot;1&quot; &amp;gt;/proc/sys/net/bridge/bridge-nf-call-iptables 提示 No such file or directory, 没有 bridge folder ，要enable br_netfilter modprobe br_netfilter， 再 sysctl net.bridge.bridge-nf-call-iptables=1， 同样 sysctl net.ipv4.ip_forward=1 kubeadm join –control-plane lack of etcd 工作流 加入节点有两种方式 节点发现 （-token + -discovery-token-ca-cert-has + api：port） token 为共享令牌，-discovery-token-ca-cert-has 为验证master ca证书 是否一致（–discovery-token-unsafe-skip-ca-verification 跳过，不建议）， 验证通过后将建立tls引导，提交CSR，签名，控制面返回确定标识，kubeadm 配置kubelet ，通信 如果是加入控制面， 下载共享证书+ 静态pod清单+ kubeconfig +etcd 以文件形式提供标准 kubeconfig 文件的一个子集 分段执行 kubeadm join phase kubelet-start --config=config,yaml kubeadm join phase kubelet-start --token=xx -discovery-token-ca-cert-has=xxxx 如果使用 –upload-certs 调用 kubeadm init 命令， 你也可以对控制平面节点调用带 –certificate-key 参数的 join 命令， 将证书复制到该节点。 failed to publish local member to cluster through raft disable firewall # ubuntusudo ufw disable# centossystemctl stop firewalld.service kubeadm reset , Failed to unmount mounted directory in /var/lib/kubelet/ umount &amp;lt;path&amp;gt;rm -rf &amp;lt;filePath&amp;gt; cert openssl genrsa -out ca.key 2048openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=kube-apiserver&quot; -days 10000 -out ca.crtopenssl genrsa -out server.key 2048 CoreDNSkubectl run -it --image=busybox:1.28.3 --rm --restart=Never bash -n kube-systemnslookup kubernetes.default:: no server could be reached.kubectl get ep kube-dns -n kube-system -o json |jq -r &quot;.subsets&quot;coredns 服务/pod 正常，但创建的 pod 无法 nslook 服务，coredns configmap 加 log 也检测不到，反而 master node 上dig @10.96.0.10 kubernetes.default.svc.cluster.local +noall +answer 可以找到域名的 ip,无果，删除 dns pod 后自动创建，恢复正常。怀疑与期间修改 flannel subnet 文件以及 cni0 网络相关,尝试 改 ipv4 允许转发,busybox image 应&amp;lt;=1.28.3, 大于此版本可能出现 No Answer 问题2023-05-23 补充，node2 存在多个 bridge network, ping 192.168.20.xx 时 出现ping 192.168.20.xxFrom 192.168.16.1 icmp seq =1 destination host unreachableFrom 192.168.16.1 icmp seq =1 destination host unreachable# 查看iptables 规则,没有发现异常iptables -L -niptables --table nat --list# firewall status inactivesystemctl status firewalld.service# 安装bind_utils工具，包括nslookup, digyum install bind_utils# dig &amp;lt;ip&amp;gt;, nslookup ip1 node2 3000ms !H# ip a 或者 ifconfig 或 安装 bridge-utils, 看到bridge network 关联 192.168.16.1brctl show# deleteip link set &amp;lt;bridge_name&amp;gt; downip link del &amp;lt;bridge_name&amp;gt;# 或者nmclinmcli con shownmcli connecion down &amp;lt;bridge_name&amp;gt;# ping ok# proxy 无法access kube-apiserver no such host , 8.8.8.8 resolv, 修改 本机和pod /etc/resolv.conf 重启会被自动覆盖# install resolvconf， 配置 /etc/resolvconf/resolv.conf.d/head亦无法修复此问题systemd-resolve --statussudo systemctl status systemd-resolved.servicesudo cat /run/systemd/resolve/resolv.conf# 修改 resolved.confsudo vim /etc/systemd/resolved.conf[Resolve]FallbackDNS=192.168.1.xxx 192.168.1.xxxDomains=xxx.com.cncrictl参考使用 crictl 对 Kubernetes 节点进行调试InstallVERSION=&quot;v1.26.0&quot;wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gzsudo tar -zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/binrm -f crictl-$VERSION-linux-amd64.tar.gzcrictl pscrictl logs &amp;lt;container_id&amp;gt; 2&amp;gt;filenamecrictl imagescrictl inspect [container_id]crictl inspecti [image_id]crictl inspectp [pod_id]ctr --namespace=k8s.io image tag registry.aliyuncs.com/google_containers/pause:3.6 registry.k8s.io/pause:3.6vim /etc/crictl.yamlruntime-endpoint: unix:///run/containerd/containerd.sockimage-endpoint: unix:///run/containerd/containerd.socktimeout: 0debug: true #调试pull-image-on-create: falsedisable-pull-on-run: false$ crictl psI0605 14:56:16.720887 254196 util_unix.go:103] &quot;Using this endpoint is deprecated, please consider using full URL format&quot; endpoint=&quot;/run/containerd/containerd.sock&quot; URL=&quot;unix:///run/containerd/containerd.sock&quot;$ crictl config --set runtime-endpoint=unix:///run/containerd/containerdetcd由 kubeadm 生成证书，kube-scheduler 调度 kubelet create etcd static pod ,清单见/etc/kubernetes/manifests/etcd.yaml分段执行kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yamlconfig file 参考 kubeadm Configurationetcdctl连接集群etcd拓扑结构分stacked(堆叠)和外部stacked 即 control-plane + etcd，节省资源donwload etcdctl# 集群成员/tmp/etcd-download-test/etcdctl \\--endpoints=172.21.14.2xx:2379,kube-apiserverx:2379 \\--cert /etc/kubernetes/pki/etcd/peer.crt \\--key /etc/kubernetes/pki/etcd/peer.key \\--cacert /etc/kubernetes/pki/etcd/ca.crt \\member list# 集群健康/tmp/etcd-download-test/etcdctl \\--endpoints=172.21.14.2xx:2379,kube-apiserverx:2379 \\--cert /etc/kubernetes/pki/etcd/peer.crt \\--key /etc/kubernetes/pki/etcd/peer.key \\--cacert /etc/kubernetes/pki/etcd/ca.crt \\endpoint health# 移除成员/tmp/etcd-download-test/etcdctl \\--endpoints=172.21.14.2xx:2379,kube-apiserverx:2379 \\--cert /etc/kubernetes/pki/etcd/peer.crt \\--key /etc/kubernetes/pki/etcd/peer.key \\--cacert /etc/kubernetes/pki/etcd/ca.crt \\member remove ${MEMBER_ID}# 添加成员/tmp/etcd-download-test/etcdctl \\--endpoints=172.21.14.2xx:2379,kube-apiserverx:2379 \\--cert /etc/kubernetes/pki/etcd/peer.crt \\--key /etc/kubernetes/pki/etcd/peer.key \\--cacert /etc/kubernetes/pki/etcd/ca.crt \\member add ${hostname} --peer-urls=https://${ip}:2380join master 过程中出现 etcd 出现 no leader 问题 和 tls connecion refused ,与证书有关，不要轻易改动 /etc/kubernetes/pki 目录，改的话要先备份。假设集群不健康，无法通过常规方式restart，可尝试命令行的方式单独run etcd as a new cluster,风险大# Force to create a new one-member cluster.--force-new-cluster &#39;true&#39;利用备份文件和 etcd.yaml 在 kubernetes 外重新搭建 etcd 实例,不验证 auth，apiserver 的 yaml 文件中 etc listen url 也相应改为 http://127.0.0.1:2379/tmp/etcd-download-test/etcd \\--listen-client-urls=http://127.0.0.1:2379 \\--advertise-client-urls=http://127.0.0.1:2379 \\--client-cert-auth=false \\--peer-client-cert-auth=false \\--name=master \\--initial-cluster=master=http://127.0.0.1:2380 \\--initial-advertise-peer-urls=http://127.0.0.1:2380 \\--data-dir=/var/lib/etcd_bak \\--initial-cluster-state=new \\--force-new-cluster=&#39;true&#39;参考手动建 etcd 集群Configuration optionsHow to Add and Remove Members如何安装 etcd 和 etcdctlruncwget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64sudo install -m 755 runc.amd64 /usr/local/sbin/runccni容器运行时必须配置为加载所需的 CNI 插件，从而实现 Kubernetes 网络模型docker default cni-plugin is bridge查看 /etc/cni/net.d/cni-pluginsudo mkdir -p /opt/cni/bin/sudo wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgzsudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz flannel plugin install curl -o kube-flannel.yml https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.ymldocker pull quay.io/coreos/flannel:v0.14.0kubectl apply -f kube-flannel.yml k8s create daemonset in node,相当于install flannel-cni-plugin , 查看/run/flannel/subnet.env和/etc/cni/net.d Flannel.1: overlay网络的设备，用来进行 vxlan 报文的处理（封包和解包）。不同node之间的pod数据流量都从overlay设备以隧道的形式发送到对端。 Cni0 :网桥设备，每创建一个pod都会创建一对 veth pair。其中一端是pod中的eth0，另一端是Cni0网桥中的端口（网卡）。Pod从网卡eth0发出的流量都会发送到Cni0网桥设备的端口（网卡）上。 pod发数据，发到cni0, 再到flannel，隧道形式发送对端flannel，flannel 解释数据 ，发cni0, 再到pod, ip route查看 参考 扁平网络 Flannel kubernetes之flannel 网络分析 问题汇总 open /run/flannel/subnet.env no such file or directory 新建 /run/flannel/subnet.env 文件 FLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.0.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=true kube-flannel pod 挂载了/run/flannel ，不同的 node，FLANNEL_SUBNET 会有所不同10.244.1.1/24,10.244.2.1/24 k8s 安装 flannel 报错“node “master” pod cidr not assigned” Kubeadm Init 的时候，没有增加 –pod-network-cidr 10.244.0.0/16 kubectl patch node gz-vmtest -p ‘{“spec”:{“podCIDR”:”10.244.5.0/16”}}’ apply pod 时出现 failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.1.1/24 del cni0 flannel1.1 ,stop &amp;amp; remove kube-flannel pod, restart ifconfig cni0 down ifconfig flannel.1 down ip link delete cni0 ip link delete flannel.1 ifconfig cni0 ifconfig flannel.1 ip link add cni0 type bridge ip link set dev cni0 up ifconfig cni0 10.244.0.1 ifconfig cni0 mtu 1450 up CRI 容器运行时kubelet 通过 cri-socket 调用 (早前dockershim 1.24后弃用，没有实现CRI标准)cri-dockerd/containerd create sandbox, run containers in it.如果单独install containerd.io , 需install runc（cgroup 相关）以及 cni(k8s 有default )containerdcontainerd plugin ls 列出 containerd 插件yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum install docker-ce docker-ce-cli containerd.io docker-compose-pluginsudo yum install containerdsystemctl enable containerdsystemctl start containerdvim /etc/containerd/config.toml注释,开放接口给kubelet创建管理container#disabled_plugins = [“cri”]或者sudo sed -i &#39;s/^disabled_plugins \\=/\\#disabled_plugins \\=/g&#39; /etc/containerd/config.tomlsandbox_image = &quot;registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8&quot; # 修改为阿里云镜像地址SystemdCgroup = true # 使用systemd cgroup镜像version = 1[plugins] [plugins.cri] [plugins.cri.registry] [plugins.cri.registry.mirrors] [plugins.cri.registry.mirrors.&quot;registry.k8s.io&quot;] endpoint = [&quot;https://registry.cn-hangzhou.aliyuncs.com/google_containers&quot;] [plugins.cri.registry.mirrors.&quot;docker.io&quot;] endpoint = [&quot;https://registry.cn-hangzhou.aliyuncs.com&quot;]version = 2[plugins.&quot;io.containerd.grpc.v1.cri&quot;] sandbox_image = &quot;http://registry.aliyuncs.com/google_containers/pause:3.8&quot;[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;] endpoint = [&quot;https://registry.docker-cn.com&quot;] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.k8s.io&quot;] endpoint = [&quot;http://registry.aliyuncs.com/google_containers&quot;] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;registry.lesaunda.io&quot;] endpoint = [&quot;http://192.168.1.193:5000&quot;]containerd v1.2 前 使用 配置语法1.3之后使用 配置语法如阿里云timeout,参考这里,改 http://hub-mirror.c.163.comsystemctl daemon-reloadsystemctl enable --now containerd 守护进程systemctl restart containerddocker enginevim /etc/docker/daemon.json{ &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;insecure-registries&quot;: [&quot;172.21.14.206:5000&quot;], &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com&quot;]}systemctl daemon-reloadsystemctl restart dockerDashboard安装kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml修改 service 成 nodeport，外网访问kubectl describe pod/kubernetes-dashboard#Normal Scheduled 42m default-scheduler Successfully assigned kubernetes-dashboard/&amp;gt;#kubernetes-dashboard-5c8bd6b59-xxvkk to node1kubectl cluster-infoKubernetes control plane is running at https://kube-apiserver:6443CoreDNS is running at https://kube-apiserver:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxykubectl proxy# node1-&amp;gt; 172.21.14.208https://172.21.14.208:30423/#/login参考Accessing Dashboard nodeport ,token 登陆 按k8s 优秀的 web 管理界面-kubernetes Dashboard apply kubectl get secret -n kubernetes-dashboard 没有找到 token 1.23 以上执行 创建 serviceaccount ```yamlapiVersion: v1kind: ServiceAccountmetadata: name: admin namespace: kubernetes-dashboard apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin namespace: kubernetes-dashboard `kubectl -n kubernetes-dashboard create token admin` 生成 token## 污点 node查看节点污点`kubectl get nodes -o=custom-columns=NodeName:.metadata.name,TaintKey:.spec.taints[*].key,TaintValue:.spec.taints[*].value,TaintEffect:.spec.taints[*].effect`NodeName TaintKey TaintValue TaintEffectgz-k8ssrv-master1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;lsd-server-01 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;node1 node-role.kubernetes.io/control-plane &amp;lt;none&amp;gt; NoSchedulenode2 node-role.kubernetes.io/control-plane &amp;lt;none&amp;gt; NoScheduleubuntucache-srv node-role.kubernetes.io/control-plane &amp;lt;none&amp;gt; NoSchedule`kubernetes 提示 1 node(s) had taints that the pod didn&#39;t tolerate` 表示 节点污点，pod不能容忍，所以不部署在此节点&amp;gt; NoSchedule,The Kubernetes scheduler will only allow scheduling pods that have tolerations for the tainted nodes.&amp;gt; PreferNoSchedule,The Kubernetes scheduler will try to avoid scheduling pods that don’t have tolerations for the tainted nodes.&amp;gt; NoExecute,Kubernetes will evict the running pods from the nodes if the pods don’t have tolerations for the tainted nodes.```bashkubectl taint nodes node1 key1=value1:NoSchedule# removekubectl taint nodes node1 key1=value1:NoSchedule-apiVersion: v1kind: Podmetadata: name: nginx labels: env: testspec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent tolerations: - key: &quot;example-key&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot;pod 将在 存在污点 key=example-key 而且 effect = NoSchedule 的节点上部署operator 的默认值是 Equalkubectl taint nodes --all node-role.kubernetes.io/master-kubectl label nodes ubuntucache-srv node.kubernetes.io/exclude-from-external-load-balancers=kubectl taint node ubuntucache-srv node-role.kubernetes.io/control-plane=:NoSchedule非体面关闭时 ，statefulSet 无法创建同名pod，要手动添加污点，分离卷操作kubectl taint node ubuntucache-srv node.kubernetes.io/out-of-service=:NoSchedule在添加 node.kubernetes.io/out-of-service 污点之前， 应该验证节点已经处于关闭或断电状态（而不是在重新启动中）。将 Pod 移动到新节点后，用户需要手动移除停止服务的污点， 并且用户要检查关闭节点是否已恢复，因为该用户是最初添加污点的用户。参考kubernetes 提示 1 node(s) had taints that the pod didn’t tolerate污点Kubernetes Taints &amp;amp; TolerationsnodeSelectornode 打labelkubectl label node1 dedicated=test# remove lablekubectl label node1 dedicated-apiVersion: v1kind: Podmetadata: name: nginx labels: env: testspec: containers: - name: nginx image: registry.lesaunda.com.cn/busybox command: - sleep - &quot;3600&quot; imagePullPolicy: IfNotPresent nodeSelector: dedicated: testToolsyum -y install net-toolsPVCnfs 卷能将 NFS (网络文件系统) 挂载到你的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，nfs 卷的内容在删除 Pod 时会被保存，卷只是被卸载。若 pvc status = Terminating打补丁kubectl -n redis-cluster patch pvc data-redis-cluster-0 -p &#39;{&quot;metadata&quot;:{&quot;finalizers&quot;:null}}&#39;ingrees-nginx部署kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.5.1/deploy/static/provider/cloud/deploy.yaml利用pod sepc 反亲和性 确保每个node只有一个ingress-nginx引用 Question: nginx - multiple replicas or daemonSet? Using a deployment with an anti-affinity rule to avoid multiple replicas in the same node is, in most of the cases, more than enough.以及参考 将 Pod 指派给节点affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - ingress-nginx topologyKey: &quot;kubernetes.io/hostname&quot;配置group, 捕获第n个组apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: sysinfo-ingress namespace: lesaunda annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2spec: ingressClassName: nginx rules: - http: paths: - pathType: Prefix path: /sysinfo/api(/|$)(.*) backend: service: name: ls-sysinfo port: number: 3000正则匹配，保留 /api/v1/staffsannotations: nginx.ingress.kubernetes.io/use-regex: &#39;true&#39;spec: ingressClassName: nginx rules: - http: paths: - path: /api/v\\d+/(staffs|accounts) pathType: Prefix backend: service: name: lsd-staff-services port: number: 80问题 镜像问题 导出文件 curl -o ingress-nginx-deploy-backup.yaml https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.5.1/deploy/static/provider/cloud/deploy.yaml register.k8s.io 无法访问，阿里云镜像代替，pull 之后修改 deployment.yaml docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v20220916-gd32f8c343docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v1.5.1 apply 即可 Internal error occurred: failed calling webhook “validate.nginx.ingress.kubernetes.io” $ kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admissionvalidatingwebhookconfiguration.admissionregistration.k8s.io &quot;ingress-nginx-admission&quot; deleted 参考Kubernetes: nginx ingress controller - failed calling webhook 在 Minikube 环境中使用 NGINX Ingress 控制器配置 Ingress tcp 转发 $kubectl -n ingress-nginx get cmtcp-services 1 191d$kubectl -n ingrees-nginx edit cm tcp-servicesdata: &quot;5672&quot;: rabbitmq-system/lsd-rabbitmq:5672 413 request entity too large 上传文件，nginx 转发，size 超过 limit查看kubectl exec -n ingress-nginx &amp;lt;ingress pod&amp;gt; cat nginx.conf|grep client_max_body_sizeclient_max_body_size 1m default 1m修改kubectl -n ingress-nginx edit cm ingress-nginx-controller data: proxy-body-size: 10m相关配置可以参考 Kubernetes NGINX Ingress: 10 Useful Configuration Optionsredis cluster参考k8s 中部署 redis 集群(三主三从)获取 pods ip 地址kubectl get pods -l app=redis-cluster -n redis-cluster -o jsonpath=&#39;{range.items[*]}{.status.podIP}:6379 &#39;redis-cli -a [password] –cluster create 10.244.2.89:6379 10.244.2.90:6379 10.244.0.20:6379 10.244.2.91:6379 10.244.0.21:6379 10.244.2.92:6379 –cluster-replicas 1所有pods down, 要手动delete rdb file, aof file, nodes.conf ，redis-cli 执行 flushdb, cluster reset,比较复杂，采用下面方式重建如果所有 pod 下线，需要重新建群$ kubectl delete -f redis-cluster.yml # 删除资源$ for i in {0..5}; do kubectl delete pvc/data-redis-cluster-$1 -n redis-cluster; done; # 删除pvc$ for i in {0..5}; do rm -rf /root/nfs_root/redis-cluster-data-redis-cluster-$i; done; # 删除nfs$ kubectl apply -f redis-cluster.yml # 部署资源$ kubectl get pods -l app=redis-cluster -n redis-cluster -o jsonpath=&#39;{range.items[*]}{.status.podIP}:6379 &#39; # 获取ip$ kubectl -n redis-cluster exec -it redis-cluster-0 /bin/sh # 进入pod$ redis-cli -a password --cluster create 10.244.2.89:6379 10.244.2.90:6379 10.244.0.20:6379 10.244.2.91:6379 10.244.0.21:6379 10.244.2.92:6379 --cluster-replicas 1 # build cluster或者(验证过，无法build cluster,要 delete rdb file, aof file, nodes.conf ，redis-cli 执行 flushdb, cluster reset) kubectl delete -f redis-cluster.yml 删除资源 rm -f /ifs/kubernetes/redis-cluster-data-redis-cluster-0*/nodes.conf* 逐一删除 nodes.conf kubectl apply -f redis-cluster.yml 重新建立集群for i in {0..5}; do rm -rf /root/nfs_root/redis-cluster-data-redis-cluster-$i; done;for i in {0..5}; do kubectl delete pvc/data-redis-cluster-$1 -n redis-cluster; done;redisinsight参考Install RedisInsight on KubernetesLoadBalancer 要实现 type: LoadBalancer 的服务，Kubernetes 通常首先进行与请求 type: NodePort 服务等效的更改。 cloud-controller-manager 组件然后配置外部负载均衡器以将流量转发到已分配的节点端口。访问 http://172.21.14.208:30738/mysql clusterInstall using Manifest Files start failed in pod mysqlcluster-2_mysql(7059d32c-bfd3-4d75-9155-5cb1c1dcc2bc): ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image “container-registry.oracle.com/mysql/community-operator:8.0.33-2.0.10”: failed to read expected number of bytes: unexpected EOFctr -n k8s.io i pull container-registry.oracle.com/mysql/community-operator:8.0.33-2.0.10# 部分layer 下载非常慢，在其他node push image，再pull,同样慢，layer 指定了url，去掉 -n k8s.io ，下载再加上即可# 删除 会stuck ，+ forcekubectl -n mysql delete pod mysqlcluster-2 --forcesystemctl restart kubelet修改时区MySQL Set UTC time as default timestampSELECT @@global.time_zone, @@session.time_zone;SELECT CURRENT_TIMESTAMP();SET @@session.time_zone=&#39;+08:00&#39;;集群-永久性生效kubectl -n mysql edit cm mysqlcluster-initconfmycnf: | [mysqld] max_connections=162 default_time_zone=&#39;+08:00&#39;kubectl -n mysql scale statefulSet mysqlcluster --replicas=0kubectl -n mysql scale statefulSet mysqlcluster --replicas=3修改 Control-Plane-Endpointapi service 做 HA 高可用kubeadm init \\--kubernetes-version=v1.25.3 \\--image-repository=registry.aliyuncs.com/google_containers \\--pod-network-cidr=10.244.0.0/16 \\--control-plane-endpoint=&amp;lt;LB address or commonname&amp;gt; \\--cri-socket=unix:///run/containerd/containerd.sock \\--v=9如果想从non-HA Convert to HA ,非常复杂，可以参考 Converting Kubernetes to an HA Control Plane保留etcd ，reinit 的方法不可行，将丢失init的node，考虑将ip指向vm，再vm上做LBInstall as node mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak $ wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo$ yum clean all$ yum makecache yum update network，安装和配置先决条件 关闭防火墙 systemctl disable firewall.service setenforce 0 #关闭安全模式 # ubuntusudo sed -i.bak &#39;/ swap / s/^\\(.*\\)$/#\\1/g&#39; /etc/fstabsudo systemctl mask swap.target swapoff -a #关闭内存交换,同时修改 /etc/fstab 注释 swap 行 配置docker.repo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo install containerd(preinstall runc and cni plugin), runc centos 7 自带（runc –version 1.1.7）， cni plugin flannel 由 kubelet 调度部署, enable 以及start 配置 /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg yum install kubelet-1.25.3 kubeadm-1.25.3 kubectl-1.25.3 --nogpgcheck -y 修改 /etc/containerd/config.toml 注释disabled_plugins = [“cri”]以及添加registry.k8s.io,docker以及自建镜像库 install cri-tools, 修改vim /etc/crictl.yaml, pull pasue3.6 以及tag kubeadm join …..container-runtime Flag –container-runtime has been deprecated, will be removed in 1.27 as the only valid value is ‘remote’vim /var/lib/kubelet/kubeadm-flags.env 移除 --container-runtime参数HelmHelm 是 Kubernetes 的包管理器install$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3$ chmod 700 get_helm.sh$ ./get_helm.sh安装Helmhelm add repo [name] [url]helm search repo [name]参考通过 kubeadm 安装不同版本的 K8Skubeadm 极速部署 Kubernetes 1.25 版本集群使用 kubeadm 搭建 k8s 1.25.3 版本K8S 部署 Redis Cluster 集群（三主三从模式） - 部署笔记Community Forums,问答论坛How to Tail Kubernetes Logs: Using the Kubectl Command to See Pod, Container, and Deployment Logskubectl 命令参考kubeadm init phaseDownward API扁平网络 FlannelMultiple databases are not supported on this server; cannot switch to databasenet.ipv4.ip_forward=1调试 DNS 问题解决 Kubernetes 中 Pod 无法正常域名解析问题分析与 IPVS parseIP Error 问题coredns 官网Network bridgeInstallation Guide防火墙规则ingress-nginx tcp/udp 转发，真一文搞定 ingress-nginx 的使用" }, { "title": "GrayLog", "url": "/posts/GrayLog/", "categories": "system", "tags": "grayLog", "date": "2022-10-10 09:59:46 +0800", "snippet": "注意Graylog 3.x does not work with Elasticsearch 7.x!Graylog Sidercar 自带 filebeat, winlogbeat,window installwindow install下载，选择 1.0.2的版本，因为 3.x graylog sidecar max version 1.0.X 静默方式运行graylog_sidecar_installer_1.0.2-1.exe /S -SERVERURL=http://192.168.20.95:9000 -APITOKEN=1s9dtljlbh37j61fo8psuneuq8sntshe7ibco8vhs649c9cf9o2j 双击exe打开安装界面填入信息 查看或修改 C:\\Program Files\\Graylog\\sidecar\\sidecar.yml以服务方式运行&amp;amp; &quot;C:\\Program Files\\graylog\\sidecar\\graylog-sidecar.exe&quot; -service install&amp;amp; &quot;C:\\Program Files\\graylog\\sidecar\\graylog-sidecar.exe&quot; -service start打开 service.msc ,会存在两个Graylog—xxx 的服务，一个sidecar ，一个 collector，即 winlogbeat.注册表Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\graylog-sidecarComputer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\graylog-collector-winlogbeat当前 graylog es 版本 &quot;Elasticsearch&quot; : &quot;6.7.2&quot;Kopfkopf 仅适用于 es2.x 版本docker run -d -p 80:80 -e KOPF_SERVER_NAME=grafana.dev -e KOPF_ES_SERVERS=192.168.20.xx:9200 --name kopf lmenezes/elasticsearch-kopfdocker run --name=kopf --env=KOPF_SERVER_NAME=grafana.dev --env=KOPF_ES_SERVERS=192.168.20.95:9200 --expose=443 -p 8080:80 --restart=always lmenezes/elasticsearch-kopfdocker run -d -p 9200:9200 -p 9300:9300 \\ -v /var/lib/elasticsearch:/var/lib/elasticsearch \\ -v /var/log/elasticsearch:/var/log/elasticsearch \\ --cap-add=IPC_LOCK --ulimit nofile=65536:65536 --ulimit memlock=-1:-1 \\ -e cluster.name=graylog \\ -e node.name=es-01 \\ -e bootstrap.memory_lock=true \\ -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; \\ -e node.master=true \\ -e network.bind_host=0.0.0.0 \\ -e network.host=0.0.0.0 \\ -e network.publish_host=192.168.1.211 \\ -e node.data=true \\ -e discovery.zen.ping_timeout=120s \\ -e discovery.zen.minimum_master_nodes=2 \\ -e client.transport.ping_timeout=60s \\ -e discovery.zen.ping.unicast.hosts=&quot;192.168.20.95&quot; \\ -e path.data=/var/lib/elasticsearch \\ -e path.logs=/var/log/elasticsearch \\ docker.elastic.co/elasticsearch/elasticsearch:6.7.2docker run -d -p 9200:9200 -p 9300:9300 \\ -e bootstrap.memory_lock=true \\ -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; \\ -v /var/lib/elasticsearch:/var/lib/elasticsearch \\ -v /var/log/elasticsearch:/var/log/elasticsearch \\ -v /root/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\ --cap-add=IPC_LOCK --ulimit nofile=65536:65536 --ulimit memlock=-1:-1 \\ docker.elastic.co/elasticsearch/elasticsearch:6.7.2sysctl -w vm.max_map_count=262144graylog 查询语法Messages where the field type includes ssh :type:ssh 查看 JDK 版本 javac -versionSiderCar 日志收集器的轻量级配置管理组件apiVersion: elasticsearch.k8s.elastic.co/v1kind: Elasticsearchmetadata: name: quickstartspec: version: 8.4.3 nodeSets: - name: default count: 1 config: node.store.allow_mmap: falseSiderCarsidecar 通过 url 发现 graylog server，并告知 server，机上有哪些日志收集器，如 filebeat， graylog server 推送 congfiguration 到收集器wget https://github.com/Graylog2/collector-sidecar/releases/download/1.0.2/graylog-sidecar_1.0.2-1_amd64.debsudo dpkg -i graylog-sidecar_1.0.2-1_amd64.debsudo apt-get update &amp;amp;&amp;amp; sudo apt-get install graylog-sidecarvi /etc/graylog/sidecar/sidecar.ymlsudo graylog-sidecar -service install# Ubuntu 14.04 with Upstartsudo start graylog-sidecar# Ubuntu 16.04 and later with systemdsudo systemctl enable graylog-sidecarsudo systemctl start graylog-sidecar修改 server_url,server_api_token获取 server_api_tokenhttp://192.168.xx.xx:9000/system/authentication/users/tokens/graylog-sidecarsudo vim /etc/graylog/sidecar/sidecar.yml填入server_url: &quot;http://192.168.xx.95:9000/api/&quot;# The API token to use to authenticate against the Graylog server API.# This field is mandatoryserver_api_token: &quot;xxxxx&quot;....SyslogThe syslog input reads Syslog events as specified by RFC 3164 and RFC 5424, over TCP, UDP, or a Unix stream socket.Filebeatcurl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.5.2-amd64.debsudo dpkg -i filebeat-8.5.2-amd64.debsudo service filebeat start创建 Collector Configuration# Needed for Graylogfields_under_root: truefields.collector_node_id: ${sidecar.nodeName}fields.gl2_source_collector: ${sidecar.nodeId}fields.source: ${sidecar.nodeName}filebeat.inputs: - input_type: log paths: - /var/log/nginx/*.log type: log fields: source: nginx tags: - staffApioutput.logstash: hosts: [&quot;192.168.xx.95:5044&quot;]path: data: /var/lib/graylog-sidecar/collectors/filebeat/data logs: /var/lib/graylog-sidecar/collectors/filebeat/log# see a list of enabled and disabled modulesfilebeat modules list# systemctl status filebeat.service is inactive# 查看, shift+f , up or down to select column and enter s to sort and enter qtop# 查看 commandps -Flww -p &amp;lt;pid&amp;gt;4 S root 1133 778 0 80 0 - 356179 - 81972 0 Feb20 ? 00:55:29 /usr/share/filebeat/bin/filebeat -c /var/lib/graylog-sidecar/generated/filebeat.conf# 查看 -c 文件docker run -p 9000:9000 -p 12201:12201 -p 1514:1514 -v ~/graylog_config:/usr/share/graylog/data/config -v ~/graylog_journal:/var/lib/graylog-server/journal graylog/graylog:3.1.3kubernetes 部署filebeat, 发现 es 6.x 只支持 beat 6.X 的版本，但 type container 要 7.x 才支持，所以不能output.elasticsearch ，改为output.logstash 指向 graylog 5044端口进行数据清洗。hits 模块，根据pod annotations 和label 自定义配置要收集的pod confThe hints system looks for hints in Kubernetes Pod annotations or Docker labels that have the prefix co.elastic.logs. As soon as the container starts, Filebeat will check if it contains any hints and launch the proper config for it.Dockerdocker run -p 9000:9000 -p 12201:12201 -p 1514:1514 -v ~/graylog_config:/usr/share/graylog/data/config -v ~/graylog_journal:/var/lib/graylog-server/journal -d graylog/graylog:3.1.3 docker run \\ -p 9000:9000 -p 12201:12201 -p 1514:1514 -p 5555:5555 \\ -e GRAYLOG_ELASTICSEARCH_HOSTS=&quot;http://192.168.20.95:9200&quot; \\ -e GRAYLOG_IS_MASTER=&quot;false&quot; \\ -e GRAYLOG_MONGODB_URI=&quot;mongodb://192.168.20.95/graylog&quot; \\ -e GRAYLOG_STALE_MASTER_TIMEOUT=30000 \\ -d graylog/graylog:3.2is_master = falsemongodb_uri = mongodb://192.168.xx./graylogstale_master_timeout = 30000elasticsearch_hosts = http://192.168.xx.xx:9200http_external_uri = http://192.168.1.xxx:9000/http_bind_address = 0.0.0.0:9000output_batch_size = 5000RestApicurl -X GET http://192.168.1.211:9000/api/system/sessionscurl -X GET http://127.0.0.1:9000/api/cluster提取器destination -&amp;gt;.*-&amp;gt;([0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3})visitor session created ([0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3})参考docker install graylog修改 es 配置文件Filebeat quick start: installation and configurationgrayLog 查询语法grayLog v3.1 官方文档grayLog 配置文件位置支持一览表filebeat采集容器日志时根据kubernetes元数据限定采集源的问题Java Minimal Template EngineGrayLog 自定义邮件内容" }, { "title": " 算法", "url": "/posts/%E7%AE%97%E6%B3%95/", "categories": "算法", "tags": "二叉树", "date": "2022-09-05 15:54:48 +0800", "snippet": "树深度 = 层次子结点叶子结点非线性度 一个结点的子结点数，最大值 = 树的度总分支数 = 1* N1 + 2N2 …..mNm总结点数 = N0+ N1+N2…Nm存储 顺序 留空位置 完全二叉树 仅浪费下标0 的地址空间 链式遍历 根左右 左根右 左右根树与二叉树工具" }, { "title": "kubernetes aspnetcore", "url": "/posts/kubernetes-aspnetcore/", "categories": "DevOps", "tags": "k8s", "date": "2022-07-11 14:55:22 +0800", "snippet": "kubectl create configmap lsd-inventory-appsettings --from-file=appsettings.json=./appsettings.json -n lsd.NET Core 使用 K8S ConfigMap的正确姿势TroubleShotserilog 记录的时间TimeStamp是UTC，可以再docker build image 时指定 timezoneENV TZ=Asia/ShanghaiENV DEBIAN_FRONTEND=noninteractiverestsharp get 没问题，put 出现 基础连接已关闭win7 enable TLS 1.2 怀疑是win7 tls 1.2 没有开启或者没有打补丁How to Handle Timezones in Docker Containers" }, { "title": "kubernetes mssql", "url": "/posts/kubernetes-mssql/", "categories": "k8s", "tags": "mssql", "date": "2022-07-08 12:22:54 +0800", "snippet": "Troubleshot provider: SSL Provider, error: 31 - Encryption(ssl/tls) handshake failed修改 dockerfileFROM base AS finalRUN sed -i &#39;s/MinProtocol = TLSv1.2/MinProtocol = TLSv1/g&#39; /etc/ssl/openssl.cnfRUN sed -i &#39;s/MinProtocol = TLSv1.2/MinProtocol = TLSv1/g&#39; /usr/lib/ssl/openssl.cnfWORKDIR /appCOPY --from=publish /app/publish .SQL Server 2012 SP1（11.0.3000.00 - 2012 年 11 月)Extended Security Updates Ends in 1 year and 11 months (08 Jul 2025)Clarify supported Microsoft SQL Server versions Sequelize does not need to claim support for versions 2012, 2014 if we can’t test that in the pipeline.filter sp_who2" }, { "title": "kubernetes Nginx", "url": "/posts/kubernetes-Nginx/", "categories": "k8s", "tags": "nginx", "date": "2022-07-08 12:02:20 +0800", "snippet": "查看 pod ingress-nginx-controller nginx.conf$(kubectl get pod -o go-template --template &#39;\\n&#39; -n ingress-nginx -l app=app.kubernetes.io/component: controller) -n ingress-nginx cat /etc/nginx/nginx.confstream { upstream rabbitmq { server 192.168.xx.x:5672; } server { listen 5672; proxy_pass rabbitmq; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }}server { listen 80; server_name www.lsd.com; location / { proxy_pass http://192.168.xx.x/lsd/; }}server { listen 15672; server_name lsd-server-01; location / { proxy_pass http://192.168.xx.x:15672; }}rabbitmq management 15672 nginx 代理upstream rabbitbackend { server 127.0.0.1:15672;}server { listen 80; server_name rabbit.erps.sunnyb2b.com; fastcgi_connect_timeout 600; fastcgi_send_timeout 600; fastcgi_read_timeout 600; location / { port_in_redirect on; proxy_redirect off; proxy_pass http://rabbitbackend; proxy_set_header X-Real-IP $remote_addr; proxy_set_header User-Agent $http_user_agent; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } location ~* /rabbitmq/api/ { rewrite ^ $request_uri; rewrite ^/rabbitmq/api/(.*) /api/$1 break; return 400; proxy_pass http://rabbitbackend$uri; proxy_buffering off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; }}Commandsudo vim /etc/nginx/nignx.confsudo nginx -s reloadcat /var/log/nginx/access.logcat /var/log/nginx/error.logvuejsvuejs history model 情况下，刷新页面，出现404 nginx 问题，手动修改进入nginx 容器vim /etc/nginx/conf.d/default.confnginx -s reloadlocation / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html =404; }也可以DOCKERFILE写入指令sed -r -i &#39;/\\s+index.*index.htm;$/a \\\\ttry_files $uri $uri/ /index.html =404;&#39; /etc/nginx/conf.d/default.confnginx的try_files指令" }, { "title": "kubernetes rabbitmq", "url": "/posts/kubernetes-rabbitmq/", "categories": "k8s", "tags": "rabbit", "date": "2022-07-08 11:44:57 +0800", "snippet": "部署kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.ymlkubectl apply -f https://raw.githubusercontent.com/rabbitmq/cluster-operator/main/docs/examples/hello-world/rabbitmq.yamlapiVersion: rabbitmq.com/v1beta1kind: RabbitmqClustermetadata: name: lsd-rabbitmq用户密码查看default account 和 passwordusername=&quot;$(kubectl get -n rabbitmq-system secret lsd-rabbitmq-default-user -o jsonpath=&#39;{.data.username}&#39; | base64 --decode)&quot;password=&quot;$(kubectl get -n rabbitmq-system secret lsd-rabbitmq-default-user -o jsonpath=&#39;{.data.password}&#39; | base64 --decode)&quot;添加用户rabbitmqctl add_user default_user_TCxQBnkrxpLW73cy0Tl ELxsVWcFNZM-cxObUxPdNby8ImsSU9TaTCP 端口转发 tcp-services 因为 Ingress does not support TCP or UDP services., ingress-nginx-controller 启动时引用 tcp-services, 修改 configmap apiVersion: v1 kind: ConfigMap metadata: name: tcp-services namespace: ingress-nginx data: 5672: &quot;rebbitmq-system/lsd-rabbitmq:5672&quot; 15672: &quot;rebbitmq-system/lsd-rabbitmq:15672&quot; 或者打补丁 kubectl patch configmap tcp-services -n ingress-nginx --patch &#39;{&quot;data&quot;:{&quot;15672&quot;: &quot;rabbitmq-system/lsd-rabbitmq:15672&quot;}}&#39; 重启 Deployment ingress-nginx-controller port-forward kubectl port-forward pod/lsd-rabbitmq --address=172.21.xx.xx 15672:15672 清除污点kubectl patch crd/rabbitmqclusters.rabbitmq.com -p &#39;{&quot;metadata&quot;:{&quot;finalizers&quot;:[]}}&#39; --type=mergePRECONDITION-FAILED客户端连接提示 timeout 以及 PRECONDITION-FAILED， 查看rabbimq-ui发现节点0内存使用已超过阈值（watermark）,进入pod，查看rabbitmq-diagnostics status 以及 rabbitmq-diagnostics memory_breakdown ，设置为4G rabbitmqctl set_vm_memory_high_watermark absolute &quot;4G&quot; 或者 rabbitmqctl set_vm_memory_high_watermark relative 0.4 剩余可用内存40%时预警参考 Memory Threshold and Limit步骤kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.ymlkubectl apply -f https://raw.githubusercontent.com/rabbitmq/cluster-operator/main/docs/examples/hello-world/rabbitmq.yaml[root@master ~]# kubectl describe pod/lsd-rabbitmq-server-01 -n rabbitmq-system[root@master ~]# kubectl get pvc[root@master ~]# kubectl get pv发现Container无法绑定PVC rabbitmq-storage-rabbitmq-cluster-0，也没有申领到pv，不存在 rabbitmq-storage-rabbitmq-cluster-0查看Troubleshooting Cluster Operator，判断应该是因为Incorrect storageClassName configuration引起查看persistence，尝试Apply配置，关于 kubectl Create vs apply 的区别查看kubectl apply vs kubectl create，create 指令式管理，apply声明式管理apiVersion: rabbitmq.com/v1beta1kind: RabbitmqClustermetadata: name: lsd-rabbitmqspec: persistence: storageClassName: fast storage: 20Gik8s default 没有任何StorageClassName， 要手动创建查看 nfs,先创建存储制备器provisioner，注意Kubernetes 不包含内部 NFS 驱动。你需要使用外部驱动为 NFS 创建 StorageClass,参考 kubernetes-retired/external-storage，先apply rbac.yaml,创建clusterrole，role，rolebinding，再apply depolyment，创建 serviceaccount，deployment，再创建StorageClass， PersistentVolumeClaim创建后发现 pvc 一直pending，查看logs，发现waiting for a volume to be created, either by external provisioner &quot;wangzy-nfs-storage&quot; or manually created by system administrator 或 selfLink was empty, can&#39;t make reference参考 selfLink was empty, can’t make reference，找到workaround的方法edit /etc/kubernetes/manifests/kube-apiserver.yamlspec: containers: - command: - kube-apiserverAdd this line: –feature-gates=RemoveSelfLink=falsekubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml发现apiserver无法正常启动，查看日志，发现 k8s v1.20+ 禁用了selflink,所以此路不通“selfLink is a URL representing this object. Populated by the system. Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release.”有位网友提到 Just use new available docker image: gcr.io/k8s-staging-sig-storage/nfs-subdir-external-provisioner:v4.0.0 and it works fine. Do not edit kube-apiserver.yaml, there is no need to.尝试升级镜像库，因为gcr.io访问不到，改为 depolyment.yaml docker.io/dyrnq/nfs-subdir-external-provisioner:v4.0.2创建StorageClassapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfs-storage annotations: storageclass.kubernetes.io/is-default-class: &quot;true&quot; #---设置为默认的storageclassprovisioner: fuseim.pri/ifs #---动态卷分配者名称，必须和上面创建的&quot;PROVISIONER_NAME&quot;变量中设置的Name一致parameters: archiveOnDelete: &quot;true&quot; #---设置为&quot;false&quot;时删除PVC不会保留数据,&quot;true&quot;则保留数据mountOptions: - hard #指定为硬挂载方式 - nfsvers=4 #指定NFS版本，这个需要根据 NFS Server 版本号设置先 delete rabbitmq.yaml，再applyapiVersion: rabbitmq.com/v1beta1kind: RabbitmqClustermetadata: name: lsd-rabbitmq namespace: rabbitmq-systemspec: persistence: storageClassName: nfs-storage storage: 10Gi查看pvc，已正常Bound[root@master ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGErabbitmq-storage-rabbitmq-cluster-0 Bound pvc-03d69ccf-3ec3-4626-b131-f9617618ccc8 2Gi RWX managed-nfs-storage 23h再查看pod status ，发现 pod container 处于Exist状态，查看pod 日志，发现lsd-rabbitmq-server-0.lsd-rabbitmq-nodes.rabbitmq-system timeout,参考epmd error for host when starting rabbitmq,要将这个域名添加入/etc/hosts127.0.0.1 localhost, lsd-rabbitmq-server-0.lsd-rabbitmq-nodes.rabbitmq-system要想办法设置host参考 Failed to get nodes from k8s hostAliases: - ip: 10.43.0.1 hostnames: - kubernetes.default.svc.cluster.local既然 persistence 可以在 rabbitmq.yaml 设置，hostAliases应该也可以，查看 cluster-operator.yml,搜索 hostAliases结构如下：override: statefulSet: spec: template: spec: hostAliases: - ip: 127.0.0.1 hostnames: - lsd-rabbitmq-server-0.lsd-rabbitmq-nodes.rabbitmq-system修改rabbitmq.yamlapiVersion: rabbitmq.com/v1beta1kind: RabbitmqClustermetadata: name: lsd-rabbitmq namespace: rabbitmq-systemspec: override: statefulSet: spec: template: containers: - command: - /manager env: - name: OPERATOR_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: rabbitmqoperator/cluster-operator:2.0.0 name: operator ports: - containerPort: 9782 name: metrics protocol: TCP resources: limits: cpu: 200m memory: 500Mi requests: cpu: 200m memory: 500Mi spec: hostAliases: - ip: 127.0.0.1 hostnames: - lsd-rabbitmq-server-0.lsd-rabbitmq-nodes.rabbitmq-system persistence: storageClassName: nfs-storage storage: 10Gicontainers 是必需的，复制cluster-operator.yml 中的deployment containers再试，发现报 443 错误，应该是访问不到 apiserver2019-09-28 11:07:57.715 [info] &amp;lt;0.224.0&amp;gt; Failed to get nodes from k8s - {failed_connect,[{to_address,{&quot;kubernetes.default&quot;,443}},{inet,[inet],closed}]}[root@master ~]# crictl psCONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID PODedec02162b08a 5185b96f0becf 6 days ago Running coredns 0 3995f75c191f5 coredns-c676cc86f-p7vdl6fa312b3b8aa1 5185b96f0becf 6 days ago Running coredns 0 3b8c9b069e1de coredns-c676cc86f-g4d49[root@master ~]# crictl inspect edec02162b08aevn:[ .... &quot;KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443&quot;]接下来修改rabbitmq.yaml hostAliases: - ip: 127.0.0.1 hostnames: - lsd-rabbitmq-server-0.lsd-rabbitmq-nodes.rabbitmq-system - ip: 10.96.0.1 hostnames: - kubernetes.default成功run pod尝试apply service ,将 rabbitmq UI 公布到外网kind: ServiceapiVersion: v1metadata: labels: app.kubernetes.io/name: lsd-rabbitmq #这里要注意 key:value ,我们查看describe是 key=value name: rabbitmq-cluster-manage namespace: rabbitmq-system #namespacespec: ports: - name: http port: 15672 protocol: TCP targetPort: 15672 selector: app.kubernetes.io/name: lsd-rabbitmq #这里要注意 key:value ,我们查看describe是 key=value type: NodePort[root@master ~]# kubectl get services -n rabbitmq-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGElsd-rabbitmq ClusterIP 10.104.215.205 &amp;lt;none&amp;gt; 5672/TCP,15672/TCP,15692/TCP 3h54mlsd-rabbitmq-nodes ClusterIP None &amp;lt;none&amp;gt; 4369/TCP,25672/TCP 3h54mrabbitmq-cluster-manage NodePort 10.99.41.142 &amp;lt;none&amp;gt; 15672:30907/TCP 3h10m因为部署在 node1 ，所以访问 http://172.21.14.208:30907/#/成功，查看user ，pwd ，登陆username=&quot;$(kubectl get -n rabbitmq-system secret lsd-rabbitmq-default-user -o jsonpath=&#39;{.data.username}&#39; | base64 --decode)&quot;password=&quot;$(kubectl get -n rabbitmq-system secret lsd-rabbitmq-default-user -o jsonpath=&#39;{.data.password}&#39; | base64 --decode)&quot;参考RabbitMQ Cluster Kubernetes Operator端口转发RabbitMQ Cluster Operator for Kubernetes" }, { "title": "kubernetes redis", "url": "/posts/kubernetes-redis/", "categories": "k8s", "tags": "redis", "date": "2022-07-08 09:42:24 +0800", "snippet": "apiVersion: v1kind: Podmetadata: name: redisspec: containers: - name: redis image: redis:5.0.4 command: - redis-server - &quot;/redis-master/redis.conf&quot; env: - name: MASTER value: &quot;true&quot; ports: - containerPort: 6379 resources: limits: cpu: &quot;0.1&quot; volumeMounts: - mountPath: /redis-master-data name: data - mountPath: /redis-master # 将config 对应的文件放入 redis-master文件夹下 name: config 对应 volumes 中的config volumes: # 你可以在 Pod 级别设置卷，然后将其挂载到 Pod 内的容器中 - name: data emptyDir: {} - name: config configMap: name: lsd-redis-config items: - key: redis-config path: redis.conf #configmap data.[key] 会生成对应的文件 redis-configkind: ConfigMapapiVersion: v1metadata: name: lsd-redis-config namespace: redisdata: redis-config: | maxmemory 2mb maxmemory-policy allkeys-lru requirepass xxxxconfigmap" }, { "title": "Quartznet", "url": "/posts/Quartznet/", "categories": "middleware", "tags": "middleware", "date": "2022-06-17 17:04:38 +0800", "snippet": "Hangfire vs QuartznetAspnetcore with Quartznet 默认注入无参的构造函数，可以利用 IJobFactory 实现注入IJob或者通过UseMicrosoftDependencyInjectionJobFactory 利用微软自带的IOC注入//FlushRedisDbJob 实现IJobservices.AddTransient&amp;lt;FlushRedisDbJob&amp;gt;();services.AddQuartz(q =&amp;gt; { q.UseMicrosoftDependencyInjectionJobFactory(); //新建scheduler,job,trigger q.ScheduleJob&amp;lt;FlushRedisDbJob&amp;gt;(trigger =&amp;gt; trigger .WithIdentity(&quot;Combined Configuration Trigger&quot;) .StartAt(DateBuilder.EvenSecondDate(DateTimeOffset.Now.AddSeconds(7))) .WithDailyTimeIntervalSchedule(x =&amp;gt; x.WithInterval(10, IntervalUnit.Second)) .WithDescription(&quot;my awesome trigger configured for a job with single call&quot;)) ; });services.AddQuartzHostedService(options =&amp;gt; { // when shutting down we want jobs to complete gracefully options.WaitForJobsToComplete = true; });" }, { "title": "gitlab", "url": "/posts/gitlab/", "categories": "工具", "tags": "工具", "date": "2022-06-15 11:13:36 +0800", "snippet": "docker install gitlab-cesudo docker run --detach \\ --hostname gitlab.example.com \\ --publish 443:443 --publish 80:80 --publish 22:22 \\ --name gitlab \\ --restart always \\ --volume $GITLAB_HOME/config:/etc/gitlab \\ --volume $GITLAB_HOME/logs:/var/log/gitlab \\ --volume $GITLAB_HOME/data:/var/opt/gitlab \\ --shm-size 256m \\ gitlab/gitlab-ce:latest登陆 http://gitlab.example.com/user/sign_in, account:root,password 查看 /srv/gitlab/config/initial_root_password,24小时后失效，在界面上重置密码如忘记root password,可进入容器，exec /bin/bash ,再启用控制台gitlab-rails console -e production, 找到user = User.where(id:1).first,修改密码user.password = “xxxxxx” 再次确认 user.password_confirm = “xxxxxx” ，保存 user.save!, 退出控制台 quitCI需 install git runner ，参考runner installsudo yum install gitlab-runner注册runner,参考registersudo gitlab-runner register注意，token 在 gitlab repository CI 可以找到，runner executor 选择 docker ， default image 根据项目中用到但没有在yml 文件中定义的镜像也可以指令注册sudo gitlab-runner register -n --url https://your_gitlab.com --registration-token project_token --executor docker --description &quot;Deployment Runner&quot; --docker-image &quot;docker:stable&quot; --tag-list deployment --docker-privilegedyml 文件image: node:18-alpinecache: paths: - node_modules/test_async: script: - npm config set proxy null - npm config set https-proxy null - npm config set registry http://registry.npmjs.org/ - npm install - node --experimental-vm-modules node_modules/jest/bin/jest.js --detectOpenHandlesnpm ERR! Error: connect ECONNREFUSED移除代理，让npm 从 offcial url download迁移# 安装相同版本的gitlabsudo EXTERNAL_URL=&quot;http://code.lesaunda.com.cn&quot; yum install -y -y gitlab-jh-15.10.3-jh.0.el7# 备份sshfind /etc/ssh -iname &#39;ssh_host_*&#39; -exec cp {} {}.backup.`date +%F` \\# 停止ci/cd jobs 调度vim /etc/gitlab/gitlab.rb# nginx[&#39;custom_gitlab_server_config&#39;] = &quot;location = /api/v4/jobs/request {\\n deny all;\\n return 503;\\n }\\n&quot;sudo gitlab-ctl reconfigure# 停gitlab服务sudo /opt/gitlab/embedded/bin/redis-cli -s /var/opt/gitlab/redis/redis.socket save &amp;amp;&amp;amp; sudo gitlab-ctl stop &amp;amp;&amp;amp; sudo gitlab-ctl start postgresql &amp;amp;&amp;amp; sudo gitlab-ctl start gitaly# 备份 secretscp /etc/gitlab/gitlab-secrets.json /etc/gitlab/gitlab-secrets.json.bak# 创建 备份sudo gitlab-backup create# 升权，以便远程复制sudo chown root /var/opt/gitlab/redis /var/opt/gitlab/backupssudo scp /var/opt/gitlab/redis/dump.rdb root@&amp;lt;newServerIp&amp;gt;:/var/opt/gitlab/redissudo scp /var/opt/gitlab/backups/1706604052_2024_01_30_15.10.3-jh_gitlab_backup.tar root@&amp;lt;newServerIp&amp;gt;:/var/opt/gitlab/backups# 替换json，做好备份sudo scp /etc/gitlab/gitlab-secrets.json.2024-01-30 root@192.168.1.192:/etc/gitlab/gitlab-secrets.json.2024-01-30# 复制 sshd 配置sudo vim /etc/ssh/sshd_config# restartsudo systemctl restart sshd# 新服务器chown root:root /etc/gitlab/gitlab-secrets.jsonchmod 0600 /etc/gitlab/gitlab-secrets.jsonsudo chown gitlab-redis /var/opt/gitlab/redissudo chown gitlab-redis:gitlab-redis /var/opt/gitlab/redis/dump.rdbsudo chown git:root /var/opt/gitlab/backupssudo chown git:git /var/opt/gitlab/backups/1706604052_2024_01_30_15.10.3-jh_gitlab_backup.targitlab-ctl reconfiguresudo gitlab-ctl stop pumasudo gitlab-ctl stop sidekiqsudo gitlab-backup restore BACKUP=1706604052_2024_01_30_15.10.3-jhgitlab-ctl reconfigure# 如果没有重启，git push 提示 unauth 401 Errorsudo gitlab-ctl restart参考Migrate to a new serverGitLab Docker images重置用户密码How To Set Up a Continuous Deployment Pipeline with GitLab CI/CD on Ubuntu 18.04" }, { "title": "plsql", "url": "/posts/plsql/", "categories": "sql", "tags": "sql", "date": "2022-06-09 11:12:55 +0800", "snippet": "正在执行的sqlSELECT sess.process, sess.status, sess.username, sess.schemaname, sql.sql_text FROM v$session sess, v$sql sql WHERE sql.sql_id(+) = sess.sql_id AND sess.type = &#39;USER&#39; AND sess.status = &#39;ACTIVE&#39;SELECT s.inst_id, s.sid, s.serial#, --s.sql_id, p.spid, s.username, s.programFROM gv$session s JOIN gv$process p ON p.addr = s.paddr AND p.inst_id = s.inst_idWHERE s.type != &#39;BACKGROUND&#39;;SELECT OSUSER 电脑登录身份, PROGRAM 发起请求的程序, USERNAME 登录系统的用户名, SCHEMANAME, B.Cpu_Time 花费cpu的时间, STATUS, B.SQL_TEXT 执行的sqlFROM V$SESSION ALEFT JOIN V$SQL B ON A.SQL_ADDRESS = B.ADDRESS AND A.SQL_HASH_VALUE = B.HASH_VALUEORDER BY b.cpu_time DESC ;SELECT l.session_id sid, s.serial#, l.locked_mode 锁模式, l.oracle_username 登录用户, l.os_user_name 登录机器用户名, s.machine 机器名, s.terminal 终端用户名, o.object_name 被锁对象名, s.logon_time 登录数据库时间FROM v$locked_object l, all_objects o, v$session sWHERE l.object_id = o.object_id AND l.session_id = s.sidORDER BY sid, s.serial#;" }, { "title": "minikube", "url": "/posts/minikube/", "categories": "DevOps", "tags": "k8s", "date": "2022-05-23 14:23:37 +0800", "snippet": "安装curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64sudo install minikube-linux-amd64 /usr/local/bin/minikube镜像minikube start --image-mirror-country=&#39;cn&#39; --insecure-registry 172.21.14.206:5000 --v=8备用方案minikube start --image-repository registry.aliyuncs.com/google_containers --image-mirror-country=&#39;cn&#39; --vm-driver=none --registry-mirror=https://registry.docker-cn.com --extra-config=kubelet.cgroup-driver=systemd --alsologtostderr --v=5minikube start --image-repository registry.aliyuncs.com/google_containers --image-mirror-country=&#39;cn&#39; --base-image=&quot;anjone/kicbase&quot; --registry-mirror=https://registry.docker-cn.com --alsologtostderr --v=8docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.24.1 k8s.gcr.io/kube-apiserver:v1.24.1docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.24.1 k8s.gcr.io/kube-proxy:v1.24.1docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.24.1 k8s.gcr.io/kube-controller-manager:v1.24.1docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.24.1 k8s.gcr.io/kube-scheduler:v1.24.1docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.3-0 k8s.gcr.io/etcd:3.5.3-0docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7 k8s.gcr.io/pause:3.7docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6 k8s.gcr.io/coredns:v1.8.6docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5 k8s.gcr.io/storage-provisioner:v5docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6 k8s.gcr.io/pause:3.6内网访问面板 kubectl proxy --port=33493 --address=‘172.21.14.xxxx’ --accept-hosts=’^.*’ &amp;amp;kubectl get pods –namespace=kubernetes-dashboard kubectl proxy –address 0.0.0.0 kubernetes-dashboard-67994cff74-t45m7 8001:80 –namespace=kubernetes-dashboard –disable-filter=true Starting to serve on [::]:8001 命令 kubectl delete pod,service -l app.kubernetes.io/name=ingress-nginx 删除 label 相关的pod 和service kubectl cordon $NODENAME 节点设置为不可调度 kubectl describe node &amp;lt;节点名称&amp;gt; 查看node kubectl get deployments 查看 deploy kubectl describe deployment nginx-deployment 查看 deploy info kubectl get rs 查看rs，ReplicaSet = rs kubectl rollout status deployment/nginx-deployment 查看滚动状态 kubectl rollout history deployment/nginx-deployment 滚动更新历史 kubectl rollout undo deployment/nginx-deployment --to-revision=2 回滚 kubectl api-resources --namespaced 查看资源,资源缩写，比如 Endpoint = ep, kubectl get ep EndpointName加深理解你需要在集群中的每个节点上都有一个可以正常工作的 容器运行时， 这样 kubelet 能启动 Pod 及其容器 kubelet 启动 容器每个 Pod 都在每个地址族中获得一个唯一的 IP 地址。 Pod 中的每个容器共享网络名字空间，包括 IP 地址和网络端口。 Pod 内 的容器可以使用 localhost 互相通信。Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡服务（Service）将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。 Service 为 Pods 负载均衡Kubernetes 为该服务分配一个 IP 地址（有时称为 “集群IP”），该 IP 地址由服务代理使用。 服务IP = 集群IPClusterIP：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是默认的 ServiceType 只能在集群内部访问对应EndPoint，如果外部访问 type = NodePortkubectl EXPOSE 服务选择算符的控制器不断扫描与其选择器匹配的 Pod，然后将所有更新发布到也称为 “my-service” 的 Endpoint 对象。 label selectorKubernetes 采用的是中心辐射型（Hub-and-Spoke）API 模式。kubelet 会在集群中每个节点（node）上运行。 它保证容器（containers）都运行在 Pod 中。控制面(API 服务器)到节点通信方式 kubelet (终端cmd) api 服务器代理功能 （webUI）API 服务器到节点、Pod 和服务 HTTP 方式向api 服务器添加 node kubelet 自注册 手动添加没有两个 Node 可以同时使用相同的名称要标记一个 Node 为不可调度 kubectl cordon $NODENAME节点状态 kubectl describe node &amp;lt;节点名称&amp;gt; HostName : 内核报告 ExternalIP: 通常是节点的可外部路由（从集群外可访问）的 IP 地址 InternalIP: 通常是节点的仅可在集群内部路由的 IP 地址心跳 .status lease(租约)对象kubelet 会更新 .status。 .status 更新的默认间隔为 5 分钟kubelet 会创建并每 10 秒（默认更新间隔时间）更新 Lease 对象与 Node 的 .status 更新相比，Lease 是一种轻量级资源。 使用 Lease 来表达心跳在大型集群中可以减少这些更新对性能的影响Deployment一个 Deployment 为 Pod 和 ReplicaSet 提供声明式的更新能力.spec.template 是一个 Pod 模板。 它和 Pod 的语法规则完全相同。 只是这里它是嵌套的，因此不需要 apiVersion 或 kind.spec.selector 必须匹配 .spec.template.metadata.labels，否则请求会被 API 拒绝检查 Deployment 是否已创建 kubectl get deployments kubectl describe deployment nginx-deployment要查看 Deployment 上线状态 kubectl rollout status deployment/nginx-deployment查看 Deployment 通过创建新的 ReplicaSet 并将其扩容到 3 个副本 kubectl get rs通常不鼓励更新标签选择算符。建议你提前规划选择算符。回滚检查 Deployment 修订历史 kubectl rollout history deployment/nginx-deployment撤消当前上线并回滚到以前的修订版本 kubectl rollout undo deployment/nginx-deployment --to-revision=2缩放 kubectl scale deployment/nginx-deployment --replicas=10仅当 Deployment Pod 模板（即 .spec.template）发生改变时，例如模板的标签或容器镜像被更新， 才会触发 Deployment 上线。其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。策略.spec.strategy 策略指定用于用新 Pods 替换旧 Pods 的策略。 .spec.strategy.type 可以是 “Recreate” 或 “RollingUpdate”。“RollingUpdate” 是默认值。Pods就 Docker 概念的术语而言，Pod 类似于共享名字空间和文件系统卷的一组 Docker 容器Pod 被设计成了相对临时性的、用后即抛的一次性实体Deployment 控制器针对每个 Deployment 对象确保运行中的 Pod 与当前的 Pod 模版匹配。如果模版被更新，则 Deployment 必须删除现有的 Pod，基于更新后的模版 创建新的 Pod。每个工作负载资源都实现了自己的规则，用来处理对 Pod 模版的更新。资源共享和通信每个 Pod 都在每个地址族中获得一个唯一的 IP 地址。 Pod 中的每个容器共享网络名字空间，包括 IP 地址和网络端口。 Pod 内 的容器可以使用 localhost 互相通信。 当 Pod 中的容器与 Pod 之外 的实体通信时，它们必须协调如何使用共享的网络资源 （例如端口）在 Linux 中，Pod 中的任何容器都可以使用容器规约中的 安全性上下文中的 privileged（Linux）参数启用特权模式。Pod 的生命周期Pod 在其生命周期中只会被调度一次。 一旦 Pod 被调度（分派）到某个节点，Pod 会一直在该节点运行，直到 Pod 停止或者 被终止。任何给定的 Pod （由 UID 定义）从不会被“重新调度（rescheduled）”到不同的节点； 相反，这一 Pod 可以被一个新的、几乎完全相同的 Pod 替换掉。 如果需要，新 Pod 的名字可以不变，但是其 UID 会不同。Pod 的 status 字段是一个 PodStatus 对象，其中包含一个 phase 字段要检查 Pod 中容器的状态，你可以使用 kubectl describe pod &amp;lt;pod 名称&amp;gt;。 其输出中包含 Pod 中每个容器的状态Pod 的 spec 中包含一个 restartPolicy 字段，其可能取值包括 Always、OnFailure 和 Never。默认值是 Always。Pod 的阶段（Phase）是 Pod 在其生命周期中所处位置的简单宏观概述容器探针Probe 是由 kubelet 对容器执行的定期诊断ExecAction（借助容器运行时执行）TCPSocketAction（由 kubelet 直接检测）HTTPGetAction（由 kubelet 直接检测）返回码为 0 则认为诊断成功gRPC健康检查HTTP GET 请求TCP 检查kubelet 可以选择是否执行以下三种探针，以及如何针对探测结果作出反应 livenessProbe 指示容器是否正在运行 ,如果存活态探测失败，则 kubelet 会杀死容器 readinessProbe 指示容器是否准备好为请求提供服务,指示容器是否准备好为请求提供服务 startupProbe 指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被 禁用，直到此探针成功为止。 Pod 的终止通常情况下，容器运行时会发送一个 TERM 信号到每个容器中的主进程。 很多容器运行时都能够注意到容器镜像中 STOPSIGNAL 的值，并发送该信号而不是 TERM。 一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL 信号，之后 Pod 就会被从 API 服务器 上移除。服务将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法Kubernetes 为该服务分配一个 IP 地址（有时称为 “集群IP”），该 IP 地址由服务代理使用Pod 中的端口定义是有名字的，你可以在 Service 的 targetPort 属性中引用这些名称在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。 kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式userspace 代理模式kube-proxy 会监视 Kubernetes 控制平面对 Service 对象和 Endpoints 对象的添加和移除操作。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。 任何连接到“代理端口”的请求，都会被代理到 Service 的后端 Pods 中的某个上面（如 Endpoints 所报告的一样）。 使用哪个后端 Pod，是 kube-proxy 基于 SessionAffinity 来确定的。最后，它配置 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP） 和 Port 的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。IPVS 代理模式在 ipvs 模式下，kube-proxy 监视 Kubernetes 服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。服务发现当 Pod 运行在 Node 上，kubelet 会为每个活跃的 Service 添加一组环境变量。 kubelet 为 Pod 添加环境变量 {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT。 这里 Service 的名称需大写，横线被转换成下划线。你可以（几乎总是应该）使用附加组件 为 Kubernetes 集群设置 DNS 服务。支持集群的 DNS 服务器（例如 CoreDNS）监视 Kubernetes API 中的新服务，并为每个服务创建一组 DNS 记录。如果你在 Kubernetes 命名空间 my-ns 中有一个名为 my-service 的服务， 则控制平面和 DNS 服务共同为 my-service.my-ns 创建 DNS 记录。发布服务Kubernetes ServiceTypes 允许指定你所需要的 Service 类型，默认是 ClusterIP ClusterIP：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 NodePort：通过每个节点上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到自动创建的 ClusterIP 服务。 通过请求 &amp;lt;节点 IP&amp;gt;:&amp;lt;节点端口&amp;gt;，你可以从集群的外部访问一个 NodePort 服务。targetPort：容器接收流量的端口；port：可任意取值的抽象的 Service 端口，其他 Pod 通过该端口访问 Servicekubectl describe svc my-nginxkubectl get ep my-nginxkubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE使用 CronJob 运行自动化任务CronJob 在创建周期性以及重复性的任务时很有帮助，例如执行备份操作或者发送邮件。 CronJob 也可以在特定时间调度单个任务，例如你想调度低活跃周期的任务。cronjob访问集群kubectl config view 使用端口转发来访问集群中的应用kubectl port-forward service/mongo 28015:27017 使用服务来访问集群中的应用kubectl expose deployment hello-world --type=NodePort --name=example-service 使用 Service 把前端连接到后端Ubuntu网络配置auto loiface lo inet loopbackauto enp2s0iface enp2s0 inet staticaddress 172.21.xx.xxnetmask 255.255.254.0gateway 172.21.14.xdns-nameservers 192.168.1.xxxsudo ifdown enp2s0 &amp;amp;&amp;amp; sudo ifup enp2s0ip addr flush dev enp2s0本地镜像安装local registrydocker run -d -p 5000:5000 --restart always --name registry registry:2 运行docker本地注册表minikube start --insecure-registry=&quot;docker.local:5000&quot; 运行minikube如果minikube启动时没有带参 --insecure-registry,需要进入minikube ssh,查看/etc/systemd/system/docker.service.d/10-machine.conf 或 /usr/lib/systemd/system/docker.service添加/修改 --insecure-registry 10.0.0.0/24 ExecStart=/usr/bin/docker daemon -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock –tlsverify –tlscacert /etc/docker/ca.pem – tlscert /etc/docker/server.pem –tlskey /etc/docker/server-key.pem –label provider=virtualbox –insecure-registry 10.0.0.0/24systemctl daemon-reload 刷新守护进程systemctl start docker 重启参考 如何在 Minikube 中使用本地 docker 镜像另外本地docker(非minikube内的docker),可以通过新建 /etc/docker/daemon.json{ &quot;insecure-registries&quot; : [&quot;172.21.14.206:5000&quot;]}systemctl daemon-reload &amp;amp;&amp;amp; systemctl restart dockerTroubleshot minikube start 提示 kubelet is not running或者apiserver process never appeared查日志 sudo journalctl -exu kubelet/kubeadm,列出 kubernetes 镜像,发现 阿里云的镜像 没有pull pause:3.6kubeadm config images list k8s.gcr.io/kube-apiserver:v1.24.2k8s.gcr.io/kube-controller-manager:v1.24.2k8s.gcr.io/kube-scheduler:v1.24.2k8s.gcr.io/kube-proxy:v1.24.2k8s.gcr.io/pause:3.7k8s.gcr.io/etcd:3.5.3-0k8s.gcr.io/coredns/coredns:v1.8.6要注意 kubernetes 1.24.1 和1.24.2 引用的镜像版本不同，需要手动pull和tag docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6 minikube addons enable ingress 一直出现 pending查看Event Warning FailedScheduling 12h default-scheduler 0/2 nodes are available: 2 node(s) didn&#39;t match Pod&#39;s node affinity.，发现ingress-nginx-controller 的 nodeselector label 不正确nodeSelector: kubernetes.io/os: linux # 添加当前node 对应的label minikube.k8s.io/primary: &#39;true&#39;参考 已解决pod/nginx-ingress-controller一直处于Pending状态，异常如下 ubuntu 加入 dns, ping 失败3.1. 修改 /etc/netplan/….yamlnetwork: ethernets: enp2s0: dhcp4: false addresses: [172.21.14.xxx/23] gateway4: 172.21.14.x nameservers: addresses: [192.168.1.xxx,192.168.1.xxx,8.8.8.8] version: 2应用配置 sudo netplan apply3.2. 安装samba修改 /etc/samba/smb.conf，include = /home/samba/etc/smb.conf.lsd-server-01 minikube start 出现 permission deniedsudo groupadd dockersudo usermod -aG docker $USERgroups 查看newgrp docker 更新参考How to Install Minikube on Ubuntu 18.04 / 20.04kubectl command" }, { "title": "悲观锁 乐观锁", "url": "/posts/%E6%82%B2%E8%A7%82%E9%94%81-%E4%B9%90%E8%A7%82%E9%94%81/", "categories": "数据库", "tags": "锁", "date": "2022-05-19 10:43:24 +0800", "snippet": " 悲观锁字面意思，假设会发生糟糕的情况，比如脏读、幻读，不可重复读等独占和排他数据库层实现，主要分共享锁（S锁）和排他锁（X锁），S锁指多个事务共享，只读不能修改，X锁指独占 乐观锁不依赖数据库，不上锁，比如数据行中包含version列/时间戳（不是自旋列，会出现ABA问题），每次更新读取比较再替换（CAS， compare and swap），冲突的话就重复读取覆盖，直至成功1.ABA问题 假设有两个线程——线程1和线程2，两个线程按照顺序进行以下操作：(1)线程1读取内存中数据为A；(2)线程2将该数据修改为B；(3)线程2将该数据修改为A；(4)线程1对数据进行CAS操作高并发中，乐观锁限制了性能，通过锁粒度放宽锁的控制MySQL for update 默认行锁，如果没有索引，上升到表锁什么是乐观锁，什么是悲观锁" }, { "title": "WebApiClient", "url": "/posts/nuget-WebApiClient/", "categories": "nuget", "tags": "httpclient", "date": "2022-04-14 16:20:57 +0800", "snippet": "WebApiClient基本用法public interface IUserApi{ [HttpGet(&quot;api/users/{id}&quot;)] Task&amp;lt;User&amp;gt; GetAsync(string id); [HttpPost(&quot;api/users&quot;)] Task&amp;lt;User&amp;gt; PostAsync([JsonContent] User user);}public void ConfigureServices(IServiceCollection services){ services.AddHttpApi&amp;lt;IUserApi&amp;gt;();}public class MyService{ private readonly IUserApi userApi; public MyService(IUserApi userApi) { this.userApi = userApi; }}源码分析源码下载注入，TryAddTransient调用了httiApiProvider.CreateHttpApi，IHttpApiActivator的实现类DefaultHttpApiActivator重写了CreateFactory，services.TryAddTransient(serviceProvider =&amp;gt;{ var httiApiProvider = serviceProvider.GetRequiredService&amp;lt;HttpApiProvider&amp;lt;THttpApi&amp;gt;&amp;gt;(); return httiApiProvider.CreateHttpApi(serviceProvider, name);});this.httpApiActivator.CreateInstance(httpApiInterceptor) this.factory(apiInterceptor, this.actionInvokers) client=httpClientFactory.CreateClient HttpApiInterceptor(HttpClientContext context) Intercept actionInvoker.Invoke(this.context, arguments) ApiRequestExecuter.ExecuteAsync(request) HandleRequestAsync(request) attr.OnRequestAsync(context)//这里利用attribute实现内容格式转换，比如object-&amp;gt;json 等BuildMethods for (var i = 0; i &amp;lt; actionMethods.Length; i++) { //每个空白的接口方法插入&amp;gt;Intercep方法 //Intercep方法则call ApiRequestExecuter.ExecuteAsync //通过emit将接口类实现为普通类 iL.Emit(OpCodes.Callvirt, interceptMethod); ... }services.AddHttpClient(&quot;IUserApi&quot;); //msdn是利用aot代码生成，见上一节的代码，这里利用emit实现" }, { "title": "线程", "url": "/posts/%E7%BA%BF%E7%A8%8B/", "categories": "dotnet", "tags": "线程", "date": "2022-04-01 10:58:16 +0800", "snippet": "线程进程是一种正在执行的程序应用程序域为公共语言运行时提供隔离单元。 它们在进程中创建和运行线程是操作系统向其分配处理器时间的基本单元,线程上下文包含线程顺畅继续执行所需的全部信息，包括线程的一组 CPU 寄存器和堆栈。多个线程可在进程上下文中运行，进程的所有线程共享其虚拟地址空间。线程可以跨应用程序域访问地址空间。CPU 寄存器指PU内部用来存放数据的一些小型存储区域，比如指令寄存器 If you compile your managed code as an .exe assembly, the runtime is started automatically by mscoree.dll when the .exe is run.CLR 也叫公共语言运行时，也运行托管代码的环境运行exe时，runtime hosting 会自动加载 对应版本的 runtime（CLR），在CLR引导下创建应用程序域，在此域中执行用户代码运行时宿主也提供了API，可以给开发者修改CLR的行为，比如GC的回收机制。单个线程最多只能与一个 I/O 完成端口相关联线程池，不销毁线程对象（CurrentUICulture），在集合中等待调用，线程池需要管理，自己实现或者被CLR托管当进程启动时，公共语言运行时会自动创建单个前台线程来执行应用程序代码。与这个主前台线程一起，一个进程可以创建一个或多个线程来执行与该进程相关的程序代码的一部分。这些线程可以在前台或后台执行。此外，您可以使用ThreadPool类在由公共语言运行时管理的工作线程上执行代码。域CLR 引导程序创建的域在 CLR 执行托管代码的第一行之前，它会创建三个应用程序域系统域单例SystemDomain 负责创建和初始化 SharedDomain 和默认的 AppDomain。它将系统库 mscorlib.dll 加载到 SharedDomain 中共享域单例（mscorlib.dll）System 命名空间中的基本类型（如 Object、ValueType、Array、Enum、String 和 Delegate）在 CLR 引导过程中被预加载到此域中默认域AppDomainDefaultDomain 是 AppDomain 的一个实例，通常在其中执行应用程序代码每个 AppDomain 都有自己的 SecurityDescriptor、SecurityContext 和 DefaultContext，以及自己的 loader 堆（High-Frequency Heap、Low-Frequency Heap 和 Stub Heap）CPU调度算法时间片轮转（RR）调度算法时间片 进程上下文切换（耗时） 抢占如果在时间片结束时进程还在运行，则CPU将被剥夺并分配给另一个进程。如果进程在时间片结束前阻塞或结束，则CPU当即进行切换。调度程序所要做的就是维护一张就绪进程列表，当进程用完它的时间片后，它被移到队列的末尾。——引用时间片轮转调度算法也可参考时间片轮转（RR）调度算法（详解版）先来先服务调度（FCFS）算法先请求 CPU 的进程首先分配到 CPUFCFS 调度算法是非抢占的。一旦 CPU 分配给了一个进程，该进程就会使用 CPU 直到释放 CPU 为止，即程序终止或是请求 I/O。FCFS 算法对于分时系统（每个用户需要定时得到一定的 CPU 时间）是特别麻烦的。允许一个进程使用 CPU 过长将是个严重错误线程取消Cancallationtoken 取消令牌CancallationtokenSource 管理取消令牌 协作方式取消传递 token 的副本 轮询监听for (int x = 0; x &amp;lt; rect.columns &amp;amp;&amp;amp; !token.IsCancellationRequested; x++) { …} callbacktoken.Register 当取消时回调 等待句柄token 有成员waithandle ，可以利用waithandle.waitany 等待 同步事件， cts.cancel 时内部 set 这个同步事件线程同步Monitorlock(obj){}是基于Monitor的语法糖，内存屏障bool lockWasTaken = false;var temp = obj;try{ Monitor.Enter(temp, ref lockWasTaken); { body }}finally{ if (lockWasTaken) Monitor.Exit(temp);}MutexMutex继承WaitHandle,WaitHandle 定义了方法Waitone和字段SafeHandleMutex 分本地（未命名）和全局（命名），和Monitor比较，Mutex是可用于进程间的互斥体EventWaitHandle继承抽象类 WaitHandle ,派生AutoResetEvent和ManualResetEvent，两者之间的区别就像收费站和门，前者通过后会自动合上。waitone 该线程等待事件终止set 将事件终止，waitone的线程将获得信号继续，如model = autoreset，则每次仅允许一个线程，因为线程waitone获得信号后将自动reset，其他线程继续waitonereset 将事件重置为非终止状态，会导致waitone的线程阻塞等待Semaphore信号量，既可用于进程间，也可用于同一进程的线程池，限制可同时访问某一资源或资源池的线程数。semaphore 继承 WaitHandle，初始化SafeNativeMethods.CreateSemaphore，传入max 线程数，释放时 调用了 kernel32.dll WaitHandle，初始化SafeNativeMethods.ReleaseSemaphore，传入SafeWaitHandleWaitOne 应与 Release 次数相等，否则会抛出异常。SemaphoreSlim 是轻量版的信号量，用于本地，同一进程。busy spinningConcurrentBag 类 ，当排序并不重要时，包可用于存储对象，而与集不同，包支持重复项CountdownEvent表示在计数变为零时处于有信号状态的同步基元wait 阻塞 直到 实例 CountdownEvent 多次 signal 到 0 时 释放线程暂停Thread.Sleep(1) 表示超时时间内该线程不可被调度，并放弃剩下的cpu时间片，如果timeout等于0，放弃剩余时间片，不保证就绪线程会立即运行，由优先级决定。和Task.Delay(1)比较，后者会创建一个新的任务，内部timer实现延迟，canceltoken实现取消。适合用来实现多少时间间隔后新建线程做某事睡眠功能 (synchapi.h)Visual C#：Thread.Sleep 与 Task.DelayThread.Join等待 join 的线程终止后继续当前线程，可以指定timeout时间常见问题死锁A线程lock C资源，同时请求D资源，但D资源被 B线程lock住，同时释放的条件是请求到C资源（此时已被A线程lock住），所以死锁。可以这样避免if (Monitor.TryEnter(lockObject, 300)) { try { // Place code protected by the Monitor here. } finally { Monitor.Exit(lockObject); }}else { // Code to execute if the attempt times out.}争用条件多线程访问同一资源，如某一static字段，线程会先加载到寄存器，再执行运算，最后存入变量Obj，假设三个线程，第一线程加载入寄存器，随后二三线程也加载并存入Obj,这时第一线程再存入就会覆盖线程二三的结果。 interlocked 可以避免争用条件的问题。前台线程 vs 后台线程注意，所有前台线程退出，后台线程将被中断，应用程序退出主线程和Thread class生成的线程是前台线程，线程池pool则是后台线程创建CPU 线程上下文切换（消耗资源）使用多线程的推荐方法是使用任务并行库 (TPL 类型（例如 Task 和 Task）使用线程池线程来运行任务和并行 LINQ (PLINQ)大量被阻塞的线程池线程可能会阻止任务启动每个进程只有一个线程池，线程池线程是后台线程如果有大于一个的线程竞争这个锁，那么他们将形成称为“就绪队列”的队列，以先到先得的方式授权锁。C#的lock 语句实际上是调用Monitor.Enter和Monitor.Exit，中间夹杂try-finally语句的简略版同步对象必须是引用类型,也建议同步对象最好私有在类里面（比如一个私有实例字段）防止无意间从外部锁定相同的对象threadpool Enqueue TryEnqueue IThreadPoolWorkItem数组原子操作原子操作 指 在cpu的级别上，读取、计算、写入序列不能在任何一个线程上被中断。情况一： 线程A写入期间B线程读取了，这样导致B线程读取了旧数据，并覆盖了A线程的值。Interlocked.Increment()Interlocked.Decrement()Interlocked.Add()情况二： 操作一个int32字段obj时，CPU会先加载到寄存器，操作后再加载到obj，如果是long字段，需要两个cpu周期才会载入到寄存器，两次载入的过程中如果该字段高低32被其他线程修改，这样会导致，线程载入的最终高低八位结果与实际存在差异。所以读取可以这样Interlocked.Read()同样32位设置long字段也可以Interlocked.Exchange()if-and-only-if 如果是某值，则交换Interlocked.CompareExchange()注意 混合 x++ 和 x=x+1 在不同的线程不是线程安全的内容来自于 .NET Interlocked Operations无锁机制ConcurrentQueue 和 ConcurrentStack 类完全不使用锁定。 相反，它们依赖于 Interlocked 操作来实现线程安全性。传统 monitor 是利用内存屏障和cpu指令内存屏障（Memory Barrier）与内存栅栏（Memory Fence）是同一个概念，不同的叫法。Timersystem.Threading.timer在线程池上的空闲线程回调方法Do System.Timers.Timer run in independent Threads?为什么timer要显示 dispose，可能引用了非托管资源，需要手动释放，而处理 IDisposable 实现的实例成员时，通常会级联 Dispose 调用实现 Dispose 方法system.timers.timer 内部也是 system.Threading.timer 实现，同步标识SynchronizingObject.BeginInvoke，底层调用QCall的AppDomainTimer_Create参考.NET Core中的SDK和Runtime有什么区别？深入了解 .NET Framework 内部结构，了解 CLR 如何创建运行时对象将公共语言运行时加载到进程中I/O 完成端口如何工作可靠性最佳做法.NET 中的内存屏障FAQ :: 所有新的并发集合都是无锁的吗？" }, { "title": "gRPC", "url": "/posts/gRPC/", "categories": "dotnet", "tags": "gRPC", "date": "2022-03-28 14:15:29 +0800", "snippet": "RPC是客户端无感调用服务器函数的框架方案，gRPC是框架实现的一种，内容交换格式是protobuf， TCP和HTTP/2传输和应用层协议服务定义的method有4种 一元 客户端流（参数steam） 服务端流 客户端服务端双流相对于 HTTP1.1优点 优化了HTTP/2协议，特别是帧，提高性能 与语言无关，平台无关 内部消息粗传递对用户隐藏(无感)HTTP2协议HTTP1.1 -&amp;gt; HTTP/2-&amp;gt;gRPC内容打包在header/body，\\r\\n 分割内容，在同一个TCP多个请求并发的话，TCP无法区分请求，再引入pipeline，要求服务方按顺序返回并发的请求，chrome在访问同一域中最高6个TCP连接，但网页需并发加载png、js等文件，这就要求TCP的多路复用，也就是HTTP/2,首先在headers采用静态表，这样请求在header只需对于表中的索引就知道是哪个header-key，再引入帧和流，每个请求只有一个流ID，一个流ID由多个帧组成，headers，data等，这样就可以分组拼接起来，形成多路复用，一个TCP并发加载js等文件，而gRPC则优化了帧的数量，比如函数方法放在headers帧中，参数放入data帧中，返回状态headers，返回数据data，优化了执行的效率，另外gRPC中流中只发送函数方法headers，其他都是客户端和服务端的data帧的交替，只有一个流id。protobufprotobuf内容交换格式， google开源， 二进制，序列化速度快，利用Interface Definition Language (IDL)接口描述语言在.proto文件定义接口，再由protoc 转为其他语言接口代码存根客户端/服务端 存根(Stub),syntax = &quot;proto3&quot;;package PublicApi.v1;option csharp_namespace = &quot;GrpcPublicApi&quot;;message GetStaffResponse { string staff_code = 1; string staff_name = 2;}message GetStaffRequest { string staff_code = 1;}service GrpcStaffService { rpc GetStaff (GetStaffRequest) returns (GetStaffResponse);}Client端 客户端install Install-Package Grpc.Net.ClientInstall-Package Google.ProtobufInstall-Package Grpc.Tools Add greet.proto csproj文件 &amp;lt;ItemGroup&amp;gt; &amp;lt;Protobuf Include=&quot;Protos\\greet.proto&quot; GrpcServices=&quot;Client&quot; /&amp;gt;&amp;lt;/ItemGroup&amp;gt; GrpcServices default Both Server and Client Build the client project 自动生成客户端类 在每次生成项目时按需生成。 不会添加到项目中或是签入到源代码管理中。 是包含在 obj 目录中的生成工件。 using var channel = GrpcChannel.ForAddress(&quot;https://localhost:60141&quot;);var client = new GrpcStaffServiceClient(channel);try{ var staff = await client.GetStaffAsync(new GrpcPublicApiClient.GetStaffRequest { StaffCode = id });}catch (RpcException ex) when (ex.StatusCode == StatusCode.NotFound){} Server端public class StaffServiceV1 : GrpcPublicApi.GrpcStaffService.GrpcStaffServiceBase{ public override async Task&amp;lt;GetStaffResponse&amp;gt; GetStaff(GetStaffRequest request, ServerCallContext context) { ... }}public void ConfigureServices(IServiceCollection services){ services.AddGrpc();}public void Configure(IApplicationBuilder app, IWebHostEnvironment env, ILogger&amp;lt;Startup&amp;gt; logger, IHostApplicationLifetime lifetime){ app.UseEndpoints(endpoints =&amp;gt; { endpoints.MapGrpcService&amp;lt;StaffServiceV1&amp;gt;(); });}问题 SSL 未能建立连接 The SSL connection could not be established, see inner exception. IOException: Cannot determine the frame size or a corrupted frame was received. 一开始查stackoverflow，说是win10 补丁支持TLS 1.3，要改注册表disable，后来发现是launchsetting.json以iis express运行，https端口是&quot;sslPort&quot;: 44324,修改client端GrpcChannel.ForAddress(&quot;https://localhost:44324&quot;) mysql远程连接端口 正常调用grpc，updateAsyn后mysql数据库未能找到xf_staff表 端口问题 mysql -h ip -u root --port=3307 -p 一开始 是 -port 3307并没有报错，静默进入mysql ,因为mysql clr 忽略了错误的参数，进入3306的数据库 unimplement, bad grpc response docker 部署了services api_A,api_B，走http协议，api_B 通过 grpc 经由docker_nginx 反向代理访问 api_A服务 api_A &amp;amp; api_B docker-compose api_A: build: context: ./src dockerfile: Dockerfile environment: - ASPNETCORE_URLS=http://+:6001; networks: - backendapi_B: build: context: ./src dockerfile: Dockerfile.TTPublic.prod environment: - ASPNETCORE_URLS=http://+:7001; networks: - backend nginx配置 upstream lsd.api { server lsd_web-api_1:6001;}location /Lsd.staff/ { grpc_pass grpc://lsd.api;} api_B 服务注入 services.AddGrpcClient&amp;lt;GrpcPublicApiClient.GrpcStaffService.GrpcStaffServiceClient&amp;gt;(o =&amp;gt; { o.Address = new Uri(Configuration.GetSection(&quot;GrpcStaffServiceClient&quot;)[&quot;Address&quot;]); }).ConfigureChannel(o =&amp;gt; { //Return `true` to allow certificates that are untrusted/ invalid var httpHandler = new HttpClientHandler(); httpHandler.ServerCertificateCustomValidationCallback = HttpClientHandler.DangerousAcceptAnyServerCertificateValidator; o.HttpHandler = httpHandler; }); api_B appsetting.json &quot;GrpcStaffServiceClient&quot;: { &quot;Address&quot;: &quot;https://dev-nginx/lsd.staff/&quot;} bad grpc response nginx反向代理,以下规则未能生效，实际上，请求Post到root的路径上，未设置的情况下，变成请求 etc/nginx/index….. upstream lsd.api { # 端口 6002 是Https端口 server lsd_web-api_1:6001;}location /lsd.staff { grpc_pass grpc://lsd.api;} 解决上面的问题，workaround方法是监听grpc的端口,client端 address 指向nginx这个代理的端口1433 server { listen 1433 http2; location / { grpc_pass grpcs://lsd.api; }} 上面的方案受 Nginx and ASP.NET Core: Running both, HTTP REST and gRPC services, at once的启发 再后来出现 Grpc.Core.RpcException HResult=0x80131500 Message=Status(StatusCode=&quot;Internal&quot;, Detail=&quot;Request protocol &#39;HTTP/1.1&#39; is not supported.&quot;) 问题，字面上理解应该是HTTP Protocol的问题。 gRPC 走HTTP/2.0协议，AspnetCore 支持 HTTP和HTTPS协议,HTTP URI请求没有在header upgrade 设置 h2c 的情况下默认采用 HTTP/1.1协议，而HTTPS HTTP OVER TLS 协议标识位hc 重点理解 解开 HTTP/2 的面纱：HTTP/2 是如何建立连接的 首先让Kestrel从IConfiguration读取端口配置，见为 ASP.NET Core Kestrel Web 服务器配置终结点 &quot;Kestrel&quot;: { &quot;Endpoints&quot;: { &quot;Htttp&quot;:{ &quot;Url&quot;: &quot;http://*:6001&quot; }, &quot;Https&quot;:{ &quot;Url&quot;: &quot;https://*:6002&quot; }, &quot;Grpc&quot;:{ &quot;Url&quot;: &quot;http://*:6003&quot;, &quot;Protocols&quot;: &quot;Http2&quot; } } } 然后nginx代理到6003或者6002端口 通过HTTP代理HTTPS请求时会出现以下问题，见配置 ASP.NET Core 以使用代理服务器和负载均衡器 当通过 HTTP 代理 HTTPS 请求时，原方案 (HTTPS) 将丢失，并且必须在标头中转接。 由于应用收到来自代理的请求，而不是 Internet 或公司网络上请求的真实源，因此原始客户端 IP 地址也必须在标头中转接。 kestrel 服务器需要install 转接头中间件 builder.Services.Configure&amp;lt;ForwardedHeadersOptions&amp;gt;(options =&amp;gt;{ options.ForwardedHeaders = ForwardedHeaders.XForwardedFor | ForwardedHeaders.XForwardedProto;});app.UseForwardedHeaders(); 而nginx需要配置x-Forwarded-For等，见ASP.NET Core 反向代理部署知多少 proxy_set_header Host $host;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_set_header X-Forwarded-Proto $scheme; 参考使用 C# 的 gRPC 服务gRPC系列(三) 如何借助HTTP2实现传输gRPC for aspnetCore 身份认证和授权详解 HTTP2.0 及 HTTPS 协议Introducing gRPC Support with NGINX 1.13.10" }, { "title": "AspnetCore HttpClient", "url": "/posts/AspnetCore-HttpClient/", "categories": "AspnetCore", "tags": "httpclient", "date": "2022-03-28 11:02:35 +0800", "snippet": "IHttpClientFactoryusing语句实例化HttpClient，会存在套接字耗尽（退出using后套接字没有及时释放，网传维持240s）和无法处理DNS更改问题（因为它的构造函数每次都new HttpMessageHandler）建议采用DI的IHttpClientFactory，此工厂将池的 HttpMessageHandler 分配给 HttpClient，实现套接字复用，客户端可配置和Polly 的容错策略RefitRefit 将您的 REST API 变成了一个实时接口,下面的内容包含了OAuth的认证安装nuget install IdentityModel.AspnetCore;nuget install Refit;代码public interface IApiOperations{ [Get(&quot;/staffs/{id}&quot;)] Task&amp;lt;ApiResponse&amp;lt;Staff&amp;gt;&amp;gt; GetStaff(string id);}services.AddAccessTokenManagement(options =&amp;gt;{ options.Client.Clients.Add(&quot;api&quot;, new ClientCredentialsTokenRequest { RequestUri = new Uri(new Uri(Configuration.GetSection(&quot;Identity&quot;)[&quot;host&quot;]), new Uri(&quot;/identity/connect/token&quot;, UriKind.Relative)), ClientId = &quot;xxx&quot;, ClientSecret = &quot;xxx&quot;, });});Func&amp;lt;HttpClientHandler&amp;gt; handlerConfigure = () =&amp;gt;{ var handler = new HttpClientHandler(); if (env.IsDevelopment() || env.IsStaging()) { handler.ServerCertificateCustomValidationCallback = HttpClientHandler.DangerousAcceptAnyServerCertificateValidator; } return handler;};services.AddHttpClient(AccessTokenManagementDefaults.BackChannelHttpClientName) .ConfigurePrimaryHttpMessageHandler(handlerConfigure);services.AddRefitClient&amp;lt;TClient&amp;gt;() .ConfigureHttpClient( (serviceProvider, httpClient) =&amp;gt; { httpClient.BaseAddress = httpClientOptions.BaseAddress; httpClient.Timeout = httpClientOptions.Timeout; }) .ConfigurePrimaryHttpMessageHandler(configureHandler) .AddPolicyHandlerFromRegistry(PolicyName.HttpRetry) .AddPolicyHandlerFromRegistry(PolicyName.HttpCircuitBreaker) .AddClientAccessTokenHandler(configurationSectionName);Policiesvar policyRegistry = services.AddPolicyRegistry();policyRegistry.Add( PolicyName.HttpRetry, HttpPolicyExtensions .HandleTransientHttpError() .WaitAndRetryAsync( policyOptions.HttpRetry.Count, retryAttempt =&amp;gt; TimeSpan.FromSeconds(Math.Pow(policyOptions.HttpRetry.BackoffPower, retryAttempt))));policyRegistry.Add( PolicyName.HttpCircuitBreaker, HttpPolicyExtensions .HandleTransientHttpError() .CircuitBreakerAsync( handledEventsAllowedBeforeBreaking: policyOptions.HttpCircuitBreaker.ExceptionsAllowedBeforeBreaking, durationOfBreak: policyOptions.HttpCircuitBreaker.DurationOfBreak));HandleTransientHttpError 仅在reponse status code = 408 超时或者 5XX 服务错误时 采用策略测试可以借助 http://httpstat.us/408?sleep=5000 测试不同status code 的响应HttpClient代码public class GitHubService{ private readonly HttpClient _httpClient; public GitHubService(HttpClient httpClient) { _httpClient = httpClient; _httpClient.BaseAddress = new Uri(&quot;https://api.github.com/&quot;); } public async Task&amp;lt;IEnumerable&amp;lt;GitHubBranch&amp;gt;?&amp;gt; GetAspNetCoreDocsBranchesAsync() =&amp;gt; await _httpClient.GetFromJsonAsync&amp;lt;IEnumerable&amp;lt;GitHubBranch&amp;gt;&amp;gt;( &quot;repos/dotnet/AspNetCore.Docs/branches&quot;);}//transientbuilder.Services.AddHttpClient&amp;lt;GitHubService&amp;gt;();//解析指定客户端服务private readonly GitHubService _gitHubService;参考Bypass SSL Certificate in .NET – GuidelinesAuthorizationHeaderValueGetter vs [Header(“Authorization”)]OAuth2 with refit跨微服务共享 DTO 的方法https://rehansaeed.com/optimally-configuring-asp-net-core-httpclientfactory/" }, { "title": "洋葱架构", "url": "/posts/%E6%B4%8B%E8%91%B1%E6%9E%B6%E6%9E%84/", "categories": "架构", "tags": "洋葱架构", "date": "2022-03-25 10:46:12 +0800", "snippet": "洋葱架构洋葱架构的提出是为解决内外层依赖关系的问题，内部的核心Core不因外部基础架构（如数据库访问）的变化而更改，依赖是向Core的，外层依赖内层提供的接口去来实现。洋葱架构不适用于小网站简而言之有以下四点： The system is built around an independent Application Core 系统的设计围绕应用核心展开 The inner layers (inside the core) define interfaces. The outer layers then implement these interfaces. 内层定义接口，外层实现 The outer layers are coupled to the inner layers 内外层耦合，内层接口改动，外部实现要跟着改动 The Application core (inner layers) can be run and separated from the infrastructure and presentation. You can peel them of the onion and still have a working core. 应用核心独立于外部基础架构和表现层 干净架构与洋葱架构的关系The term “Clean Architecture” is just the name of the article. The onion architecture is a specific application of the concepts explained in the article. 相当于汽车与奔驰宝马的关系参考Onion Architecture" }, { "title": "SSO 单点登陆", "url": "/posts/SSO-%E5%8D%95%E7%82%B9%E7%99%BB%E9%99%86/", "categories": "DevOps", "tags": "sso", "date": "2022-03-23 11:03:52 +0800", "snippet": "CASCAS = Central Authentication Service 集中授权服务TGT = Ticket Granting Ticket, stored in the TGC cookie, represents a SSO session for a user 存储在cookie中，代表user的sessionTGC = Ticket Granting Cookie,表示cookie，key-valu形式 SET-Cookie: CASTGC=TGT-2345678ST = Service Ticket， transmitted as a GET parameter in urls 在Get的url中？ticket=XXXX大致流程app A 重定向 CAS，user 登陆，CAS Set-Cookie TGC并重定向app A url+ticket=ST-123456,app A 向CAS Get serviceValidate?ticket=ST-123456，CAS返回200，app A 响应 user request并set-cookie: JSESSIONID=ABCD1234,并重定向回user一开始访问的地址，app A 验证cookie中的sessionId,允许user访问受限资源。登陆app B，重定向CAS，携带着TGC，CAS认证已user登陆以及app B是受信任的，重定向app B 带ticket，其余流程与app A一致。CAS是SSO协议之一，服务于单点登陆，解决是否已登陆过的问题，而OAuth/OIDC协议服务于授权认证,但针对跨域的会话管理新增了补充协议:OpenID Connect 会话管理 ，同时解决了sso和第三方应用授权认证的问题。OIDC问题 app B 请求认证，为什么CAS能识别user已登陆？ 因为，重定向CAS时带上了TGC，CAS验证TGC中的TGT知道了user已登陆 sso向所有系统发出注销请求？ 正确，其他service将注销与此相关的sessionId OIDC如何请求 userInfo Endpoint OIDC面对多个第三方应用如何实现SSO 在博客中看到这样一句: 浏览器携带cookie重定向，再度进入OP授权接口GET op.com/authorization，发现用户已在OP登录（名为pyoidc的cookie校验通过），于是执行授权逻辑，签发OP授权码，重定向到RP的redirect_uri（RP在步骤1中提供的）。 提出疑问：SSO是可信客户端之间的单点登陆，第三方没有授权的情况下，即使user已登陆也不应该给授权码 找到一些资料： Call Oidc.UserManager.signinRedirect() when an access token is already expired bypasses an actual signin. 颁发给第三方应用的令牌已过期，但user再次登陆时会RP会获得新的token，不用输入账户密码，最后提到了OIDC Connect Session Management 前提是user已授权这个第三方应用，OP才会在user已登陆的情况下静默地签发授权码，也就能理解博客中的...于是执行授权逻辑，签发OP授权码这段话 退出时，需要OP和RP联动，前端OP或RP退出，可以利用iframe，OP的iframe负责通知，RP的iframe（html不可见框架）负责监听和退出RP；后端退出，RP要有Post的方法接受OP的通知，包含了sid和sub，哪个user，哪个会话，而RP退出也需要通知OP，id4的后端支持接受退出通知。 关于会话管理，前端通道退出，后端通道退出见从零搭建一个IdentityServer——会话管理与登出 术语CAS ProtocolSAML VS OIDC" }, { "title": "EF Core", "url": "/posts/EF-Core/", "categories": "dotnet", "tags": "ef", "date": "2022-03-22 14:59:27 +0800", "snippet": "性能优化下面8点是对高效查询的总结 正确使用索引, startwith 而不是endwith 预加载而不是延迟加载，一次性查询 指定列返回 分页 skip take buffering缓冲，一次性加载所有到内存， 流式传输 streaming，单个结果地返回，比前者节省内存 不跟踪，不身份解析 raw sql 异步保存 SaveChangesAsync，不阻塞请务必了解 EF Core 始终针对数据库在 DbSet 上执行 LINQ 查询，并且仅根据数据库中的内容返回结果。 但是，对于跟踪查询，如果返回的实体已被跟踪，则使用被跟踪的实例（而不是根据数据库中的数据）创建实例。标识解析将具有相同键值的实体的多个实例解析为单个实例重复选择不重复的根或者序列化重复的，去重var serialized = JsonSerializer.Serialize( posts, new JsonSerializerOptions { ReferenceHandler = ReferenceHandler.Preserve, WriteIndented = true });不存在对于没有设置键值的实体，非identity列，可以设置由数据库决定插入的默认值protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&amp;lt;Blog&amp;gt;() .Property(b =&amp;gt; b.LastUpdated) .ValueGeneratedOnAddOrUpdate();}标识解析的目的是为了更新更新实体的方法 http post 传入blog， dbcontext.update(blog),savechanges 全属性更新 ，数据库一次往返 dbcontext.find 返回实体，在实体上修改，savechanges 启用跟踪，两次往返 dbcontext.find, 再currentvalues.setvalues,可以传入字典或实体，根据属性名匹配,savechanges context.Entry(trackedBlog).CurrentValues.SetValues(blog); 启用跟踪，两次往返，好处是不用逐个修改实体的属性 http post 传入blog 和new blog ,先attach 再currentvalues.setvalues,savechanges 不启用跟踪，一次往返，需要将原先的blog发送到客户端 " }, { "title": "托管与非托管", "url": "/posts/%E6%89%98%E7%AE%A1%E4%B8%8E%E9%9D%9E%E6%89%98%E7%AE%A1/", "categories": "其他", "tags": "托管", "date": "2022-03-21 14:58:20 +0800", "snippet": "堆栈 VS 托管堆堆栈，由编译器向系统申请，从高地址到低地址分配，LIFO（后入先出）的数据结构，每个线程通常分配2M的栈空间而32 位计算机上的每个进程都具有 2 GB的用户模式虚拟地址空间（VMM）,所以理论上该进程可以拥有1024个线程声明int类型变量，push入栈，对应的汇编的指令 push ax运行操作指令（运算）如何访问：栈顶地址+偏移量，也就是说机器码到汇编的时候加载入RAM（随机存储器）时指令集包含了指令要访问的地址和操作符退出函数或线程时将这一段栈内的数据pop出，指令也回到主函数的原先的位置，这样栈被释放的空间将被其他的函数或线程继续使用，这些都将写在指令集里堆栈的访问速度也因指针的上下移动而比堆（二叉树）的寻址要快。托管堆，托管堆与数据结构的堆不同，它是一段连续的内存，而堆是经过排序的二叉树栈中有堆对应的地址，出栈后，堆内的数据将由GC释放,GC是CLR的一部分，GC调用汇编指令delete释放内存.net程序将被编译为MSIL（微软中间语言）存放在PE文件中（.exe或dll），也称为程序集，是编译的一个逻辑单元，包括了程序集清单、图片资源等，加载时由CLR调用JIT转为IL（中间语言，也叫本机代码，汇编），再由cpu执行汇编指令，当IL加载入RAM时会向系统申请堆栈和连续的托管堆内存,每个线程都有它的堆栈,堆始终在所有线程之间共享，即使程序意外退出CLR的垃圾回收算法也能回收延伸非托管类型,int等内置类型，枚举类型，指针类型，struct类型,不由GC管理，系统分配和释放托管类型，引用类型，新建实例时，存放在托管堆内，GC管理托管资源，由GC管理释放的内存非托管资源GC可以跟踪封装非托管资源的对象的生存期，但它不了解具体如何清理这些资源。涉及大量非托管代码：所有文件句柄、数据库连接、网络套接字……所有这些都是普通的非托管 Win32 代码每次打开文件 ( FileStream) 时，您基本上都在调用（当然是在幕后）CreateFile非托管 Win32 函数参考什么是 CLR ？托管执行过程" }, { "title": "表达式树", "url": "/posts/%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%A0%91/", "categories": "其他", "tags": "表达式树", "date": "2022-03-21 14:54:38 +0800", "snippet": "表达式树允许将 lambda 表达式表示为数据结构而不是可执行代码表达式树以树形数据结构表示代码，其中每一个节点都是一种表达式，比如方法调用和 x &amp;lt; y 这样的二元运算等您可以使用 API 方法创建和修改表达式树，而根本不需要使用 lambda 表达式语法。另一方面，并不是每个 lambda 表达式都可以隐式转换为表达式树。例如，多行 lambda（也称为语句 lambda）不能隐式转换为表达式树解释表达式" }, { "title": "营销原则", "url": "/posts/%E8%90%A5%E9%94%80%E5%8E%9F%E5%88%99/", "categories": "其他", "tags": "营销", "date": "2022-03-18 15:27:25 +0800", "snippet": "" }, { "title": "装箱与取消装箱", "url": "/posts/Boxing-and-UnBoxing/", "categories": "dotnet", "tags": "装箱, 取消装箱", "date": "2022-03-17 10:19:52 +0800", "snippet": "装箱在堆栈上新增o，在堆上新增i的副本，包括类型和值取消装箱，如下在没有泛型之前，用于ArrayList，见Add函数 Add(Object) ,现在采用List&amp;lt;T&amp;gt;,装箱比简单的引用赋值花费近20倍的时间，取消装箱则比赋值花费近4倍的时间Boxing and Unboxing (C# Programming Guide)" }, { "title": "LINQ VS Lambda", "url": "/posts/LINQ-vs-lambda/", "categories": "dotnet", "tags": "linq, lambda", "date": "2022-03-16 16:53:23 +0800", "snippet": "LINQ语言集成查询 Language Integrated Query,构造查询不执行任何操作，叫做延迟执行。LINQ技术栈 XML 文档：LINQ to XML ADO.NET 实体框架：LINQ to DataSet，LINQ to SQL，LINQ to Entities .NET 集合、文件、字符串等：LINQ to objects这里的LINQ TO SQL = LINQ TO SQL SERVER,并不支持ORACLE或Mysql，但EF Core支持，因为LINQ providers 抽象了linq表达式到t-sql 或pl/sql的接口和方法参考 微软 EF Core passes a representation of the LINQ query to the database provider. Database providers in turn translate it to database-specific query language (for example, SQL for a relational database).linq构建表达式树，再由providers转化为raw sql示例var q = from s in db.Suppliers join c in db.Customers on s.City equals c.City into sc from x in sc.DefaultIfEmpty() select new { Supplier = s.CompanyName, Customer = x.CompanyName, City = x.City };注意到与SQL查询顺序相反，LINQ 数据源是支持泛型 IEnumerable 接口或从中继承的接口的任意对象System.Linq 命名空间System.Linq 命名空间提供支持某些查询的类和接口，这些查询使用语言集成查询 (LINQ)如投影IEnumerable&amp;lt;int&amp;gt; squares = Enumerable.Range(1, 10).Select(x =&amp;gt; x * x);Lambda使用 Lambda 表达式来创建匿名函数，任何 Lambda 表达式都可以转换为委托类型,多行则不能。public delegate Query(int page, int pageSize);public Query QueryDelegate;//lambda转为委托实例QueryDelegate+=(page,pageSize)=&amp;gt;{ ...}从 C# 9.0 开始，可以使用弃元指定 lambda 表达式中不使用的两个或更多输入参数Func&amp;lt;int, int, int&amp;gt; constant = (_, _) =&amp;gt; 42;LINQ to SQL：关系数据的 .NET 语言集成查询Understanding LINQ to SQL (3) Expression TreeDifference Between LINQ To SQL And Entity Framework" }, { "title": "委托 vs 事件", "url": "/posts/%E5%A7%94%E6%89%98-vs-%E4%BA%8B%E4%BB%B6/", "categories": "dotnet", "tags": "委托, 事件", "date": "2022-03-16 15:50:08 +0800", "snippet": "委托提供后期绑定机制,Delegate类是委托类型的基类，Delegate本身不是委托类型，它是用于派生委托类型的类委托类型 特定委托类型 delegate关键字，编译器会生成一个类，它派生自与使用的签名匹配的 System.Delegate，类中有MethodInfo等属性，用于签名检查 强类型委托 Func和Action是强类型泛型委托 函数指针函数指针只能引用静态函数，而委托可以引用静态和实例方法。 委托不仅存储对方法入口点的引用，还存储对调用方法的对象实例的引用。事件提供后期绑定机制事件类型 特定事件类型例如：public event EventHandler&amp;lt;FileListArgs&amp;gt; Progress;，EventHandler&amp;lt;FileListArgs&amp;gt; 是.NET 事件委托的标准签名实现事件void OnEventRaised(object sender, EventArgs args);签名对应EventHandler委托和事件的区别 返回值 用于事件的委托均具有void的返回类型 内部调用 只有包含事件的类才能调用事件，外部只能添加和删除事件侦听器 生存期 事件源可能会在程序的整个生存期内引发事件 " }, { "title": " OSI 网络7层模式", "url": "/posts/OSI-%E7%BD%91%E7%BB%9C7%E5%B1%82%E6%A8%A1%E5%BC%8F/", "categories": "其他", "tags": "network", "date": "2022-03-15 16:30:00 +0800", "snippet": "记忆法All people seem to need data processing All = Application Layer People = Presentation Layer Seem = Session Layer To = Transport Layer Need = Network Layer Data = Data Link Layer Processing = Physical Layer Application Layer应用层， HTTP、HTTPS、FTP Presentation Layer表现层，提供编码方案和加密/解密，它将应用程序格式转换为网络格式 Session Layer会话层，需要通信时，必须创建一个会话，RDBMS数据流重新同步该层允许进程将称为同步点的检查点添加到数据流中。示例：如果系统正在发送 2500 页的文件，建议在每 100 页后添加检查点，以确保成功接收并独立确认 100 页单元。在这种情况下，如果在第 824 页的传输过程中发生崩溃；然后从第 801 页开始重新传输。无需重新传输第 1 页到第 800 页。 Transport Layer传输层，OSI 模型的核心，负责控制两个设备之间的数据流，TCP协议 Network Layer网络层， 网络层负责数据包转发和路由器之间的数据路由 Data Link Layer数据链路层，以太网 Physical Layer物理层，调制解调器路由器和交换机的区别交换机和路由器的区别路由器在网络层，路由器根据IP地址寻址，路由器可以处理TCP/IP协议，交换机不可以。交换机在中继层，交换机根据MAC地址寻址。中继器的作用中继器主要完成物理层的功能，负责在两个节点的物理层上按位传递信息，完成信号的复制、调整和放大功能，以此来延长网络的长度。由于存在损耗，在线路上传输的信号功率会逐渐衰减，衰减到一定程度时将造成信号失真，因此会导致接收错误。中继器就是为解决这一问题而设计的。TCP协议数据分割 确认数据最终是否送达到对方IPV4地址，子网掩码，子网划分IPv4地址：：={&amp;lt;网络号&amp;gt;，&amp;lt;主机号&amp;gt;}子网划分技术是一种IP地址复用方式网关的IP地址是具有路由功能的设备的IP地址当发向目标地址，路由器根据目标地址与子网列表中的值AND运算，如与子网列表中的网络地址一致，则转发到子网的网关，由网关路由至pc(主机号)。TCT/IP协议报文（message），一般指完整的信息，传输层实现报文交付，位于应用层的信息分组称为报文； 传输层：报文段（segment），组成报文的每个分组； 网络层：分组（packet）是网络传输中的二进制格式单元，数据包（datapacket）是TCP/IP通信协议传输中的数据单位；通过网络传输的数据基本单元，包含一个报头和数据本身，其中报头描述了数据的目的地及其与其他数据之间的关系，可以理解为数据传输的分组，我们将通过网络传输的基本数据单元称为数据报（Datagram）；TCP/IP协议详解建立连接断开连接全双工：意味着，TCP的收发是可以同时进行的半双工：就是指一个时间段内只有一个动作发生，早期的 对讲机单工：模式的数据传输是单向的。通信双方中，一方固定为发送端，一方则固定为接收端。滑动的数据发送窗口HTTP工作原理对于 HTTP，在客户端和服务器可以交换 HTTP 请求/响应之前，它们必须先建立 TCP 连接。因此，HTTP 依赖于 TCP 标准才能成功完成其工作。可以看出，报文没有socket，因为socket在传输层TCP协议中解析，tcp 解析如何访问哪台服务器哪个端口的服务，而http解析向这个服务请求什么资源（cookie,auth,session等）。Socket 的出现只是使得程序员更方便地使用 TCP/IP 协议栈而已，是对TCP/IP协议的抽象，从而形成了我们知道 的一些最基本的函数接口，比如 create、listen、connect、accept、send、read和 write 等。TCP/IP 只是一个协议栈，就像操作系统的运行机制一样，必须要具体实现，同时还要提供对外 的操作接口Telnet协议是TCP/IP协议族中的一员Virtual Network Computing (VNC)" }, { "title": "Linq 语法", "url": "/posts/Linq-%E8%AF%AD%E6%B3%95/", "categories": "dotnet", "tags": "linq", "date": "2022-03-14 16:57:50 +0800", "snippet": "SelectManySelectMany&amp;lt;TSource,TCollection,TResult&amp;gt;(IEnumerable&amp;lt;TSource&amp;gt;, Func&amp;lt;TSource,IEnumerable&amp;lt;TCollection&amp;gt;&amp;gt;, Func&amp;lt;TSource,TCollection,TResult&amp;gt;)关键词 投影，中间序列，结果序列Func&amp;lt;TSource,IEnumerable&amp;lt;TCollection&amp;gt;&amp;gt; 集合选择器，指示如何将1转为多，也就是一个petOwner转换为多个Pets而Func&amp;lt;TSource,TCollection,TResult&amp;gt;) 是结果选择器, 其中TCollection是指集合选择器的结果，这里将对中间序列（也就是集合选择器的结果）进行投影并产生最后的TResult" }, { "title": "Transact-Sql", "url": "/posts/T-Sql/", "categories": "dotnet", "tags": "sql", "date": "2022-03-14 15:16:17 +0800", "snippet": "JOIN INNER JOIN RIGHT OUTER JOIN LEFT OUTER JOIN FULL OUTER JOIN 全外连接，没有匹配的情况下结果会有Null，由此推断匹配项 CROSS JOIN 笛卡尔积，没有ON Clause子句，everything to everything NATURAL JOIN 自然连接，没有ON Clause，先匹配列名，再匹配值 内连接 VS 外连接引用What is the difference between “INNER JOIN” and “OUTER JOIN”? 内连接是intersect（交叉）集，而外连接则是union（联合）集全连接 VS 笛卡尔积引用Difference between FULL JOIN &amp;amp; INNER JOIN cross join + on clause = full join = left join union right join标识列CREATE TABLE 和 ALTER TABLE Transact-SQL 语句一起使用IDENTITY [ (seed , increment) ]唯一性必须通过“PRIMARY KEY”或“UNIQUE”约束或者通过“UNIQUE”索引来实现如果特定 insert 语句失败或回滚该 insert 语句，则使用的标识值会丢失，且不会重新生成。函数@@IDENTITY如果语句未影响任何包含标识列的表，则 @@IDENTITY 返回 NULL作用域是执行该函数的本地服务器上的当前会话。 此函数不能应用于远程或链接服务器。可以在存储过程中使用如果语句触发了一个或多个触发器，该触发器又执行了生成标识值的插入操作，那么，在语句执行后立即调用 @@IDENTITY 将返回触发器生成的最后一个标识值。此场景用SCOPE_IDENTITYT-SQL片段--分页SELECT * FROM tblSoftwareInstalled_logs ORDER BY TimeStamp OFFSET 2 ROWS FETCH NEXT 4 ROWS ONLY--百分比SELECT TOP 10 PERCENT * FROM tblSoftwareInstalled_logs--INSERT多行INSERT INTO Booking(No,Floor,Room,)VALUES (&#39;1&#39;,&#39;2&#39;,&#39;2&#39;),(&#39;5&#39;,&#39;4&#39;,&#39;4&#39;);--子查询WITH CR AS (SELECT * FROM Booking);--窗口函数，分组排名，按MachineId分组，按Id排序，序号为RN，取序号1--外卖平台，每一个用户下单最多的品类--PARTITION BY 分组，未指定的话视每一行为一组，指定后在不同的组内排名的顺序重新开始SELECT * FROM (SELECT MachineId ,ROW_NUMBER() OVER (PARTITION BY MachineId ORDER BY Id DESC ) AS RN FROM tblSoftwareInstalled_detail ) temp WHERE RN = 1--ROW_NUMBER,返回结果集分区内行的序列号，每个分区的第一行从 1 开始。显示（1，2，3，4，5）,没有重复并列的排名--Rank,RANK 函数并不总返回连续整数,具有不确定性，排序中有并列的话，显示（1，2，2，4，5）--ENSE_RANK()，排序中有并列的话，显示（1，2，2，3，4）--NTILE，将有序分区中的行分发到指定数目的组中。 各个组有编号，编号从一开始。 对于每一个行，NTILE 将返回此行所属的组的编号。--重点是指定数目的组NTILE(@NTILE_Var) OVER(PARTITION BY PostalCode ORDER BY SalesYTD DESC)--COALESCE 返回第三个值，和case 对比SELECT COALESCE(NULL, NULL, &#39;third_value&#39;, &#39;fourth_value&#39;);--CHECKSUM，生成哈希索引，可以提高索引速度，无法应用于如下非可比数据类型： text、ntext、image 和 cursorSELECT * FROM tablename WHERE URL = @URL（@URL nvarchar(500) 无法创建索引）alter table tablename add csURL as CHECKSUM(URL)CREATE INDEX URL_index ON tablename (csURL);SELECT * FROM tablename WHERE csURL =CHECKSUM(@URL)--Having的使用SELECT OrderDateKey, SUM(SalesAmount) AS TotalSalesFROM FactInternetSalesGROUP BY OrderDateKeyHAVING SUM(SalesAmount) &amp;gt; 80000ORDER BY OrderDateKey;--PIVOT,通过将表达式中的一个列的唯一值转换为输出中的多列-- Pivot table with one row and five columns-- DaysToManufacture 行转列的列名-- AVG(StandardCost) 列值-- 该结果经过透视以使 DaysToManufacture 值成为列标题，提供一个列表示三 [3] 天，即使结果为 NULL。SELECT &#39;AverageCost&#39; AS Cost_Sorted_By_Production_Days, [0], [1], [2], [3], [4]FROM( SELECT DaysToManufacture, StandardCost FROM Production.Product) AS SourceTablePIVOT( AVG(StandardCost) FOR DaysToManufacture IN ([0], [1], [2], [3], [4])) AS PivotTable;SELECT DATEADD(day, -1, CAST(GETDATE() AS date)) AS YesterdayDate;Useful ShortcutCtrl + Shift + R 刷新智能提示sqlserverguides命名约定CTE 只能在查询期间使用CTE 只能在查询期间使用" }, { "title": "Dotnet 6", "url": "/posts/Dotnet-6/", "categories": "其他", "tags": "语言", "date": "2022-03-11 10:05:43 +0800", "snippet": "dotnet 源码分析.NET Core 编码支持默认情况下，.NET Core 不提供除代码页 28591 以外的其他任何代码页编码和 Unicode 编码，例如 UTF-8 和 UTF-16。nuget install package System.Text.Encoding.CodePages//注册编码表Encoding.RegisterProvider(CodePagesEncodingProvider.Instance);System.Text.Json 自定义字符编码如何使用 System.Text.Json 自定义字符编码UnicodeRanges.All.Net 5 发布于 2020年3月份泛型如果构造函数参数是开放类型，叫泛型类，如Dictionary&amp;lt;string，string&amp;gt;泛型也分绑定和未绑定，如未绑定，如Dictionary&amp;lt;,&amp;gt;但不允许绑定和未绑定一起用，即 Dictionary&amp;lt;,string&amp;gt;见 解析extern 关键字方法是外部实现的[DllImport(&quot;avifil32.dll&quot;)]private static extern void AVIFileInit();volatile 关键字易变的，表示某字段可能会被多线程修改class VolatileTest{ public volatile int sharedStorage; public void Test(int i) { sharedStorage = i; }}结构 vs 类有何不同 释放的成本不同， 值类型是stack 的pop出，而类实例的释放要等GC（标记，清除，压缩（压缩成连续的内存块）） Array数组，值类型要boxing和unxboxing，后来有了泛型，这一点可以忽略。Local variables are stored on stack no matter what type (class or struct) they have. The difference is that a local variable of struct type stores the struct instance and a local variable of reference types stores a reference to the class instance that’s stored on the heap.Fields are stored in the object they belong too. If the object is of reference type then it’s stored on the heap and so are its fields. If the object is of struct type then it may be stored on stack (as a local variable) or on the heap (as a field of another object).struct中有class 成员，stack中存储member的引用，member的数据在heaprecord 关键字用于修饰类或结构，用于相等性比较，引用类型时，值和引用obj一致才相等Choosing Between Class and Struct关于值类型的真相堆栈的实现细节string特殊的引用类型因为其不变性（Immutability）string s1 = &quot;Hello &quot;;string s2 = s1;//s1指向新地址，值等于 Hello World，而s2仍然是旧地址s1 += &quot;World&quot;;System.Console.WriteLine(s2);//Output: Hello频繁拼接字符串的话，建议StringBuilder，内部Buffer.MemCopy实现，不会在堆内冗余过多。按值传参 vs 按引用传参与类不同的是，结构是值类型，不需要进行堆分配。 结构类型的变量直接包含结构的数据，而类类型的变量包含对数据的引用，后者称为对象。数据类型分为值类型和引用类型。 值类型是堆栈分配的，或者是在 结构中内联分配的。 引用类型是堆分配的。类型和值类型都派生自最终基类 Object引用ValueType 是值类型的隐式基类，实现了更适合值类型的Equals(Object)等虚方法传参分三类 按值传参，复制数据的副本； 按引用传参，复制引用的副本； ref/in/out 传参，已非装箱的方式传递值类型的地址，或引用类型的真实引用；关于ref/in/out1) in 值类型，传地址，但不能修改，包括本身和成员； 引用类型，传真实地址，本身不能修改，但成员可以；2) ref 同 in，可以被修改，要求传入前初始化；3) out 同ref，可以被修改，要求返回前赋值；C# 语言规范表达式default(type),this,base…表达式多态性关键词 ： 重写 virtual,Abstract,new, base,sealed面向对象编程OOP（Object-Oriented programming）的四项基本原则为： 抽象：将实体的相关特性和交互建模为类，以定义系统的抽象表示。 封装：隐藏对象的内部状态和功能，并仅允许通过一组公共函数进行访问。 继承：根据现有抽象创建新抽象的能力。 多态性：跨多个抽象以不同方式实现继承属性或方法的能力。Abstraction Encapsulation Inheritance Polymorphism 首字母 AEIPa egg in pocket 一个鸡蛋在口袋弃元if (DateTime.TryParse(dateString, out _)) Console.WriteLine($&quot;&#39;{dateString}&#39;: valid&quot;);var (_, _, _, pop1, _, pop2) = QueryCityDataForYears(&quot;New York City&quot;, 1960, 2010);协变 vs 逆变变体协变和逆变都是术语，前者指能够使用比原始指定的派生类型的派生程度更大（更具体的）的类型，后者指能够使用比原始指定的派生类型的派生程度更小（不太具体的）的类型 。Func 泛型委托的最后一个泛型类型参数指定委托签名中返回值的类型。 该参数是协变的（out 关键字），而其他泛型类型参数是逆变的（in 关键字）。指针unsafe private void* m_value;int* p：p 是指向整数的指针。int** p：p 是指向整数的指针的指针。int[] p：p 是指向整数的指针的一维数组。char p：p 是指向字符的指针。void* p：p 是指向未知类型的指针。new 约束指定泛型类声明中的类型实参必须有公共的无参数构造函数default 约束? 可空注解除非 添加 struct 约束，否则出现警告 #nullable enabledefault约束 允许对 可空注解参数的重写class A2{ public virtual void F2&amp;lt;T&amp;gt;(T? t) where T : struct { } public virtual void F2&amp;lt;T&amp;gt;(T? t) { }}class B2 : A2{ public override void F2&amp;lt;T&amp;gt;(T? t) /*where T : struct*/ { } public override void F2&amp;lt;T&amp;gt;(T? t) where T : default { }}tabcontrol" }, { "title": "分布式锁", "url": "/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/", "categories": "其他", "tags": "分布式锁", "date": "2022-03-10 12:28:45 +0800", "snippet": "Redis 使用INCR加锁 设置过期时间，如自增&amp;gt;1，表示没有获得锁 INCR($key) expire($key,$ttl) 使用SETNX加锁 set if not exist 设置过期时间，避免死锁，如返回0，表示没有获得锁，任务结束需删除锁， SETNX($key,$value) expire($key,$ttl) Del($key) 使用SET加锁 redis 2.6.12 开始 SET($key,$value,[‘nx’,’ex’=&amp;gt;$ttl])1，2方案可以用事务避免获得锁之后没有设置过期时间程序循环请求锁，如前一个任务未完成但已过时，为保证不会删除到其他任务的锁，$value = UUID + threadIDRedLock: 看完这篇文章后请不要有任何疑惑了" }, { "title": "MediatR", "url": "/posts/MediatR/", "categories": "系统框架", "tags": "MediatR", "date": "2022-03-10 11:44:54 +0800", "snippet": "介绍MediatR is a low-ambition library trying to solve a simple problem — decoupling the in-process sending of messages from handling messages.Asp.Net Core引用包MediatR.Extensions.Microsoft.DependencyInjection public void ConfigureServices(IServiceCollection services){ services.AddMvc(); services.AddMediatR(typeof(Startup));}Request/responsepublic class Ping : IRequest&amp;lt;string&amp;gt; { }handlerpublic class PingHandler : IRequestHandler&amp;lt;Ping, string&amp;gt;{ public Task&amp;lt;string&amp;gt; Handle(Ping request, CancellationToken cancellationToken) { return Task.FromResult(&quot;Pong&quot;); }}send a message through the mediator:var response = await mediator.Send(new Ping());Debug.WriteLine(response); // &quot;Pong&quot;Notificationspublic class Ping : INotification { }handlerspublic class Pong1 : INotificationHandler&amp;lt;Ping&amp;gt;{ public Task Handle(Ping notification, CancellationToken cancellationToken) { Debug.WriteLine(&quot;Pong 1&quot;); return Task.CompletedTask; }}public class Pong2 : INotificationHandler&amp;lt;Ping&amp;gt;{ public Task Handle(Ping notification, CancellationToken cancellationToken) { Debug.WriteLine(&quot;Pong 2&quot;); return Task.CompletedTask; }}Publishawait mediator.Publish(new Ping());pipeline在请求前或后执行，await next()public class LoggingBehavior&amp;lt;TRequest, TResponse&amp;gt; : IPipelineBehavior&amp;lt;TRequest, TResponse&amp;gt;{ private readonly ILogger&amp;lt;LoggingBehavior&amp;lt;TRequest, TResponse&amp;gt;&amp;gt; _logger; public LoggingBehavior(ILogger&amp;lt;LoggingBehavior&amp;lt;TRequest, TResponse&amp;gt;&amp;gt; logger) =&amp;gt; _logger = logger; public async Task&amp;lt;TResponse&amp;gt; Handle(TRequest request, CancellationToken cancellationToken, RequestHandlerDelegate&amp;lt;TResponse&amp;gt; next) { _logger.LogInformation(&quot;----- Handling command {CommandName} ({@Command})&quot;, request.GetGenericTypeName(), request); var response = await next(); _logger.LogInformation(&quot;----- Command {CommandName} handled - response: {@Response}&quot;, request.GetGenericTypeName(), response); return response; }}参考MediatR 介绍MediatR 库为什么要使用MediatR？3个原因和1个原因ASP.NET Core中的MediatR管道行为–记录和验证" }, { "title": "AspnetCore Docker部署https", "url": "/posts/AspnetCore-Docker%E9%83%A8%E7%BD%B2https/", "categories": "AspnetCore", "tags": "部署", "date": "2022-03-07 15:05:33 +0800", "snippet": "1. 证书在本机Ubuntu/Windows利用OpenSSL生成自签名的CA root证书(Ubuntu认crt格式)，利用该证书签发IdentityServer和api项目证书（apsnetcore认pfx格式）分别将ca证书和pfx挂载到对应的docker容器中，并让容器信任该ca证书如果出现partialchain问题，可以进入容器apt-get update安装curlapt-get install curl执行检查curl -v IP：prot2. 网络docker-compose 文件中可以设置networks,实现网络隔离同一网络下如何访问：https://container—name:port在IdentityServer中修改IssuerUri，api中修改Authoritydocker命令为容器添加网络 docker networks connect network_name container_name3. MySql抛出异常 mbind: Operation not permitted设置docker环境参数cap_add = SYS_NICE另外要设置密码 MYSQL_ROOT_PASSWORD=1234如何设置远程访问，进入容器 docker exec -it mysql_databse /bin/bash，进入mysqlmysql,切换数据库use mysql，修改root的hostupdate user set host =&#39;%&#39; where user=&#39;root&#39;;执行flush privileged远程访问MySQL容器命令： mysql -h IP -u root -p 4. 自签名证书4.1. Ubuntu$ openssl req -x509 \\ -newkey rsa \\ -outform PEM -out tls-rootca.pem \\ -keyform PEM -keyout tls-rootca.key.pem \\ -days 35600 \\ -nodes \\ -subj &quot;/C=cn/O=mycomp/OU=mygroup/CN=rootca&quot;# 查看证书$ openssl x509 -text -noout -in tls-rootca.pem$ openssl req -newkey rsa:2048 \\ -outform PEM -out tls-intermca.csr \\ -keyform PEM -keyout tls-intermca.key.pem \\ -nodes \\ -extensions v3_ca \\ -config /etc/pki/tls/openssl.cnf \\ -subj &quot;/C=cn/O=mycomp/OU=mygroup/CN=intermca&quot;$ openssl x509 \\ -req -days 365 \\ -in tls-intermca.csr \\ -out tls-intermca.pem \\ -CA tls-rootca.pem \\ -CAkey tls-rootca.key.pem \\ -CAcreateserial \\ -extensions v3_ca \\ -extfile /etc/pki/tls/openssl.cnfextfile = /usr/lib/ssl/openssl.cnf具体可参考 使用openssl创建自签名的证书链4.2. windowsPowerShell脚本具体可参考考 在本地启用 HTTPS 在 Docker 上运行 IdentityServer4 时保护 API" }, { "title": "Nginx 负载均衡", "url": "/posts/Nginx-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/", "categories": "DevOps", "tags": "nginx", "date": "2022-02-28 15:49:03 +0800", "snippet": "https://nginx.org/en/docs/http/load_balancing.html round-robin 轮询 least-connected 最少连接 ip-hash iphash Weighted load balancing 权重 http { upstream myapp1 { server srv1.example.com weight=3; server srv2.example.com; server srv3.example.com; } server { listen 80; location / { proxy_pass http://myapp1; } }} map $http_upgrade $connection_upgrade { default Upgrade; &#39;&#39; close; }map 连接升级通常与 WebSockets 结合使用。在 nginx 中，您可以根据$http_upgrade变量将 HTTP 连接升级为 WebSocket 连接。nginx — 如何修复未知的“connection_upgrade”变量sudo nginx -t 检查配置,just test the configuration filecommandlineserver name 为虚拟服务器的识别路径。因此不同的域名会通过请求头中的HOST字段，匹配到特定的server块，转发到对应的应用服务器中去。Nginx配置server_name详细教程nginx作用 负载均衡 正/反向代理 HTTP服务器（动静分离）关于域名利用nginx配置server_name,监听443/80端口接受的请求（header 中的Host 指定server），代理到其他服务通常有 3 个标头添加到请求标头中，以告诉您的应用程序它是如何从负载均衡器转发的。X-Forwarded-For：接收请求的原始客户端和代理的 IP 地址的逗号空间列表列表。X-Forwarded-Proto：来自原始客户端和代理的方案。X-Forwarded-Host：原始主机头。配置proxy_redirect 替换header中的locationproxy_pass 规则替换规则location /identity/ { proxy_pass https://IdentityServer:5001/;}uri 中 …/identity/ 将被替换为 https://IdentityServer:5001/…命令nginx -s reload-s 代表的是向主进程发送信号。其中信号有 4 个，stop, quit, reopen, reload。透传代理HTTP加密封装在了TLS/SSL中，代理服务器无法看到客户端请求URL中想要访问的域名 HTTP CONNECT隧道 (7层解决方案)客户端给代理服务器发送HTTP CONNECT请求客户端和代理服务器建立起HTTP CONNECT隧道，HTTPS流量到达代理服务器后，直接通过TCP透传给远端目的服务器。代理服务器的角色是透传HTTPS流量，并不需要解密HTTPS。 NGINX stream (4层解决方案)要在不解密的情况下拿到HTTPS流量访问的域名，只有利用TLS/SSL握手的第一个Client Hello报文中的扩展地址SNI (Server Name Indication)来获取$ nginx -t # 检查配置$ nginx -s relaod[emerg] bind() to 0.0.0.0:8086 failed (13: Permission denied)# can not open pid file &#39;/var/run/nginx.pid&#39;$ semanage port -l | grep http_port_t$ semanage port -a -t http_port_t -p tcp 8086$ systemctl restart nginx$ ps -ef |grep nginx$ ps auxZ | grep -v grep | grep nginx # 查看进程安全上下文参考selinux 正向代理 vs 反向代理正向，指user 利用代理服务器 代理自己请求到某个服务器反向，指user无感情况下，请求了代理服务器，而服务器隐身代理了这个请求到后台服务器从user的角度看待正反向使用NGINX作为HTTPS正向代理服务器location 正则 精准匹配location = /imgs { …}= 是精准匹配，注意 mydomain/imgs/index.html 和 mydomain/imgs/ 不会 被匹配到 不分大小写匹配location ~ /imgs { …} 大小写敏感匹配location ~* .(png|ico|gif|jpg|jpeg|css|js)$ { …} 正则匹配，匹配到则停止，继续搜索下一个更好的匹配location ^~ /imgs {…}/imgs/pico.png 会被匹配到# --------------------------------------------------------------------------------------------------------------------------------------------# Search-Order Modifier Description Match-Type Stops-search-on-match# --------------------------------------------------------------------------------------------------------------------------------------------# 1st = The URI must match the specified pattern exactly Simple-string Yes# 2nd ^~ The URI must begin with the specified pattern Simple-string Yes# 3rd (None) The URI must begin with the specified pattern Simple-string No# 4th ~ The URI must be a case-sensitive match to the specified Rx Perl-Compatible-Rx Yes (first match)# 4th ~* The URI must be a case-insensitive match to the specified Rx Perl-Compatible-Rx Yes (first match)# N/A @ Defines a named location block. Simple-string Yes# --------------------------------------------------------------------------------------------------------------------------------------------nginx匹配顺序是先匹配普通location，在匹配正则location； 2.普通匹配规则无顺序无论写在配置文件的那个地方都一样，但是正则匹配则是按照匹配配置文件中由上到下的先后顺序匹配；" }, { "title": "HTTPS", "url": "/posts/HTTPS/", "categories": "其他", "tags": "算法", "date": "2022-02-24 14:54:25 +0800", "snippet": "1. HTTPSHTTPS 相比 HTTP 多了一层 SSL/TLSHTTPS VS HTTP VS HTTP2HTTP 请求资源的协议 GET /staff/id HTTP 1.1，请中HTTP 1.1 表示此次请求的header是基于文本换行的格式所以，HTTP 协议包含了 HTTP/1.1 、HTTP/2HTTP/2 提到了二进制，头部压缩、多路复用，帧，流的概念，与1.1版本的文本换行格式区别开。而 HTTPS 表示 HTTP OVER TLS,指 HTTPS 使用SSL/TLS协议握手并加密报文传输，但没有规定一定要采用HTTP/2协议。HTTP/1.1 消息可以未加密（网站地址以 http:// 开头）或使用 HTTPS 加密（网站地址以 https:// 开头),HTTP/2 与 HTTP/1.1 一样，可通过未加密 (http://) 和加密 (https://) 通道使用，但 Web 浏览器仅通过 HTTPS 支持它，由其决定是使用 HTTP/1.1 还是 HTTP/ 2 作为连接开始时 HTTPS 协商的一部分。 SSL（Secure Socket Layer，安全套接字层）握手协议，记录协议服务端可以向客户端发出 Cerficate Request 消息，要求客户端发送证书对客户端的合法性进行验证。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供USB密钥，里面就包含了一张客户端证书。SSL客户端（也是TCP的客户端）在TCP链接建立之后，发出一个ClientHello来发起握手，这个消息里面包含了自己可实现的算法列表和其它一些需要的消息，SSL的服务器端会回应一个ServerHello，这里面确定了这次通信所需要的算法，然后发过去自己的证书（里面包含了身份和自己的公钥）。Client在收到这个消息后会生成一个秘密消息，用SSL服务器的公钥加密后传过去，SSL服务器端用自己的私钥解密后，会话密钥协商成功，双方可以用同一份会话密钥来通信了。 TLS（Transport Layer Security，传输层安全）TLS 实际上只是 SSL 的更新版本 SSL 1.0 – 由于安全问题从未公开发布。 SSL 2.0 – 1995 年发布。2011 年弃用。存在已知的安全问题。 SSL 3.0 – 1996 年发布。2015 年弃用。存在已知的安全问题。 TLS 1.0 – 于 1999 年作为 SSL 3.0 的升级发布。计划于 2020 年弃用。 TLS 1.1 – 2006 年发布。计划在 2020 年弃用。 TLS 1.2 – 2008 年发布。 TLS 1.3 – 2018 年发布。大多数人仍然将它们称为 SSL 证书的原因基本上是品牌问题。大多数主要证书提供商仍将证书称为 SSL 证书，这就是命名约定保持不变的原因。实际上，您在广告中看到的所有“SSL 证书”实际上都是SSL/TLS 证书2. 算法 对称加密DES（Data Encryption Standard） 长度 56/64 ，容易破解AES（Advanced Encryption Standard） AES的密钥长度是128/192/256位 非对称加密比如RSA算法，在N个人之间通信的时候：使用非对称加密只需要N个密钥对，每个人只管理自己的密钥对。而使用对称加密需要则需要N*(N-1)/2个密钥，因此每个人需要管理N-1个密钥，密钥管理难度大，而且非常容易泄漏。简单理解，你们都有我的公钥，即使被拦截，没有我的私钥也不会泄密，我只需要管理好我的私钥就行了。我先生成一对密钥（公钥和私钥），先用你的公钥加密我的公钥和信息，你收到后就用我的公钥加密消息发给我加了签名，我们就可以保证信息没有被篡改，如果被篡改过，我们连解密都不用了，直接忽视。运算速度非常慢不如，你解密后，我们通过DH算法沟通生成私钥，我们后面就直接用这个私钥沟通，这也是我们在浏览器中常用的HTTPS协议的做法，即浏览器和服务器先通过RSA交换AES口令 哈希散列算法怎么加签名，先了解哈希算法，也叫摘要算法（Digest），相同的输入永远会得到相同的输出，比如MD5、SHA-1、SHA-2、SHA-256网站后台存储密码，要避免彩虹表攻击 常用密码和md5结果的表，黑客对比后知道密码，对应的方法是对每个口令额外添加随机数，这个方法称之为加盐（salt）salt值需要存储起来，以便用户登陆时反向验证 数字签名数字签名就是用发送方的私钥对原始数据进行签名，只有用发送方公钥才能通过签名验证。服务器响应客户端，客户端通过 签名+公钥 解密出hash 来确认在传输过程中没有被篡改数字签名用于： 防止伪造； 防止抵赖； 检测篡改。对原始消息的哈希进行签名signature = encrypt(privateKey, sha256(message)),公钥解密hash = decrypt(publicKey, signature)需要注意，假如一开始就有人发送假公钥给我，假装是你，也就是中间人攻击，加了签名也没法阻止。 数字证书为了证明一开始和我沟通的人就是你，需要证书来证明。服务器发送证书给客户端（浏览器），浏览器利用操作系统自带的CA认证方式认证（链式签名认证），如果公钥可靠，客户端将生成的密钥对中的私钥用证书的公钥加密发送给服务器，中间人拦截网络，无法自证其发出的证书有效。 CSR 即证书签名申请（Certificate Signing Request）获取 SSL 证书，需要先生成 CSR 文件并提交给证书颁发机构（CA）,CSR 包含了公钥和标识名称（Distinguished Name） CRT 即 certificate的缩写，即证书。X.509 是一种证书格式 PEM - Privacy Enhanced Mail以”—–BEGIN…“开头, “—–END…“结尾 内容是BASE64编码 DER Distinguished Encoding Rules二进制格式 Diffie-Hellman算法 Diffie-Hellman算法,非对称加密的基础DH算法是一种密钥交换协议，通信双方通过不安全的信道协商密钥，然后进行对称加密传输我们来看DH算法交换密钥的步骤。假设甲乙双方需要传递密钥，他们之间可以这么做：甲首选选择一个素数p，例如509，底数g，任选，例如5，随机数a，例如123，然后计算A=g^a mod p，结果是215，然后，甲发送p＝509，g=5，A=215给乙；乙方收到后，也选择一个随机数b，例如，456，然后计算B=g^b mod p，结果是181，乙再同时计算s=A^b mod p，结果是121；乙把计算的B=181发给甲，甲计算s＝B^a mod p的余数，计算结果与乙算出的结果一样，都是121。所以最终双方协商出的密钥s是121。注意到这个密钥s并没有在网络上传输。而通过网络传输的p，g，A和B是无法推算出s的，因为实际算法选择的素数是非常大的。所以，更确切地说，DH算法是一个密钥协商算法，双方最终协商出一个共同的密钥，而这个密钥不会通过网络传输。3. 参考非对称加密算法SSL/TLS原理详解 - 云+社区 - 腾讯云RSA加密、解密、签名、验签的原理及方法MD5算法+盐Saltopenssl生成服务器证书时报unable to load number from ./demoCA/serial基于 OpenSSL 的 CA 建立及证书签发pem证书转为pfxssl证书生成与转换(pfx, pem, key, crt)" }, { "title": "AspnetCore Configuration", "url": "/posts/AspnetCore-Configuration/", "categories": "AspnetCore", "tags": "配置", "date": "2022-02-23 11:01:15 +0800", "snippet": "How to use the IOptions pattern for configuration in ASP.NET Core RC2 Properties must have a public Get method Properties must have a public Set method.. Dictionaries must have string keys Unforunately while the binder can bind any properties which are a type that derives from IDictionary&amp;lt;,&amp;gt;, it will not bind an IDictionary&amp;lt;,&amp;gt; property directly. but I guess the common use case is you will be exposing List&amp;lt;&amp;gt; and IList&amp;lt;&amp;gt; etc. Feels like they should be looking for IList&amp;lt;&amp;gt; if that is what they need though!Kestrel终结点配置{ &quot;Kestrel&quot;: { &quot;Endpoints&quot;: { &quot;Http&quot;: { &quot;Url&quot;: &quot;http://localhost:5000&quot; }, &quot;Https&quot;: { &quot;Url&quot;: &quot;https://localhost:5001&quot; }, &quot;Grpc&quot;: { &quot;Url&quot;: &quot;http://localhost:5002&quot;, &quot;Protocols&quot;: &quot;Http2&quot; } } }}https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel/endpoints" }, { "title": "Consul", "url": "/posts/Consul/", "categories": "AspnetCore", "tags": "服务发现", "date": "2022-02-22 12:09:26 +0800", "snippet": "客户端代码var _consulClient = new ConsulClient(c =&amp;gt;{ var uri = new Uri(&quot;http://ip:8701&quot;); c.Address = uri;});var services = _consulClient.Catalog.Services().Result.Response;foreach (var service in services){ var checks = _consulClient.Health .Checks(&quot;Lsd&quot;) .Result; foreach (var checkResult in checks.Response) { Console.WriteLine($&quot;{checkResult.ServiceID} - {checkResult.Status.Status}&quot;); }}服务发现：Zookeeper vs etcd vs ConsulConsul常见的坑Docker-compose部署Consul集群Docker-容器部署Consul集群-云+社区 CONSUL_BIND_INTERFACE 设置为默认桥接网络 eth0 并且主机上不显示任何服务。AspnetCore注册ConsulC#客户端访问Consul服务注册并实现客户端负载均衡使用Consul做服务发现的若干姿势Client 转发注册信息到Server，本身不存储，Server 持久化注册信息，ServerLeader 同步注册到其他Server 服务发现：支持服务发现。你可以通过 DNS 或 HTTP 的方式获取服务信息。 健康检查：支持健康检查。可以提供与给定服务相关联的任何数量的健康检查（如 web 状态码或 cpu 使用率）。 K/V 存储：键/值对存储。你可用通过 consul 存储如动态配置之类的相关信息。 多数据中心：支持多数据中心，开箱即用。mysql WEB-UI：支持WEB-UI。点点点，你就能够了解你的服务现在的运行情况，一目了然，对开发运维是非常友好的。curl $(boot2docker ip):8500/v1/catalog/services" }, { "title": "RESTFUL", "url": "/posts/RESTFUL/", "categories": "DevOps", "tags": "api", "date": "2022-02-21 12:12:51 +0800", "snippet": "Richardson Maturity Model提到成熟度模型： 0级：客户端通过POST向其唯一的 URL 端点发出 HTTP 请求来调用服务 1级：要对资源执行操作，客户端会发出一个POST请求 2级别：使用 HTTP 动词来执行操作 3级别：基本思想是GET请求返回的资源表示包含用于对该资源执行允许操作的链接" }, { "title": "window commands", "url": "/posts/window-commands/", "categories": "其他", "tags": "工具", "date": "2022-02-17 12:19:45 +0800", "snippet": "# smb serverGet-SmbServerConfiguration | Select EnableSMB2Protocol,EnableSMB1Protocol# detectGet-WindowsOptionalFeature -Online -FeatureName SMB1Protocol# detect service is runningsc.exe query mrxsmb10# refresh dnsipconfig ./flushdnsHow to detect, enable and disable SMBv1, SMBv2, and SMBv3 in WindowsHow to Check, Enable or Disable SMB Protocol Versions on Windows?Windows 7 SP1 does not contain any major improvements; it’s basically a rollup of updates that have been released for the operating system since it went to manufacturing July 22 2009. If you have been diligently updating your computer through Windows Update since then, you basically have all that SP1 has to offer.在 Windows 10 上查找 CPU 内核数的 4 种方法如何在 Linux 中从命令行查找 CPU 核心数w32tm /config /manualpeerlist:&quot;ntp1.aliyun.com&quot; /syncfromflags:manual /reliable:yes /update同步系统时钟Enable TLS 1.2 on Windows 7" }, { "title": "分布式 最终一致性", "url": "/posts/%E5%88%86%E5%B8%83%E5%BC%8F-%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7/", "categories": "DevOps", "tags": "一致性", "date": "2022-02-11 10:53:04 +0800", "snippet": " 我们以前谈到过 CAP，知道如果牺牲一定的一致性就可以保证分区容错性和可用性数据库事务可靠原则ACIDWhat does ACID mean in Database Systems?ACID vs. BASE: How is Eventual Consistency Similar to Strong Consistency? In contrast to SQL’s ACID guarantees, NoSQL databases provide so-called BASE guarantees.DBMS以ACID原则设计事务，而Redis利用主从异步的方式保证数据最终一致性 Redis uses a master-slave topology for how to handle eventual consistency, relying on replication from the master node to the slave nodes.原子性all-or-nothing 成功或者没有发生一致性数据一致，指没有违反数据库定，包括triggers，cascades,rules,constraints等隔离性并发中事务之间独立持久性发生了就应被记录下来分布式事务XA扩展架构协议[XA Transactions (2 Phase Commit): A Simple Guide][5]大致分为事务管理器和本地资源管理器Two-Phase Commit2PC是强一致、中心化的原子提交协议分为准备prepare和commit阶段 vote 准备/投票阶段 由协调者发出命令到每一个参与者，参与者响应ack已执行本地事务，资源预留，未提交（vote request），超时再认为投反对 commit 提交阶段 协调者决策，所有参与者Yes，再通知参与者commit事务，如收到任一反对，则由通知其他参与者回滚优点：强一致性，因为一阶段预留了资源，所有只要节点或者网络最终恢复正常，协议就能保证二阶段执行成功；业界标准支持，二阶段协议在业界有标准规范——XA 规范，许多数据库和框架都有针对XA规范的分布式事务实现。缺点：在提交请求阶段，需要预留资源，在资源预留期间，其他人不能操作（比如，XA 在第一阶段会将相关资源锁定） ，会造成分布式系统吞吐量大幅下降；容错能力较差，比如在节点宕机或者超时的情况下，无法确定流程的状态，只能不断重试，同时这也会导致事务在访问共享资源时发生冲突和死锁的概率增高，随着数据库节点的增多，这种趋势会越来越严重，从而成为系统在数据库层面上水平伸缩的”枷锁”；Three-Phase Commit3PC 询问阶段（CanCommit） 准备阶段（PreCommit） 锁定资源 提交阶段（DoCommit）当参与者响应ACK后，即使在指定时间内没收到doCommit指令，也会进行事务的最终提交；一旦进入提交阶段，即使因为网络原因导致参与者无法收到协调者的doCommit或Abort请求，超时时间一过，参与者也会自动完成事务的提交。优点：增加了一个询问阶段，询问阶段可以确保尽早的发现无法执行操作的参与者节点，提升效率；在准备阶段成功以后，协调者和参与者执行的任务中都增加了超时，一旦超时，参与者都会继续提交事务(释放资源)，默认为成功，降低了阻塞范围。缺点：如果准备阶段执行事务后，某些参与者反馈执行事务失败，但是由于出现网络分区，导致这些参与者无法收到协调者的中止请求，那么由于超时机制，这些参与者仍会提交事务，导致出现不一致；性能瓶颈，不适合高并发场景。所以无论是 2PC 还是 3PC，当出现网络分区且不能及时恢复时， 都不能保证分布式系统中的数据 100% 一致。增加系统负载和响应延迟。也正是因为这些问题，三阶段提交协议很少被使用。TCC Try-Confirm-Cancel两阶段提交（2PC）和三阶段提交（3PC）并不适用于并发量大的业务场景。TCC事务机制相比于2PC、3PC，不会锁定整个资源，而是通过引入补偿机制，将资源转换为业务逻辑形式，锁的粒度变小。TCC仍然是一个两阶段提交协议。但是，在执行出现问题的时候，有一定的自我修复能力，如果任何一个事务参与者出现了问题，协调者可以通过执行逆操作来取消之前的操作，达到最终的一致状态（比如冲正交易、查询交易）又称补偿事务，针对每个操作都要注册一个与其对应的确认和补偿（撤销操作）目前相对比较成熟的是阿里开源的分布式事务框架seataConfirm 和 Cancel 应幂等Try 资源预留，比如库存冻结,而不是直接扣减库存（可以理解为真正意义上的锁定资源）Confirm 事务提交 扣减库存Cancel 补偿 释放之前的冻结库存 ，也可以是业务层面的操作，跨库操作优点跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些，当然性能也可以得到提升。缺点TCC模型对业务的侵入性太强，事务回滚实际上就是自己写业务代码来进行回滚和补偿，改造的难度大。一般来说支付、交易等核心业务场景，可能会用TCC来严格保证分布式事务的一致性，要么全部成功，要么全部自动回滚。这些业务场景都是整个公司的核心业务有，比如银行核心主机的账务系统，不容半点差池。但是，在一般的业务场景下，尽量别没事就用TCC作为分布式事务的解决方案，因为自己手写回滚/补偿逻辑，会造成业务代码臃肿且很难维护。引用分布式理论之分布式事务：TCC TCC模型对业务的侵入性太强，事务回滚实际上就是自己写业务代码来进行回滚和补偿，改造的难度大。一般来说支付、交易等核心业务场景，可能会用TCC来严格保证分布式事务的一致性，要么全部成功，要么全部自动回滚。这些业务场景都是整个公司的核心业务有，比如银行核心主机的账务系统，不容半点差池。 但是，在一般的业务场景下，尽量别没事就用TCC作为分布式事务的解决方案，因为自己手写回滚/补偿逻辑，会造成业务代码臃肿且很难维护。本地消息表T2PC、3PC 、TCC 无法保证本地事务和投递消息MQ all-or-nothing，可能会导致数据不一致，所以引入消息表常见问题： 上游服务 1.执行本地事务 2.投递消息 有可能1完成，2因为网络返回失败（但实际已发送MQ），这样1回滚，2已发送，影响下游。 1.投递 2.执行本地，投递完成，2失败，同样影响下游引入消息表1.执行本地事务， 2.更新消息表 3.投递消息将12放在同一个事务执行，新建定时任务扫描消息表，发现已执行未投递的则投递，并更新消息状态未已发送常见问题： 定时任务投递投递后宕机，恢复后重复投递，这种情况要求下游业务支持幂等操作 维护消息表成本 2. 消息处理和业务耦合 3. 消息处理本地数据库（持久化）读写对高并发不友好 4. 定时扫描增加系统间的延迟 所以引入可靠消息可靠消息最终一致性分布式事务中的最终一致性非常通俗易懂，以案例解析本地消息表的实践。在本地消息的基础上新增了可靠消息服务、本地事务表和消息表（待确认，已发送，取消，完全的）上游服务在prepare后会向可靠消息服务发送 待确认和回调接口（询问上游服务是否发送待确认的消息和补偿接口），上游服务将本地事务完成后发送确认信息给消息服务，消息服务在同一个事务中发送消息到MQ并将待确认修改已发送，下游订阅事件，处理完成后ack答复MQ，同时消息服务订阅ack答复事件，收到后在同一个事务中更新消息状态为已完成/取消，消息服务通过上游的回调接口通知上游完成或补偿。1.上游发送准备消息和回调接口到消息服务 2.上游执行本地事务 3.上游发送确认发送消息到消息服务 4.消息服务消息发送到MQ 5.MQ等待下游消费者消费常见问题 23应放在同一个事务中，保证all-or-nothing 消息服务持久化， 上游在3之前宕机，消息服务会通过回调接口查询上游是否可以投递确认消息到MQ，上游决定是否commit或rollback 消息服务如出现故障，将重复投递消息到MQ，所以下游应支持幂等，同样上游的补偿也应支持幂等（判重表）可靠消息最终一致性方案，一般适用于异步的服务调用，比如支付成功后，调用积分服务进行积分累加、调用库存服务进行发货等等。总结一下，可靠消息最终一致性方案其实最基本的思想就两点：通过引入消息中间件，保证生产者对消息的100%可靠投递；通过引入Zookeeper，保证消费者能够对未成功消费的消息进行重新消费（消费者要保证自身接口的幂等性）。优缺点可靠消息最终一致性方案是目前业务主流的分布式事务落地方案，其优缺点主要如下：优点： 消息数据独立存储，降低业务系统与消息系统间的耦合。缺点： 一次消息发送需要两次请求，业务服务需要提供消息状态查询的回调接口。一般来讲，99%的分布式接口调用不需要做分布式事务，通过监控（邮件、短信告警）、记录日志，就可以事后快速定位问题，然后就是排查、出解决方案、修复数据。因为用分布式事务一定是有成本的，而且这个成本会比较高，特别是对于一些中小型公司。同时，引入分布式事务后，代码复杂度、开发周期会大幅上升，系统性能和吞吐量会大幅下跌，这就导致系统更加更加脆弱，更容易出bug。当然，如果有资源能够持续投入，分布式事务做好了的话，好处就是可以100%保证数据一致性不会出错。 Saga模式相比TCC，少了confirmtry 直接提交操作， 服务之间的Saga模式实现 编排式 开发编排器类，发起后通过MQ， 统一管理各个服务的事务和补偿 协同式 各个服务通过MQ互相订阅事件 分布式系统理论基础 - 一致性、2PC和3PC分布式系统CAP定理CAP 定理中的“CAP”解释分布式系统理论上只能满足任意两个特性，而牺牲另一个。一致性不同节点（集群服务）获取数据是一致的可用性即使某个节点关闭，对所有请求都有响应，不会抛出异常或超时分区容忍性指系统能够容忍节点之间的网络通信的故障节点之间连接丢失，集群依然能正常工作，虽然不能保证数据的正确。因为网络原因，分区成为必不可少的因素，一般选择CP，或AP定理去设计。分布式系统BASE理论Basically Available/Soft State/Eventually consistent对AP定理的实际总结，强调可用性部分可用 Basically Available当系统发生错误时，为保证核心功能可用，可以牺牲一些其他功能，可以通过下面的方式实现流削峰 分时间段打包请求 MQ 削峰填谷延迟响应先响应”正在处理中…“,再notify通知user服务降级请求高保真图片变为低分辨率图片过载保护/熔断熔断后直接拒绝接下来的请求软状态 Soft State返回中间值，而不是最终值，中间值不是随便的value，只是不是最终的。最终一致性 Eventually consistent当某一节点出现错误，同步节点的数据将被缓存，在节点恢复后重新提交数据一致性模型 强一致性 弱一致性，有不一致窗口期 最终一致性，没有确定的窗口期，但最终会一致图解分布式之：最终一致性，一致只会迟到，但绝不缺席[5]:https://dzone.com/articles/xa-transactions-2-phase-commit事务隔离现象多了，少了，变了 Dirty Read 脏读 AB事务，同一查询，A事务查询到B事务提交或未提交的更改，数据集变了 Non Repeatable read 不可重复读 AB事务，同一查询，A事务查询不到B事务删除的数据，数据集少了 Phantom Read 幻读 AB事务，同一查询，A事务读到B事务提交或未提交的数据，数据集多了 数据库隔离基于上面三种现象，SQL定义了以下4种隔离级别 Read Uncommitted 读未提交 Read Committed 读已提交,它通过指定语句不能读取已由其他事务修改但尚未提交的数据值来防止脏读。 Repeatable Read 可重复读,任何其他事务都不能修改或删除已由当前事务读取的数据。 由于读取数据上的共享锁一直保持到事务结束 Serializable 序列化, 它包含 REPEATABLE READ 并添加了限制，即在事务完成之前，其他事务不能将新行插入到事务已读取的范围中 snapshot 快照隔离，利用tempdb和row version行版本复制（行副本，带事务序列号TNS） 快照事务始终使用乐观并发控制，这会保留阻止其他事务更新行的任何锁。 如果快照事务尝试提交对事务开始后更改的行的更新，则将回滚该事务，并引发错误。 事务将看到事务开始时存在的所有数据，而不会对基础表执行或设置任何锁。 在存在争用的情况下，这可能导致性能改进 SQL Server default Isolation level Read Committed交付保证幂等性SET TRANSACTION ISOLATION LEVEL REPEATABLE READ; 手动修改隔离级别https://docs.microsoft.com/zh-cn/dotnet/framework/data/adonet/sql/snapshot-isolation-in-sql-server" }, { "title": "AspnetCore Message Bus", "url": "/posts/AspnetCore-Message-Bus/", "categories": "AspnetCore", "tags": "Bus", "date": "2022-01-20 14:51:45 +0800", "snippet": "数据结构环形队列Circular Queue Data Structure关键在求余数 、head 和 rear 指针的位置与传统队列不同，传统队列FIFO，queue 的内部实现是array，当容量不够的时候会按MinimumGrow = 4或_growFactor（成长因子） Array.Copy 扩充，当出列Dequeue时_array[_head] = null; ,这样无法再次利用数组空闲的内存空间。而环形队列定义了固定的数组，并通过求余数和head、rear指针的关系再此利用内存场景1. 高效延时消息设计与实现例如：滴滴打车订单完成后，如果用户一直不评价，48小时后会将自动评价为5星。一般来说怎么实现这类“48小时后自动评价为5星”需求呢？传统定时器低效，可以创建一个包含3600个slot的环形队列，每个slot可以放set或queue类型,并定期内存优化 当我们用 修剪多余的空间时TrimExcess()，队列的内存开销会缩小。但是当我们随后添加一个新元素时，必须再次调整底层数组的大小。所以在内存优化和昂贵的数组重新分配之间需要找到一个平衡点。消息总线和消息代理的区别代理一般会实现点对点， 而总线实现发布订阅 hub（集线器）-spoke（辐条） 模式微服务间通信 HTTP 同步协议 AMQP 异步协议 RabbitMQ 事件驱动 webhook 异步微服务集成强化了微服务的自治性进一步 RabbitMQ 负载均衡 削峰填谷 消息代理不需要服务发现 pgbouncerBuilding event driven microservices that scales提到 What’s important to note here is, the sign-up service will talk to the broker and the broker simply acknowledges the receipt of the message. There are no guarantees that the event has been processed or not.消息代理只保证下游已收到，并不提供给上游，下游处理的结果nservicebus vs masstransit 两者都适合做消息代理，但nservicebus并不是商用免费服务发现提到： There are two types of service discovery: Server-side and Client-side. Server-side service discovery allows clients applications to find services through a router or a load balancer. Client-side service discovery allows clients applications to find services by looking through or querying a service registry, in which service instances and endpoints are all within the service registry. 服务实例 服务注册表客户端发现服务发现分服务端和客户端 服务端发现客户端通过一个负载均衡器向服务发送请求，负载均衡器查询服务注册表并把请求路由到一台可用的服务实例上。 客户端发现客户端访问服务注册表，获取可用服务列表 然后客户端使用一种负载均衡算法选择一个可用的服务实例然后发起请求。服务注册和客户端耦合，不同语言的客户端，需要开发不同版本的组件与服务注册中心对接服务注册表 Self-Registration模式服务实例自己负责通过服务注册表对自己进行注册和注销 Third-Party Registration服务实例本身并不负责通过服务注册表注册自己，相反的，通过另一个被称作 service registrar系统组件来处理注册etcdconsulApache Zookeeper A service registry consists of a cluster of servers that use a replication protocol to maintain consistency.服务注册包括了集群服务器和主从来维护一致性参考消息总线（MQ）知多少下面是介绍 array 、heap和stack的关系，传array 实际上是传 address 到stack 里Arrays, heap and stack and value types集合复杂度" }, { "title": "visual studio 常见问题", "url": "/posts/visual-studio-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/", "categories": "其他", "tags": "IDE", "date": "2022-01-19 14:25:58 +0800", "snippet": " vs 2022 安装后启动卡在重启的界面stuck-in-restart-requiredremove 注册表HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Microsoft\\VisualStudio\\Setup\\Reboot" }, { "title": "AspnetCore Meditor", "url": "/posts/AspnetCore-Meditor/", "categories": "AspnetCore", "tags": "中介者模式", "date": "2022-01-18 14:51:34 +0800", "snippet": "1. 源码1.1. 项目 MediatR.Extensions.Microsoft.DependencyInjection public static IServiceCollection AddMediatR(this IServiceCollection services, params Assembly[] assemblies) =&amp;gt; services.AddMediatR(assemblies, configuration: null); public static IServiceCollection AddMediatR(this IServiceCollection services, IEnumerable&amp;lt;Assembly&amp;gt; assemblies, Action&amp;lt;MediatRServiceConfiguration&amp;gt;? configuration) { if (!assemblies.Any()) { throw new ArgumentException(&quot;No assemblies found to scan. Supply at least one assembly to scan for handlers.&quot;); } var serviceConfig = new MediatRServiceConfiguration(); configuration?.Invoke(serviceConfig); //注册项目默认服务 ServiceRegistrar.AddRequiredServices(services, serviceConfig); //注册程序集相关接口继承类 ServiceRegistrar.AddMediatRClasses(services, assemblies, serviceConfig); return services; }This registers: IMediator as transient IRequestHandler&amp;lt;&amp;gt; concrete implementations as transient INotificationHandler&amp;lt;&amp;gt; concrete implementations as transient IStreamRequestHandler&amp;lt;&amp;gt; concrete implementations as transient IRequestPreProcessor&amp;lt;&amp;gt; concrete implementations as transient IRequestPostProcessor&amp;lt;,&amp;gt; concrete implementations as transient IRequestExceptionHandler&amp;lt;,,&amp;gt; concrete implementations as transient IRequestExceptionAction&amp;lt;,&amp;gt;) concrete implementations as transientThis also registers open generic implementations for: INotificationHandler&amp;lt;&amp;gt; IRequestPreProcessor&amp;lt;&amp;gt; IRequestPostProcessor&amp;lt;,&amp;gt; IRequestExceptionHandler&amp;lt;,,&amp;gt; IRequestExceptionAction&amp;lt;,&amp;gt;注册泛型 var multiOpenInterfaces = new[] { typeof(INotificationHandler&amp;lt;&amp;gt;), typeof(IRequestPreProcessor&amp;lt;&amp;gt;), typeof(IRequestPostProcessor&amp;lt;,&amp;gt;), typeof(IRequestExceptionHandler&amp;lt;,,&amp;gt;), typeof(IRequestExceptionAction&amp;lt;,&amp;gt;) }; foreach (var multiOpenInterface in multiOpenInterfaces) { var arity = multiOpenInterface.GetGenericArguments().Length; var concretions = assembliesToScan .SelectMany(a =&amp;gt; a.DefinedTypes) .Where(type =&amp;gt; type.FindInterfacesThatClose(multiOpenInterface).Any()) .Where(type =&amp;gt; type.IsConcrete() &amp;amp;&amp;amp; type.IsOpenGeneric()) .Where(type =&amp;gt; type.GetGenericArguments().Length == arity) .Where(configuration.TypeEvaluator) .ToList(); foreach (var type in concretions) { services.AddTransient(multiOpenInterface, type); } }IsOpenGeneric 中调用 Type.IsGenericTypeDefinition 判断 是否为泛型 typeof(List&amp;lt;&amp;gt;).IsGenericTypeDefinition true typeof(List&amp;lt;&amp;gt;).IsGenericTypeDefinition false//定义委托public delegate object ServiceFactory(Type serviceType); //委托扩展方法，factory 相当于 GetRequiredService public static class ServiceFactoryExtensions { public static T GetInstance&amp;lt;T&amp;gt;(this ServiceFactory factory) =&amp;gt; (T) factory(typeof(T)); public static IEnumerable&amp;lt;T&amp;gt; GetInstances&amp;lt;T&amp;gt;(this ServiceFactory factory) =&amp;gt; (IEnumerable&amp;lt;T&amp;gt;) factory(typeof(IEnumerable&amp;lt;T&amp;gt;)); }//工厂方法，传入Microsoft.Extensions.DependencyInjection.IServiceCollection，在接下来的项目中作为解析实例工厂services.TryAddTransient&amp;lt;ServiceFactory&amp;gt;(p =&amp;gt; p.GetRequiredService);//将IOC自带的GetRequiredService解析为ServiceFactory 委托实例public static T GetRequiredService&amp;lt;T&amp;gt;(this IServiceProvider provider) where T : notnull;The difference between GetService() and GetRequiredService() in ASP.NET Core GetRequiredService 找不到服务时会抛出异常，而GetService会返回null，建议采用GetRequiredServiceRequestHandlerWrapperImpl 类 public override Task&amp;lt;TResponse&amp;gt; Handle(IRequest&amp;lt;TResponse&amp;gt; request, CancellationToken cancellationToken, ServiceFactory serviceFactory) { Task&amp;lt;TResponse&amp;gt; Handler() =&amp;gt; GetHandler&amp;lt;IRequestHandler&amp;lt;TRequest, TResponse&amp;gt;&amp;gt;(serviceFactory).Handle((TRequest) request, cancellationToken); //累加器嵌套调用 return serviceFactory .GetInstances&amp;lt;IPipelineBehavior&amp;lt;TRequest, TResponse&amp;gt;&amp;gt;() .Reverse() .Aggregate((RequestHandlerDelegate&amp;lt;TResponse&amp;gt;) Handler, (next, pipeline) =&amp;gt; () =&amp;gt; pipeline.Handle((TRequest)request, cancellationToken, next))(); }NotificationHandlerWrapperImpl 类public class NotificationHandlerWrapperImpl&amp;lt;TNotification&amp;gt; : NotificationHandlerWrapper where TNotification : INotification{ public override Task Handle(INotification notification, CancellationToken cancellationToken, ServiceFactory serviceFactory, Func&amp;lt;IEnumerable&amp;lt;Func&amp;lt;INotification, CancellationToken, Task&amp;gt;&amp;gt;, INotification, CancellationToken, Task&amp;gt; publish) { //linq select 来遍历通知 var handlers = serviceFactory .GetInstances&amp;lt;INotificationHandler&amp;lt;TNotification&amp;gt;&amp;gt;() .Select(x =&amp;gt; new Func&amp;lt;INotification, CancellationToken, Task&amp;gt;((theNotification, theToken) =&amp;gt; x.Handle((TNotification)theNotification, theToken))); return publish(handlers, notification, cancellationToken); }}2. 总结扩展按一定的顺序注册程序集中接口继承，再根据request type 反射 相关WrapperImpl，Wrapper解析接口，嵌套或遍历执行IPipelineBehavior 解析 之前注册的 RequestPreProcessorBehavior和RequestPostProcessorBehavior，而这两者的构造函数均解析了一组IRequestPreProcessor 和IRequestPostProcessor，利用上面的累加器嵌套和 next.Handle 执行的位置决定 pre 会在 IRequestHandler前执行，而 post 则在其之后执行，而非泛型会在泛型之前，这则由扩展注册的顺序（AddMediatRClasses）决定。调用的顺序： Send 根据type构造RequestHandlerWrapperImpl RequestHandlerWrapperImpl.Handle 解析 IPipelineBehavior IPipelineBehavior解析一组Processor,并在内部Handle方法中利用**累加器**迭代执行Processor.process 本质上是通过IOC解析和累加器嵌套实现解耦 publish NotificationHandlerWrapperImpl INotificationHandler 利用linq Select 遍历执行（通知）INotificationHandler3. 知识点3.1. Array转化为IEnumerableparams Assembly[] assemblies 可以转化为 IEnumerable assemblies,因为Array继承了IEnumerable In the .NET Framework version 2.0, the Array class implements the System.Collections.Generic.IList, System.Collections.Generic.ICollection, and System.Collections.Generic.IEnumerable generic interfaces.3.2. linq Aggregate 累加器using System;using System.Linq;namespace LINQDemo{ class Program { static void Main(string[] args) { int[] intNumbers = { 3, 5, 7, 9 }; int result = intNumbers.Aggregate(2, (n1, n2) =&amp;gt; n1 * n2); Console.WriteLine(result); Console.ReadKey(); } }Step1: First it multiplies (2*3) to produce the result as 6Step2: Result of Step 1 i.e. 6 is then multiplied with 5 to produce the result as 30Step3: Result of Step 2 i.e. 30 is then multiplied with 7 to produce the result as 210.Step4: Result of Step 3 i.e. 210 is then multiplied with 9 to produce the final result as 1890.4. 引用中介者模式MediatR仓库MediatR.Extensions.Microsoft.DependencyInjection" }, { "title": "AspnetCore RabbitMQ", "url": "/posts/AspnetCore-RabbitMQ/", "categories": "AspnetCore", "tags": "rabbitmq", "date": "2022-01-18 14:30:20 +0800", "snippet": "MassTransit类似于EF core 的作用，让开发者更容易与RabbitMQ、kafka等消息代理系统交互（等同于EF core 与MySQL、MSSQL、Sqlite的关系）。当然，我们也可以引用RabbitMQ等提供的原生Client支持。长远来看，如果想让系统更具兼容性（面对抽象接口开发），MassTransit更为合适。Topology Topology is how message types are used to configure broker topics (exchanges in RabbitMQ) and queues.如何 利用 message type 配置 交换机和队列的关系比如，type-&amp;gt;创建交换机 OrderSystem.Events.OrderSubmitted， 路由到交换机Order-Submitted-Event,再传递给队列Order-Submitted-EventRabbitMQ配置rabbitmq 也就是broker，经纪人， 而masstransit 定义了broker topology(经纪人拓扑) 来 生成 交换机、队列、以及它们之间的binding关系系统与rabbitmq交互，不会直接发信息到queue，而是经过exchange路由到queue举例namespace OrderSystem.Events{ public interface OrderSubmitted { string OrderId { get; } DateTime OrderDate { get; } }}class OrderSubmittedEventConsumer : IConsumer&amp;lt;OrderSubmitted&amp;gt;{ public async Task Consume(ConsumeContext&amp;lt;OrderSubmitted&amp;gt; context) { .... }}services.AddMassTransit(x =&amp;gt;{ x.AddConsumer&amp;lt;OrderSubmittedEventConsumer&amp;gt;(); //Default = OrderSubmittedEvent; Snake Case= Order_Submitted_Event;Kebab Case = Order-Submitted-Event x.SetKebabCaseEndpointNameFormatter(); x.UsingRabbitMq((context, cfg) =&amp;gt; { cfg.ReceiveEndpoint(&quot;order-events-listener&quot;, e =&amp;gt; { e.ConfigureConsumer&amp;lt;OrderSubmittedEventConsumer&amp;gt;(context); }); }); /*endpoint queue name 等于 OrderSubmittedEvent x.UsingRabbitMq((context, cfg) =&amp;gt; cfg.ConfigureEndpoints(context)); DiscardFaultedMessages 发送错误不入 _error 队列 DiscardSkippedMessages 没有consumer，不入 _skipped 队列 */});message type ：OrderSystem.Events.OrderSubmittedconsumer : OrderSubmittedEventConsumerqueue： order-events-listener AspnetCore,Using the ConfigureEndpoints method will automatically create a receive endpoint for every added consumer因为message type会创建exchange，所以即使没有consumer在该项目中，也能利用message topology send/publish 到 exchange ，但没有queue绑定再在另外一个项目中定义consumer，这时会创建另一个exchange和queue，这样就可以打通和上一个项目的通道。SendSend 的方式有三种 ConsumeContext ISendEndpointProvider IBus后面两种是Di解析出来的，所有需要提前注入，第一种是在Consume的同时利用上下文Send出去masstransit Send信息到 rabbitmq 终端地址有两种exchange:order-events-listener如exchange不存在，自动创建queue:order-events-listener如exchange或queue不存在，会创建同名的exchange和queue并bindingPublishpublished信息到 rabbitmq 的话方式有三种 ConsumeContext IPublishEndpoint IBusawait publishEndpoint.Publish&amp;lt;OrderSubmitted&amp;gt;(new{ OrderId = &quot;27&quot;, OrderDate = DateTime.UtcNow,});如exchange不存在，自动创建 exchange OrderSystem.Events:OrderSubmitted,这个exchange会绑定另外一个exchange order-events-listener,消息再路由到 queue order-events-listenerSend vs PublishA command tells a service to do something.An event signifies that something has happened.message name 主要分两种， Commands 用send， Events 用PublishCommands UpdateCustomerAddress UpgradeCustomerAccount SubmitOrderEvents CustomerAddressUpdated CustomerAccountUpgraded OrderSubmitted, OrderAccepted, OrderRejected, OrderShipped自动创建exchange和queue会根据首字母大小写加分隔符 比如 OrderSubmittedEventConsumer = Order-Submitted-Event异常 当没有consumer时，send/publish message to exchange,会被放置在Order-Submitted-Event_skipped 队列 当aspnetcore 在consume 中throw exception，会被放置在 Order-Submitted-Event_error 队列基本用法AddMassTransit委托定义：Action configure内部方法：ServiceCollectionBusConfigurator：RegistrationConfigurator AddConsumer委托调用：configure?.Invoke(configurator);作用：委托传出configurator，提供了AddConsumer的方法AddConsumerActivator.CreateInstance ConsumerDefinition 转为IRegister 通过 IContainerRegistrar.GetOrAdd 注册为ScopedTConsumer 注入为scope collection.TryAddScopedEndpoint以上面同样的方式注册为ScopedAddMassTransitComponentscollection.TryAddScoped(GetCurrentPublishEndpoint) new PublishEndpoint(…) 注册为scoped，解析为IPublishEndpointIPublishEndpoint.Publish 调用 sendEndpoint.SendUsingRabbitMqIBusRegistrationConfigurator.SetBusFactoryServiceCollectionBusConfigurator实现了IBusRegistrationConfigurator，并在SetBusFactory中注入依赖，其中调用了RabbitMqRegistrationBusFactory.CreateBus(嵌套了基类的CreateBus)生成IBusInstance，，生命周期是单例。其中参数列表中Action&amp;lt;IBusRegistrationContext, IRabbitMqBusFactoryConfigurator&amp;gt; configure,则表示你可以在生成bus前修改某些配置BusRegistrationContext提供configure Consumer/Endpoint 的方法IRabbitMqBusFactoryConfigurator对应Endpoint注意，IRabbitMqBusFactoryConfigurator定义了接口方法Host,意味着UsingRabbitMq可以通过委托修改hostsettingvoid Host(RabbitMqHostSettings settings);IBus的Publish原理MassTransitBus构造函数中传入IReceiveEndpointConfiguration，并初始化PublishEndpoint，后续Publish中调用PublishEndpoint.PublishPublishEndpoint调用PublishInternal，通过PublishEndpointProvider.GetPublishSendEndpoint获取SendEndpoint列表，再依次调用SendEndpoint.SendSendEndpoint调用(RabbitMqSendTransport)ISendTransport.SendISendTransport.Send调用(SendTransportContext)RabbitMqSendTransportContext.Send 中 实例化并传入SendPipeSendTransportContext调用(PipeContextSupervisor) IModelContextSupervisor.SendIModelContextSupervisor.Send调用SendPipe.Send(context)SendPipe.Send 中调用 RabbitMqModelContext.BasicPublishAsyncRabbitMqModelContext.BasicPublishAsync 调用 ImmediatePublisher.PublishImmediatePublisher.Publish 调用 RabbitMQ.Client.IModel.BasicPublishMasstransit规则UsingRabbitMq中ConfigureEndpoints(context)将在rabbitMQ上以SetKebabCaseEndpointNameFormatter方式自动配置exchange和queueexchange: 命名空间+:+interfaceName,如Lsd.Events:StaffUpdated,类型fanout，durable staff-updated，接口名短横线隔开,类型fanout，durablequeue： staff-updated,binding from exchange(staff-updated)，同名交换机webapi 控制器中注入IPublishEndpoint，_publishEndpoint.Publish(...)将发送staff-updated exchange, routingkey = staff-updated,即匹配queue staff-updated在不同的项目中，共用相同命名空间的接口（shared project），可以打通默认规则的sender/publisher对应的consumeraspnetcore 依赖注入 IBusControl（单例） IBus（单件） ISendEndpointProvider（范围） IPublishEndpoint（范围） Send发送信息到端点如果是注入ISendEndpointProvider，需要通过EndpointConvention匹配endpoint或者通过GetSendEndpoint或者endpoint Publish广播消息到订阅了该消息类型的所有消费者consumersIPublishEndpoint,则与默认规则一致，或在UsingRabbitMq中手动配置Message(…交换机)和Send(…routingkey)MediatR与 MassTransit 不同，目的是为了解耦，避免Controller变得越来越臃肿，适用于in-process的信息交互基本用法先定义IRequest/INotification 、IRequestHandler/INotificationHandler，在Controller中通过解析的IMediator调用Publish,方法返回Task/TaskRabbitMQ with ASP.NET Core – Microservice Communication with MassTransit.NET Core微服务之基于MassTransit实现数据最终一致性（Part 1） MassTransit on RabbitMQ in ASP.NET Core流量冲击MQ-client提供拉模式，定时或者批量拉取，可以起到削平流量，下游自我保护的作用（MQ需要做的）要想提升整体吞吐量，需要下游优化，例如批量处理等方式（消息接收方需要做的）" }, { "title": "AspnetCore HealthCheck", "url": "/posts/AspnetCore-HealthCheck/", "categories": "AspnetCore", "tags": "健康检查", "date": "2022-01-17 17:46:45 +0800", "snippet": "引用Health checks in ASP.NET Core" }, { "title": "AspnetCore Fault Tolerance", "url": "/posts/AspnetCore-Fault-Tolerance/", "categories": "AspnetCore", "tags": "容灾", "date": "2022-01-17 15:53:28 +0800", "snippet": "1. 容错1.1. 超时与重试（Timeout and Retry）1.2. 电路熔断器(Circuit Breaker)1.3. 舱壁隔离(Bulkhead Isolation)1.4. 回退(Fallback)也称服务降级 自定义处理：在这种场景下，可以使用默认数据，本地数据，缓存数据来临时支撑，也可以将请求放入队列，或者使用备用服务获取数据等，适用于业务的关键流程与严重影响用户体验的场景，如商家/产品信息等核心服务。 故障沉默（fail-silent）：直接返回空值或缺省值，适用于可降级功能的场景，如产品推荐之类的功能，数据为空也不太影响用户体验。 快速失败（fail-fast）：直接抛出异常，适用于数据非强依赖的场景，如非核心服务超时的处理。 2. 限流(Rate Limiting/Load Shedder)分速率限流（rate limit）和节流 （throttle）速率限流利用队列控制访问资源的速率，节流则利用Semaphore信号量控制并发线程，达到瓶颈时服务降级var semaphoreSlim = new SemaphoreSlim( initialCount: 10, maxCount: 10);await semaphoreSlim.WaitAsync();semaphoreSlim.Release();Semaphore控制并发2.1. 熔断2.2. 限流算法2.2.1. 计数器算法 Semaphore (信号量，控制线程数或者数据库链接数)2.2.2. 令牌桶算法(Token Bucket)2.3. 参考Build Resilient Microservices (Web API) using Polly in ASP.NET Core服务容错模式Token Bucket" }, { "title": "vscode keymap", "url": "/posts/vscode-keymap/", "categories": "其他", "tags": "工具", "date": "2022-01-17 11:54:11 +0800", "snippet": "1. 折叠ctrl+k, ctrl+o 折叠所有ctrl+k，ctrl+l 折叠当前块ctrl+k, ctrl+j 展开所有2. 打开资源管理器ctrl+alt+E, shift+alt+R 组合打开[`]?e3\\w+[`]?.[`]?\\w+[`]?匹配 百胜 tableslist to sql in query install List to SQL IN Query (…) builder open keybinding.json { &quot;key&quot;: &quot;ctrl+cmd+/&quot;, &quot;command&quot;: &quot;extension.list-to-in-query.new-line&quot;} vscode-sqltools" }, { "title": "AspnetCore FluentValidator", "url": "/posts/AspnetCore-FluentValidator/", "categories": "AspnetCore", "tags": "FluentValidator", "date": "2022-01-14 16:29:46 +0800", "snippet": "传统上，.NET中的大多数验证都是使用数据注释来完成的public class SampleClass{ [Required] public int Id { get; set; } [MaxLength(100)] public string Name { get; set; }}这种方法存在一些问题： 我们的模型可能会““肿” 扩展性有限 测试不是最好的体验为了解决其中的一些问题，我们将使用一个名为FluentValidation的.NET库对我们的类进行验证。1. 安装packagedotnet add package FluentValidation.AspNetCore2. 自动注册public void ConfigureServices(IServiceCollection services){ services.AddControllers() .AddFluentValidation(fv =&amp;gt; { fv.RegisterValidatorsFromAssemblyContaining&amp;lt;PersonValidator&amp;gt;(); }); ...}3. 控制器中使用验证器 定义验证器 ```CSharp public class Person {public int Id { get; set; }public string Name { get; set; }public string Email { get; set; }public int Age { get; set; } }public class PersonValidator : AbstractValidator { public PersonValidator() { RuleFor(x =&amp;gt; x.Id).NotNull(); RuleFor(x =&amp;gt; x.Name).Length(0, 10); RuleFor(x =&amp;gt; x.Email).EmailAddress(); RuleFor(x =&amp;gt; x.Age).InclusiveBetween(18, 60); } } ```注意,实例生命周期为Transient,而不是单例,避免避免生命周期范围问题4. FluentValidation.AspnetCore源码 services.Configure&amp;lt;MvcOptions&amp;gt; 中 options.ModelMetadataDetailsProviders.Add(new FluentValidationBindingMetadataProvider());将FluentValidationBindingMetadataProvider 添加入 ModelMetadataDetailsProvidersModelValidationContext -&amp;gt; validator-&amp;gt; Validate()-&amp;gt;result.Errors参考fluentvalidation文档```l)fluentvalidationh简单示例fluentvalidation与asp.net coreASP.NET Core + FluentValidation + Swagger(进阶)" }, { "title": "AspnetCore Caching", "url": "/posts/AspnetCore-Caching/", "categories": "AspnetCore", "tags": "缓存", "date": "2022-01-13 15:16:40 +0800", "snippet": "考虑竞争，引入CacheSignal，内部实现是SemaphoreSlimAddMemoryCache 注册为sigleton，指service中构造函数注入的IMemoryCache。AddDistributedMemoryCache 不适用于正式生产分布式缓存依赖于网络I/O ，所以尽量不要用asynchronous 异步操作Sliding Expiration 变化的过期 表示这个时间内如没有client访问，将被清除Absolute Expiration 绝对过期，表示到了这个时间会被驱逐注意，**Absolute Expiration 应大于 Sliding Expiration ** 系统不应依赖于缓存数据 应限制缓存数据的生长 合理设置过期日期分布式缓存 可以参考 Distributed Caching in ASP.NET Core with Redis，注意，项目中引入了SemaphoreSlim，避免资源争用。1. SemaphoreSlimSemaphores are of two types: local semaphores and named system semaphores.SemaphoreSlim 适合 单应用 sigleton app， wait和release 应配对出现Semaphore 可指定name ,通过OpenExisting(String)访问进程中的an existing named system semaphore。waitone与 release 配对，release 返回 The count on the semaphore before the Release method was called.而 release(number) 会增加信号量的最大值 The main thread uses the Release(Int32) method overload to increase the semaphore count to its maximumSemaphore中的SemaphoreRights允许当前用户修改访问权限Microsoft.Extensions.Caching.StackExchangeRedis 包中RedisCache继承IDistributedCache，SetAsync利用lua 脚本 SetScript 中的HSET 存储，所以本质是存储hash数据，而GetAsync 也是利用 stackexchange.redis 原生包中的HashGetAsync方法，可以参考runtime源码2. BackgroundServiceAddHostedService 注册为 sigleton，集成BackgroundService 类。参考 Worker Services in .NET" }, { "title": "Dotnet 逆变协变", "url": "/posts/Dotnet-%E9%80%86%E5%8F%98%E5%8D%8F%E5%8F%98/", "categories": "其他", "tags": "关键词", "date": "2022-01-07 15:02:30 +0800", "snippet": " Covariance 协变IEnumerable 可以赋值 IEnumerable IEnumerable&amp;lt;Derived&amp;gt; d = new List&amp;lt;Derived&amp;gt;(); IEnumerable&amp;lt;Base&amp;gt; b = d; Contravariance 逆变与协变相反,base 到 Drivied 的转化 Action&amp;lt;Base&amp;gt; b = (target) =&amp;gt; { Console.WriteLine(target.GetType().Name); }; Action&amp;lt;Derived&amp;gt; d = b; d(new Derived()); Invariance 不可变双方不可互相转化The last generic type parameter of the Func generic delegates specifies the type of the return value in the delegate signature. It is covariant (out keyword), whereas the other generic type parameters are contravariant (in keyword).此处说明Func传参，out 可协变，in 可逆变" }, { "title": "AspnetCore logging", "url": "/posts/AspnetCore-logging/", "categories": "AspnetCore", "tags": "logging", "date": "2022-01-07 11:34:53 +0800", "snippet": "LogLevelTrace = 0, Debug = 1, Information = 2, Warning = 3, Error = 4, Critical = 5, and None = 6.When a LogLevel is specified, logging is enabled for messages at the specified level and higher.常见的 logLevel categoriesASP.NET Core and EF Core categoriesSuppress SQL Queries logging in Entity Framework core { &quot;Logging&quot;: { &quot;LogLevel&quot;: { // All providers, LogLevel applies to all the enabled providers. &quot;Default&quot;: &quot;Error&quot;, // Default logging, Error and higher. &quot;Microsoft&quot;: &quot;Warning&quot; // All Microsoft* categories, Warning and higher. }, &quot;Debug&quot;: { // Debug provider. &quot;LogLevel&quot;: { &quot;Default&quot;: &quot;Information&quot;, // Overrides preceding LogLevel:Default setting. &quot;Microsoft.Hosting&quot;: &quot;Trace&quot; // Debug:Microsoft.Hosting category. } }, &quot;EventSource&quot;: { // EventSource provider &quot;LogLevel&quot;: { &quot;Default&quot;: &quot;Warning&quot; // All categories of EventSource provider. } } } }providers Generic Host .NET Generic Host in ASP.NET Core Adds the following logging providers: Console Debug EventSource EventLog (only when running on Windows) LoggerMessageHigh-performance logging with LoggerMessage in ASP.NET Core模板化，相对传统的没有装箱和封箱操作（Action,Func,Strongly-type）,插入符$ (string interpolate)不管loglevel是否匹配，和string.format(插入符编译与string.format 一致 见这里)一样，但模板化会先检查loglevel,还有一个就是编译检查格式化是否缺少参数，没有编译错误，只会在运行时抛出异常,最后是方便过滤，filter。 if (!IsEnabled(logLevel)) { return; }所以 _logger.LogInformation($&quot;Writing hello in {index}&quot;); _logger.LogInformation(string.format(&quot;Writing hello in {index}&quot;,index)); _logger.LogInformation(&quot;Writing hello in {index}&quot;,index);与下面的实现不同，体现在是否检查loglevel再转换 if (_logger.IsEnabled(LogLevel.Information)) { _logger.LogInformation(&quot;Writing hello world response to {Person}&quot;, person); } private static readonly Action&amp;lt;ILogger, Person, Exception?&amp;gt; _logHelloWorld = LoggerMessage.Define&amp;lt;Person&amp;gt;( logLevel: LogLevel.Information, eventId: 0, formatString: &quot;Writing hello world response to {Person}&quot;);.Net 6 有代码生成器 static partial class Log { [LoggerMessage(EventId = 0, Message = &quot;Could not open socket for {hostName}&quot;)] static partial void CouldNotOpenSocket(ILogger logger, LogLevel level, string hostName); }CorrelationIdRequest Tracing And Logging Between Multiple MicroService With Correlation Id Using Serilog In .NET Core利用中间件和header中的correlationId记录日志Optimally Configuring ASP.NET Core HttpClientFactory参考Improving logging performance with source generatorsLogging Guidelines and Best Practices for RESTful API" }, { "title": "AspnetCore Middleware logging", "url": "/posts/AspnetCore-Middleware-logging/", "categories": "AspnetCore", "tags": "日志, log", "date": "2022-01-04 17:32:41 +0800", "snippet": "1. Serilog引用Serilog.Extensions.Hosting ,UseSerilog注入SerilogLoggerFactory(继承ILoggerFactory),所以default的logger不会被加载， &amp;lt;PackageReference Include=&quot;Serilog.AspNetCore&quot; Version=&quot;4.1.0&quot; /&amp;gt; &amp;lt;PackageReference Include=&quot;Serilog.Settings.Configuration&quot; Version=&quot;3.3.0&quot; /&amp;gt; &amp;lt;PackageReference Include=&quot;Serilog.Sinks.MSSqlServer&quot; Version=&quot;5.6.1&quot; /&amp;gt;Net core 3.1 之后，不允许在ConfigureServices中解析ILogger或IConfiguration，但Configure中允许How do I write logs from within Startup.cs?ConfigureServices 从文件中读取配置 public static IConfiguration Configuration { get; } = new ConfigurationBuilder() .SetBasePath(Directory.GetCurrentDirectory()) .AddJsonFile(&quot;appsettings.json&quot;, optional: false, reloadOnChange: true) .AddJsonFile($&quot;appsettings.{Environment.GetEnvironmentVariable(&quot;ASPNETCORE_ENVIRONMENT&quot;) ?? &quot;Production&quot;}.json&quot;, optional: true) .Build(); Log.Logger = new LoggerConfiguration() .ReadFrom.Configuration(Configuration) .CreateLogger(); public static IHostBuilder CreateHostBuilder(string[] args) =&amp;gt; Host.CreateDefaultBuilder(args) .UseSerilog() .ConfigureWebHostDefaults(webBuilder =&amp;gt; { webBuilder.UseStartup&amp;lt;Startup&amp;gt;(); });2. selflog用于调试Serilog.Debugging.SelfLog.Enable(msg =&amp;gt; Debug.WriteLine(msg));3. appsettings.json &quot;Serilog&quot;: { &quot;MinimumLevel&quot;: { &quot;Default&quot;: &quot;Information&quot;, &quot;Override&quot;: { &quot;Microsoft.AspNetCore&quot;: &quot;Warning&quot;, //&quot;Microsoft.EntityFrameworkCore.Database.Command&quot;: &quot;Warning&quot; &quot;Microsoft.EntityFrameworkCore&quot;: &quot;Warning&quot; } }, &quot;WriteTo&quot;: [ { &quot;Name&quot;: &quot;MSSqlServer&quot;, &quot;Args&quot;: { &quot;connectionString&quot;: &quot;lsj50Connection&quot;, &quot;sinkOptionsSection&quot;: { &quot;tableName&quot;: &quot;LsdLogs&quot;, &quot;schemaName&quot;: &quot;dbo&quot;, &quot;autoCreateSqlTable&quot;: true, &quot;batchPostingLimit&quot;: 1000, &quot;period&quot;: &quot;0.00:00:30&quot; }, &quot;restrictedToMinimumLevel&quot;: &quot;Information&quot; } }, { &quot;Name&quot;: &quot;Console&quot;, &quot;Args&quot;: { &quot;restrictedToMinimumLevel&quot;: &quot;Information&quot;, &quot;outputTemplate&quot;: &quot;{Timestamp:yyyy-MM-dd HH:mm:ss} [{Level}] [{SourceContext:l}] {Message}{NewLine}{Exception}&quot; } } ] },Using:可以缺省MinimumLevel: If no rules are selected, use MinimumLevel4. 参考Serilog.Settings.ConfigurationConfigure Serilog in ASP.NET Core – few practical tips" }, { "title": "AspnetCore Testing", "url": "/posts/AspnetCore-Testing/", "categories": "AspnetCore", "tags": "测试", "date": "2022-01-04 14:55:42 +0800", "snippet": "常见的三种测试 单元测试（Unit tests） 集成测试 （Integration tests） 点对点测试 （End-to-end (E2E) tests）经典实现 a typical approach follows the so-called AAA pattern. Arrange. With this action, you prepare all the required data and preconditions. Act. This action performs the actual test. Assert. This final action checks if the expected result has occurred.xunit 依赖 &amp;lt;ItemGroup&amp;gt; &amp;lt;PackageReference Include=&quot;MartinCostello.Logging.XUnit&quot; Version=&quot;0.2.0&quot; /&amp;gt; &amp;lt;PackageReference Include=&quot;Microsoft.AspNetCore.Mvc.Testing&quot; Version=&quot;5.0.13&quot; /&amp;gt; &amp;lt;PackageReference Include=&quot;Microsoft.EntityFrameworkCore.InMemory&quot; Version=&quot;5.0.13&quot; /&amp;gt; &amp;lt;PackageReference Include=&quot;Microsoft.NET.Test.Sdk&quot; Version=&quot;16.9.4&quot; /&amp;gt; &amp;lt;PackageReference Include=&quot;xunit&quot; Version=&quot;2.4.1&quot; /&amp;gt; &amp;lt;PackageReference Include=&quot;xunit.runner.visualstudio&quot; Version=&quot;2.4.3&quot;&amp;gt; &amp;lt;PrivateAssets&amp;gt;all&amp;lt;/PrivateAssets&amp;gt; &amp;lt;IncludeAssets&amp;gt;runtime; build; native; contentfiles; analyzers; buildtransitive&amp;lt;/IncludeAssets&amp;gt; &amp;lt;/PackageReference&amp;gt; &amp;lt;DotNetCliToolReference Include=&quot;dotnet-xunit&quot; Version=&quot;2.3.1&quot; /&amp;gt; &amp;lt;/ItemGroup&amp;gt; 基本语法 AAA pattern [Fact] public void ValidPassword() { //Arrange var passwordValidator = new PasswordValidator(); const string password = &quot;Th1sIsapassword!&quot;; //Act bool isValid = passwordValidator.IsValid(password); //Assert Assert.True(isValid, $&quot;The password {password} is not valid&quot;); } Class Fixtures When to use: when you want to create a single test context and share it among all the tests in the class, and have it cleaned up after all the tests in the class have finished. …If the test class needs access to the fixture instance, add it as a constructor argument, and it will be provided automatically. 单类中共享单例， 构造函数自动注入 Collection When to use: when you want to create a single test context and share it among tests in several test classes, and have it cleaned up after all the tests in the test classes have finished. 多类中共享单例 参考 Shared Context between Tests 也有人解释为单例 AspnetCore集成测试集成测试中引用Microsoft.AspNetCore.Mvc.Testing其中mock（模拟授权）的工作原理是 重写以上测试引用包中的 ConfigureWebHost 中的jwt验证，以及post request 前生成伪装的accesstoken。具体查看Using xUnit to Test your C# CodeWebApplicationFactory 适用于点对点的测试,重写`CreateHostBuilder`和`ConfigureWebHost` public CustomWebApplicationFactory(ITestOutputHelper testOutputHelper) { _testOutputHelper = testOutputHelper; } protected override IHostBuilder CreateHostBuilder() { var builder = base.CreateHostBuilder(); builder.ConfigureLogging(logging =&amp;gt; { logging.ClearProviders(); // Remove other loggers logging.AddXUnit(_testOutputHelper); // Use the ITestOutputHelper instance }); return builder; } protected override void ConfigureWebHost(IWebHostBuilder builder) { // Don&#39;t run IHostedServices when running as a test builder.ConfigureTestServices((services) =&amp;gt; { services.RemoveAll(typeof(IHostedService)); }); builder.UseEnvironment(_environment); //Add mock/ test services to the builder here builder.ConfigureServices(services =&amp;gt; { services.AddScoped(sp =&amp;gt; { // Replace SQLite with in-memory database for tests return new DbContextOptionsBuilder&amp;lt;lsj50Context&amp;gt;() .UseInMemoryDatabase(&quot;DbForPublicApi&quot;) .UseApplicationServiceProvider(sp) .Options; }); }); // Overwrite registrations from Startup.cs builder.ConfigureTestServices(serviceCollection =&amp;gt; { var authenticationBuilder = serviceCollection.AddAuthentication(); authenticationBuilder.Services.Configure&amp;lt;AuthenticationOptions&amp;gt;(o =&amp;gt; { o.SchemeMap.Clear(); ((IList&amp;lt;AuthenticationSchemeBuilder&amp;gt;)o.Schemes).Clear(); o.DefaultScheme = JwtBearerDefaults.AuthenticationScheme; }); }); builder.ConfigureTestServices(services =&amp;gt; { services.PostConfigure&amp;lt;JwtBearerOptions&amp;gt;(JwtBearerDefaults.AuthenticationScheme, options =&amp;gt; { options.TokenValidationParameters = new TokenValidationParameters() { IssuerSigningKey = FakeJwtManager.SecurityKey, ValidIssuer = FakeJwtManager.Issuer, ValidAudience = FakeJwtManager.Audience }; }); }); }在sqlite或in-memory中测试 ef core in-memory引用 Microsoft.EntityFrameworkCore.InMemory // The database name allows the scope of the in-memory database // to be controlled independently of the context. The in-memory database is shared // anywhere the same name is used. var options = new DbContextOptionsBuilder&amp;lt;SampleDbContext&amp;gt;() .UseInMemoryDatabase(databaseName: &quot;Test1&quot;) .Options;名字相同的话，共享同一个in-memory database。 sqlite var options = new DbContextOptionsBuilder&amp;lt;SampleDbContext&amp;gt;() .UseSqlite(&quot;DataSource=:memory:&quot;) .Options;When the connection is opened, a new database is created in memory. This database is destroyed when the connection is closed.连接关闭后，databse 销毁参考 Testing EF Core in Memory using SQLite xunit 日志 输出参考Converting integration tests to .NET Core 3.0重点参考 传入TestOutputHelper在factory.createClient前 传入 factory.TestOutputHelper = ...在测试类中实例化，传入ITestOutputHelper public ListEndpoint(ITestOutputHelper testOutputHelper) { var factory = new CustomWebApplicationFactory&amp;lt;Startup&amp;gt;(testOutputHelper); httpClient = factory.CreateClient(); _testOutputHelper = testOutputHelper; } xunit 模拟jwt获取认证授权参考 Using xUnit to Test your C# Code如果服务中添加了服务发现 services.AddAuthentication(/*&quot;Bearer&quot;*/options =&amp;gt; { options.DefaultScheme = JwtBearerDefaults.AuthenticationScheme; }) .AddJwtBearer(&quot;Bearer&quot;, options =&amp;gt; { options.Authority = &quot;https://localhost:5001&quot;; options.TokenValidationParameters = new TokenValidationParameters { ValidateAudience = false }; // if you are using API resources, you can specify the name here //options.Audience = &quot;resource1&quot;; // IdentityServer emits a typ header by default, recommended extra check //options.TokenValidationParameters.ValidTypes = new[] { &quot;at+jwt&quot; }; });需要在测试项目中重写For SUTs that still use the Web Host, the test app’s builder.ConfigureServices callback is executed before the SUT’s Startup.ConfigureServices code. The test app’s builder.ConfigureTestServices callback is executed after. // Overwrite registrations from Startup.cs builder.ConfigureTestServices(serviceCollection =&amp;gt; { var authenticationBuilder = serviceCollection.AddAuthentication(); authenticationBuilder.Services.Configure&amp;lt;AuthenticationOptions&amp;gt;(o =&amp;gt; { o.SchemeMap.Clear(); ((IList&amp;lt;AuthenticationSchemeBuilder&amp;gt;)o.Schemes).Clear(); o.DefaultScheme = JwtBearerDefaults.AuthenticationScheme; }); }); 在测试库中 seed Data if (env.IsDevelopment()) { logger.LogInformation(&quot;Seeding Database...&quot;); using (var scope = app.ApplicationServices.CreateScope()) { var scopedProvider = scope.ServiceProvider; try { var lsj50Context = scopedProvider.GetRequiredService&amp;lt;lsj50Context&amp;gt;(); await lsj50ContextSeed.SeedAsync(lsj50Context, logger); } catch (Exception ex) { logger.LogError(ex, &quot;An error occurred seeding the DB.&quot;); } } } 常见问题 Response status code does not indicate success: 415 (Unsupported Media Type). ```csharp public override async Task&amp;lt;ActionResult&amp;gt; HandleAsync(**[FromRoute]**DeleteStaffRequest request, CancellationToken cancellationToken = default)``` System.InvalidOperationException : Each parameter in constructor ‘Void .ctor(System.Guid)’ on type ‘Lsd.PublicApi.StaffEndpoints. 序列化需要默认不带参数的构造函数 " }, { "title": "AspnetCore Authorize", "url": "/posts/AspnetCore-Authorize/", "categories": "AspnetCore", "tags": "auth", "date": "2021-12-28 15:37:11 +0800", "snippet": "1. 重点jwt 包括三部分，header、payload以及signature，前两部分可以被base64解析，而保证jwt不被篡改的关键在于 signature,因为采用了SHA256hash散列算法，当验证服务端（identity server）收到token时，将解析header和payload后利用密钥和算法生成对应的signature,与原token中的作比较，另外jwt中的nonce可以抵御重放攻击。颁发令牌token之后，客户端所请求的资源范围如与jwt中的不符将不被允许，如客户端或中间人修改jwt playload base64内容，因签名不一致，将被拒绝参考这里2. audience预期的受众ids 4 中提到 When using the scope-only model, no aud (audience) claim will be added to the token, since this concept does not apply. If you need an aud claim, you can enable the EmitStaticAudience setting on the options. This will emit an aud claim in the issuer_name/resources format. If you need more control of the aud claim, use API resources.EmitStaticAudience=true启用后默认aud=issuer_name/resources还有QAStack中的回答： JWT将包含一个aud声明，该声明指定JWT适用于哪些资源服务器。如果aud包含www.myfunwebapp.com，但客户端应用程序尝试在上使用JWT www.supersecretwebapp.com，则访问将被拒绝，因为该资源服务器将看到JWT并不适合它。例如，当某ApiResourse的scope包含XXX.read，某ApiClient也包含该scope，请求的access token的aud将包含apiresourse的name。Identity Server 4 ClientCredentials with POSTMAN 非常简单易懂的项目，有源码，可以作为入门。API 资源服务中设置 .AddJwtBearer(&quot;Bearer&quot;, options =&amp;gt; { options.Authority = &quot;https://localhost:5001&quot;; options.TokenValidationParameters = new TokenValidationParameters { ValidateAudience = true }; // if you are using API resources, you can specify the name here // options.Audience = &quot;https://localhost:5001/resources&quot;; options.Audience = &quot;StaffApi&quot;; // IdentityServer emits a typ header by default, recommended extra check options.TokenValidationParameters.ValidTypes = new[] { &quot;at+jwt&quot; }; });表示该API服务的受众是某个授权中心，用户请求的令牌中的aud需与此一致。IdentityServer4discovery Document The discovery endpoint is available via /.well-known/openid-configuration relative to the base address of your Token Server在docker中运行的ids4,discovery endpoint link 显示问题,因为docker 的桥接网络和由nginx代理，所有实际的endpoint是 https://IdentityServer:5002/connect/authorize 或者 https://172.xx.xx.xxx/identityserver/connect/authorize (由nginx location rule proxy){ “issuer”: “https://IdentityServer:5002”, “jwks_uri”: “https://172.xx.xx.xxx/.well-known/openid-configuration/jwks”, “authorization_endpoint”: “https://172.xx.xx.xxx/connect/authorize”, ….}见IdentityServer4: Building a Simple Token Server and Protecting Your ASP.NET Core APIs with JWT3. ClaimsUnderstanding Claims提到 Scopes are identifiers used to specify what access privileges are being requested. Claims are name/value pairs that contain information about a user.So an example of a good scope would be “read_only”. Whilst an example of a claim would be “email”: “john.smith@example.com”.scope代表该token的权限范围，claims 代表user信息。4. Scopedids 4 配置 Client Scoped , 客户端（web）通过disconvery 中的 endpoint 发送授权请求，其中request 中 scoped会与ids4中已注册的ApiScoped比较并放入claims，返回granted code 或 access token客户端 再 利用access token 请求 资源，资源服务（microservice） 通过[Authorize] 和[RequiredScope] 验证 token 中scope 是否符合。Secure Applications with OAuth2 and OpenID Connect in ASP.NET Core 5 – Complete Guide 详尽地介绍了OpenID Connection和OAuth 2.0 ，并结合identity server 4 讲解原理，可读性高，又源码，可以细读学习。5. 笔记 Identity Resources are some standard open id connect scopes… 用户身份识别的内容，比如email，profile，website等等 API Resources are used to define the API that the identity server is protecting 被保护的资源，比如microservive架构的服务提供，体现在Request中的aud （audient受众） ApiResource 包含 one or more ApiScope，参考Defining Resources new ApiResource(&quot;invoice&quot;, &quot;Invoice API&quot;) { Scopes = { &quot;invoice.read&quot;, &quot;invoice.pay&quot;, &quot;manage&quot; } }, api服务端 This Authentication configuration will make use of the discovery document on startup to configure the security for this APIapi服务利用 ids4 发现文档 知道endpoints，比如验证 token是否有效等当客户端（网页/手机）请求资源时，必须带上tokenInstall-Package IdentityServer4.AccessTokenValidationservices.AddAuthentication(&quot;Bearer&quot;).AddIdentityServerAuthentication(&quot;Bearer&quot;, options =&amp;gt;{ options.ApiName = &quot;weatherApi&quot;; options.Authority = &quot;https://localhost:44343&quot;;});app.UseAuthorization();ApiName 对应已注册的API Resources，最后在pipline中调用 Client Credentials flow 中不建议根据Scope来限制访问Client Credentials scope (optional)Your service can support different scopes for the client credentials grant. In practice, not many services actually support this.Authorization based on Scopes and other Claims中建议services.AddAuthorization(options =&amp;gt;{ options.AddPolicy(&quot;StaffRead&quot;, policy =&amp;gt; policy.RequireClaim(JwtClaimTypes.Scope, &quot;staffApi.read&quot;)); options.AddPolicy(&quot;Staff&quot;, policy =&amp;gt; policy.RequireClaim(JwtClaimTypes.Scope, &quot;staffApi.read&quot;,&quot;staffApi.write&quot;));});[Authorize(Policy =&quot;Staff&quot;)] AddJwtBearer AddJwtBearer 从header中提取和验证jwt token client credential grant 客户端授权将无法获取userinfo，返回ForbiddenGet UserInfo from Access Token - “Forbidden” client credentials 授权时利用Client中的claim中的role结合Policy限制访问，但不建议，应采用 Policy-Base 比较适合。 claim 针对用户的信息，键值对。scope 针对token 可以访问的范围role 针对user identity 的信息所以下面的方案并不适合客户端授权 new Client { ClientId = &quot;x&quot;, ClientName = &quot;x Api&quot;, AllowedGrantTypes = GrantTypes.ClientCredentials, ClientSecrets = new List&amp;lt;Secret&amp;gt; {new Secret(&quot;xxx&quot;.Sha256())}, AllowedScopes = new List&amp;lt;string&amp;gt; { &quot;staffApi.read&quot;}, Claims = new List&amp;lt;ClientClaim&amp;gt;{ new ClientClaim(JwtClaimTypes.Role, &quot;Staff&quot;) } }, options.AddPolicy(&quot;Staff&quot;, policy =&amp;gt; policy.RequireClaim(&quot;client_role&quot;, &quot;Staff&quot;)); 在不同局域/DNS的部署Identity和其他服务，出现 The remote certificate is invalid according to the validation procedure: RemoteCertificateNameMismatch参考 RemoteCertificateNameMismatch I suspect that remote certificate is issued against some DNS name, but you are connecting to IP address which apparently isn’t specified in certificate subject/SAN extension. 返回200，但无内容在ExceptionMiddleware 中错误被拦截，context.status = 200内部报错： The MetadataAddress or Authority must use HTTPS unless disabled for development by setting RequireHttpsMetadata=false.只用于开发环境.AddJwtBearer(&quot;Bearer&quot;, options =&amp;gt; { options.Authority = Configuration.GetSection(&quot;Identity&quot;)[&quot;Authority&quot;]; //This should be disabled only in development environments options.RequireHttpsMetadata = false; }6. 参考IdentityServer4 in ASP.NET Core – Ultimate Beginner’s GuideProtected web API: Verify scopes and app rolesRole-based authorization in ASP.NET Core" }, { "title": "DotnetCore CLI", "url": "/posts/DotnetCore-CLI/", "categories": "其他", "tags": "工具", "date": "2021-12-21 15:20:16 +0800", "snippet": "dotnet CLI安装工具dotnet tool install --global dotnet-efdotnet tool update --global dotnet-efdotnet add package Microsoft.EntityFrameworkCore.Design验证dotnet ef反向工程dotnet ef dbcontext scaffold “Data Source=(localdb)\\MSSQLLocalDB;Initial Catalog=Chinook” Microsoft.EntityFrameworkCore.SqlServerdotnet ef dbcontext scaffold “Server=192.168.1.111;Database=xxxx;User Id=xxxx;Password=xxxx;” Microsoft.EntityFrameworkCore.SqlServer -t tblpurchase_detail -f –use-database-namesmkdir Ordercd Orderdotnet new sln Orderdotnet new console -o Order.Datadotnet sln add Order.Datadotnet add package Microsoft.EntityFrameworkCore.Designdotnet add package Microsoft.EntityFrameworkCore.SqlServerdotnet ef dbcontext scaffold “Data Source=(localdb)\\MSSQLLocalDB;Initial Catalog=Chinook” Microsoft.EntityFrameworkCore.SqlServer构建控制器dotnet add package Microsoft.VisualStudio.Web.CodeGeneration.Designdotnet add package Microsoft.EntityFrameworkCore.Designdotnet tool install -g dotnet-aspnet-codegeneratordotnet tool update -g dotnet-aspnet-codegeneratordotnet aspnet-codegenerator controller -name TodoItemsController -async -api -m TodoItem -dc TodoContext -outDir Controllersdotnet ef dbcontext scaffold “Server=192.168.1.111;Database=xxx;User Id=xxx;Password=xxx;” Microsoft.EntityFrameworkCore.SqlServer -t tblEmployeeProperties -f –use-database-names –output-dir Models –projetc projectname" }, { "title": "AspnetCore Database", "url": "/posts/AspnetCore-Database/", "categories": "AspnetCore", "tags": "database", "date": "2021-12-21 12:12:25 +0800", "snippet": "1. 分离迁移项目services.AddDbContext&amp;lt;lsj50Context&amp;gt;( options =&amp;gt; { options.UseMySQL(Configuration.GetConnectionString(&quot;mysqlConnection&quot;),x=&amp;gt;x.MigrationsAssembly(&quot;Infrastructure&quot;)); });dotnet ef migrations add StaffRole --project ../infrastructure -vdotnet ef migration 工具首先尝试通过调用 Program.CreateHostBuilder()、调用 Build()，然后访问 Services 属性来获取服务提供程序。[链接][3]2. 问题 add migration 失败，提示unable to create object type 或 no database provider按[Using a Separate Migrations Project][1]创建不同的项目存储Migration，提示以上失败，检查项目相对路径dotnet ef migrations add init --project ../StaffApi.Database.Migrations EF Core 配置外键Relationships Single navigation property public List&amp;lt;Post&amp;gt; Posts { get; set; } ... public int **PostId** { get; set; } Manual configuration modelBuilder.Entity&amp;lt;Post&amp;gt;() .HasOne(p =&amp;gt; p.Blog) .WithMany(b =&amp;gt; b.Posts) .HasForeignKey(p =&amp;gt; p.BlogForeignKey) .OnDelete(DeleteBehavior.Cascade); ; 迁移mysql迁移到mysql时提示无法从char转为int，因为’1’在数据库中被定义为int，需要添加HasColumnType(&quot;char&quot;)builder.Property(e =&amp;gt; e.p_enabled) .IsRequired() .HasMaxLength(1) .HasDefaultValue&amp;lt;char&amp;gt;(&#39;1&#39;) .HasColumnType(&quot;char&quot;) .IsUnicode(false);3.EnsureCreated 和 migrationsEnsureCreated 和 Migrations 不能很好地协同工作。 如果使用 Migrations，请勿使用 EnsureCreated 初始化架构。请勿在 Migrate() 前调用 EnsureCreated()。 EnsureCreated() 会绕过迁移创建架构，这会导致 Migrate() 失败在运行时应用迁移3. 参考配置dbcontext-configurationImplement the infrastructure persistence layer with Entity Framework Core 针对DDD 开发展开讨论Modular Architecture in ASP.NET Core – Building Better Monolithsdotnet-core-cli数据迁移Using EF Core in a Separate Class Library projecthow does entity framework migration deal with dbcontext?EF Core Migrations in ASP .NET Core上下文Managing DbContext the right way with Entity Framework 6: an in-depth guide" }, { "title": "AspnetCore Middleware Redis", "url": "/posts/AspnetCore-Middleware-Redis/", "categories": "AspnetCore", "tags": "redis, middlerware", "date": "2021-12-20 17:19:49 +0800", "snippet": "1. 命名惯例Yes, colon sign : is a convention when naming keys. In this tutorial on redis website is stated: Try to stick with a schema. For instance “object-type:id:field” can be a nice idea, like in “user:1000:password”. I like to use dots for multi-words fields, like in “comment:1234:reply.to”.Redis key naming conventions? 2. 第三方依赖库StackExchange.Redis.Extensions.CoreStackExchange.Redis.Extensions.AspNetCoreStackExchange.Redis.Extensions.Newtonsoft&quot;Redis&quot;: { &quot;Password&quot;: &quot;uefEg7DelBFFbAIw=&quot;, &quot;AllowAdmin&quot;: true, &quot;Ssl&quot;: true, &quot;ConnectTimeout&quot;: 6000, &quot;ConnectRetry&quot;: 2, &quot;Database&quot;: 0, &quot;Hosts&quot;: [ { &quot;Host&quot;: &quot;CoreCacheDemo.redis.cache.windows.net&quot;, &quot;Port&quot;: &quot;6380&quot; } ] }services.AddStackExchangeRedisExtensions&amp;lt;NewtonsoftSerializer&amp;gt;( Configuration.GetSection(&quot;Redis&quot;).Get&amp;lt;RedisConfiguration&amp;gt;());以上代码注入了IRedisCacheClientservices.AddSingleton&amp;lt;IRedisCacheClient, RedisCacheClient&amp;gt;();主从情况下，不同Host，写入主，从负责读3. 参考Using Redis Cache with ASP.NET Core 3.1 using StackExchange.Redis.Extensions.Core ExtensionsStackExchange.Redis 官方文档(二) Configuration" }, { "title": "AspnetCore Middleware Components", "url": "/posts/AspnetCore-profile/", "categories": "AspnetCore", "tags": "中间件", "date": "2021-12-20 11:52:29 +0800", "snippet": "serviceservices.Add{GROUP_NAME} 添加服务DI lifetimes Transient Scoped （DbContext） Singleton注意： 不要从单例服务 中解析范围 服务Dependency injection in .NETmiddleware重点：pipeline USE 结合next.invoke() Run 终端路由 Map 分支MapWhen 如匹配进入，否则主线或终端UseWhen 匹配执行后rejoin进入主线loggingDI（dependency injection）方式调用 appsettings.{Environment}.json LogLevel 指定类别的日志等级， 等于或高于指定等级会被记录，默认 Information Logging.{providername}.LogLevel 重载覆盖 Logging.LogLevelLogging in .NET Core and ASP.NET CoreconfigurationController DI读取IConfiguration 或 statup读取 Configuration.GetSection之后再DI注入，构造函数再注入IOptions层级读取 var defaultLogLevel = Configuration[“Logging:LogLevel:Default”];Configuration in ASP.NET CoreenvironmentslaunchSettings.json Development Staging ProductioniisSettings IIS 服务器环境参数profiles 集合 key， 项目名 commandName Project 对应 Kestrel web server，IISExpress 对应 IISExpress 服务 environmentVariables&quot;environmentVariables&quot;: { &quot;ASPNETCORE_ENVIRONMENT&quot;: &quot;Development&quot; }Use multiple environments in ASP.NET Core问题 middleware 和 actionfilter 均无法catch ModelValid Exception 因为内置的 ModelStateInvalidFilter中OnActionExecuting 中 设置 context.Result Automatic HTTP 400 responses 提到 The [ApiController] attribute makes model validation errors automatically trigger an HTTP 400 response. apiendpoint (Controller) automapper mapprofile (Dto) fluentvalidation (验证) middlerware exceptionfilter actionfilter actionresult (pipline ,异常处理，返回类型) authentication (验证) authorization (授权) cache (缓存) discovery (发现) gateway (网关)" }, { "title": "Database", "url": "/posts/Database/", "categories": "DevOps", "tags": "database", "date": "2021-12-15 14:57:26 +0800", "snippet": "What does ACID mean in Database Systems?" }, { "title": "编码", "url": "/posts/%E7%BC%96%E7%A0%81/", "categories": "其他", "tags": "编码", "date": "2021-12-14 16:20:19 +0800", "snippet": "sqlserver中的字符编码、排序规则、nvarchar和varchar、大N‘‘Unicode 和 UTF-8 之间的关系UTF-8 stands for “Unicode Transformation Format - 8 bits.”1 byte(字节) = 8 bits" }, { "title": "OpenID Connect 和 OAuth2.0", "url": "/posts/OpenID-Connect-%E5%92%8C-OAuth2.0/", "categories": "授权认证", "tags": "OAuth2, OpenID", "date": "2021-12-10 17:53:31 +0800", "snippet": "OAuth2.0 获取授权OAuth is an open authorization protocol Authorization Code Grant（RP在授权过程中不知道这个过程，用户在授权端点登陆，授权码post给后台，后台拿到code后获取access_token和userinfo，用户转入RP主页） Implicit Grant（适用于无后台应用，如js网站，用户在授权端点登陆，RP直接拿到accsess_token，会暴露给资源所有者或其他应用，因为token是jwt,payload 包含了用户profile） Resource Owner Password Credentials Grant(需要用户输入username和password，RP不得保存，仅在1，2授权不可用的情况下) Client Credentials Grant（用于机器对机器，没有用户参与，如后台维护工作）OpenID ConnectOpenID Connect将身份验证实现为OAuth 2.0授权过程的扩展 Authorisation code flow - 代码流，跳转到授权页面，allow后OP发送code到RP指定的url，RP 后台通过code 再向OP获取id_token和access_token，通过access_token访问UserInfo端点，获取授权码必须走TLSuserinfo中提到 Communication with the UserInfo Endpoint MUST utilize TLS 注意: scope中必须包含openid范围值,即scope = ‘openid…’ Implicit flow - 隐式流，比如js网站，无后台，授权后直接返回id_token和access_token给RP，易暴露 Hybrid flow - 混合流，一些令牌是由授权端点返回的，而其他令牌是从令牌端点返回的ID token sub-subject, 用户身份 iss-issuing authority，发行机构 aud-audience, client ,受众 nonce-随机数（避免重放攻击） auth-time-何时认证 acr-强度 iat-issue at ,发行时间 exp-expiration time， 过期时间 name &amp;amp; email address 数字签名 加密（JWE）例子 GET /authorize? response_type=code &amp;amp;scope=openid%20profile%20email &amp;amp;client_id=s6BhdRkqt3 &amp;amp;state=af0ifjsldkj &amp;amp;redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb HTTP/1.1 Host: server.example.com HTTP/1.1 302 Found Location: https://client.example.org/cb? code=SplxlOBeZQQYbYS6WxSbIA &amp;amp;state=af0ifjsldkj HTTP/1.1 200 OK Content-Type: application/json Cache-Control: no-store Pragma: no-cache { &quot;access_token&quot;: &quot;SlAV32hkKG&quot;, &quot;token_type&quot;: &quot;Bearer&quot;, &quot;refresh_token&quot;: &quot;8xLOxBtZp8&quot;, &quot;expires_in&quot;: 3600, &quot;id_token&quot;: &quot;eyJhbGciOiJSUzI1NiIsImtpZCI6IjFlOWdkazcifQ.ewogImlzc yI6ICJodHRwOi8vc2VydmVyLmV4YW1wbGUuY29tIiwKICJzdWIiOiAiMjQ4Mjg5 NzYxMDAxIiwKICJhdWQiOiAiczZCaGRSa3F0MyIsCiAibm9uY2UiOiAibi0wUzZ fV3pBMk1qIiwKICJleHAiOiAxMzExMjgxOTcwLAogImlhdCI6IDEzMTEyODA5Nz AKfQ.ggW8hZ1EuVLuxNuuIJKX_V8a_OMXzR0EHR9R6jgdqrOOF4daGU96Sr_P6q Jp6IcmD3HP99Obi1PRs-cwh3LO-p146waJ8IhehcwL7F09JdijmBqkvPeB2T9CJ NqeGpe-gccMg4vfKjkM8FcGvnzZUN4_KSP0aAp1tOJ1zZwgjxqGByKHiOtX7Tpd QyHE5lcMiKPXfEIQILVq0pc_E2DzL7emopWoaoZTF_m0_N0YzFC6g6EJbOEoRoS K5hoDalrcvRYLSrQAZZKflyuVCyixEoV9GfNQC3_osjzw2PAithfubEEBLuVVk4 XUVrWOLrLl0nx7RkKU8NXNHq-rvKMzqg&quot; }Payload{ &quot;sub&quot; : &quot;alice&quot;, &quot;iss&quot; : &quot;https://openid.c2id.com&quot;, &quot;aud&quot; : &quot;client-12345&quot;, &quot;nonce&quot; : &quot;n-0S6_WzA2Mj&quot;, &quot;auth_time&quot; : 1311280969, &quot;acr&quot; : &quot;c2id.loa.hisec&quot;, &quot;iat&quot; : 1311280970, &quot;exp&quot; : 1311281970}其他JWS：JSON Web Signature，Digital signature/HMAC specification（签名） Authorisation response 中包含了三个部分，header，payload，signature signature：可以通过JWS签名，保证数据完整，没有被篡改，返回响应中的header包含了alg（加密方式，如HS256）利用服务端的密钥secret通过哈希256（SHA256）HMACSHA256(base64UrlEncode(header)+ “.” + base64UrlEncode(payload),secret（公钥）) 加密取最左128bit，通过jwt网站了解 https://jwt.io/JWE：JSON Web Encryption，Encryption specification（加密） 加密和解密方采用同一个密钥。采用这种模式的算法就叫做对称加密算法。加密和解密方采用不同的密钥。采用这种模式的算法就叫做非对称加密算法。JWK：JSON Web Key，Public key specificationJWA：JSON Web Algorithms，Algorithms and identifiers specification（算法）JWT：JSON Web Token参考JWT、JWE、JWS 、JWK 到底是什么？该用 JWT 还是 JWS？RSA加密、解密、签名、验签的原理及方法JWT介绍及其安全性分析web环境中如何防止token被窃取进行重放攻击OAuth 2.0OpenID Connect Core 1.0A Guide To OAuth 2.0 GrantsWhat is the OAuth 2.0 Authorization Code Grant Type?What Is OpenID Connect and How Does It Work? What You Need to Knowokta上的讲解，非常经典有实例，简单易懂获取授权码： response_type=code client_id 获取授权码不需要client_secret scope state 防 csrf 攻击后台服务利用授权码获取token grant_type=authorization_code 带 client_secretOpenID ConnectOpenID Connect 1.0 is a simple identity layer on top of the OAuth 2.0 protocol.Clients use OAuth to request access to an API on a user’s behalf, but nothing in the OAuth protocol tells the client user information. OpenID Connect enables a client to access additional information about a user, such as the user’s real name, email address, birthdate or other profile information.The user’s SSO experience is made possible by the delivery of the ID token from the authorization server to the client.请求中scope=openidWhat is the JSON Web Token structure? Header Payload SignatureTherefore, a JWT typically looks like the following.xxxxx.yyyyy.zzzzz" }, { "title": "术语", "url": "/posts/%E6%9C%AF%E8%AF%AD/", "categories": "其他", "tags": "术语", "date": "2021-12-10 17:27:10 +0800", "snippet": "PV = page view 点击率QPS = Queries Per Second 每秒查询率TPS = transactions Per Second 每秒事务率参考TPS、QPS和系统吞吐量的区别和理解聊聊TPS、QPS、CPS概念和区别QPS 计算公式每秒查询率QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。原理：每天80%的访问集中在20%的时间里，这20%时间叫做峰值时间公式：( 总PV数 * 80% ) / ( 每天秒数 * 20% ) = 峰值时间每秒请求数(QPS)机器：峰值时间每秒QPS / 单台机器的QPS = 需要的机器问：每天300w PV 的在单台机器上，这台机器需要多少QPS？答：( 3000000 * 0.8 ) / (86400 * 0.2 ) = 139 (QPS)问：如果一台机器的QPS是58，需要几台机器来支持？答：139 / 58 = 3NAS: （Network Attached Storage：网络附属存储）按字面简单说就是连接在网络上，具备资料存储功能的装置，因此也称为“ 网络存储器 ”。CLI: Core Command Line InterfacePOCO: Plain Old CLR ObjectsCQS/CQRS: Command and Query Responsibility SegregationGUID: Globally Unique IdentifierPKCE： proof-key for code exchangeasync [əˈsɪŋk]sync [sɪŋk]局域网 Local Area Network 简称 LANx86 指8086（x是通配）的cpu指令架构x86-32 = x86x86-64 = x6432和64位bit 指cpu每个周期能处理的位数，也称字长操作系统也分32位和64位1024bit = 1kb2的乘方 2^9=512，2^11=2048，只有 2^10=1024最接近1000 同底数幂相乘除，原来的底数作底数，指数的和或差作指数2^10 = 1024 bit = 1kb2^20 = 2^10 * 2^10 = 1024kb = 1M(MegaBit) 兆2^30 = 2^10 * 2^10 * 2^10 = 1024M = 1G (gigaBit) 千兆地址总线 32条 01 00 01 … 表示，2^32 = 2^(2+30) = 2^2 * 2^30 =4 * 1G = 4G RAMWindows 10 Home 32 bit 4 GBsWindows 10 Home 64 bit 128 GBsWindows 10 Pro 32 bit 4 GBsWindows 10 Pro 64 bit 512 GBsWindows 10 Enterprise/Education 32 bit 4 GBsWindows 10 Enterprise/Education 64 bit 512 GBscpu i7-7700 最大支持内存 64 GB英特尔® 酷睿™ i7-7700 处理器.Net 堆和栈堆栈原理C# 等较高级的 .NET 语言编译为称为中间语言 (IL) 的硬件无关性指令集。 应用运行时，JIT 编译器将 IL 转换为处理器可理解的计算机代码。 JIT 编译发生在要运行代码的同一台计算机上。垃圾回收的基本知识32 位计算机上的每个进程都具有 2 GB 的用户模式虚拟地址空间初始化新进程时，垃圾回收器在运行时会为进程保留一个连续的地址空间区域，它会分配一段内存用于存储和管理对象， 这个保留的地址空间被称为托管堆。应用程序创建下一个对象时，垃圾回收器在紧接第一个对象后面的地址空间内为它分配内存在回收中，垃圾回收器检查托管堆，查找无法访问对象所占据的地址空间块。 发现无法访问的对象时，它就使用内存复制功能来压缩内存中可以访问的对象，释放分配给不可访问对象的地址空间块。 栈的结构是后进先出 栈地址从高往底分配 类型的引用也存储在栈中为什么 CLR 不处理清理代码？整型byte 8位整数short 16char 16int 32long 64float 32double 64decimal 128动态语言运行时 (DLR) 是一种运行时环境，可以将一组动态语言服务添加到公共语言运行时 (CLR)" }, { "title": "供应链", "url": "/posts/%E4%BE%9B%E5%BA%94%E9%93%BE/", "categories": "其他", "tags": "供应链", "date": "2021-12-10 16:24:19 +0800", "snippet": "写给供应链产品经理：浅谈订单系统的设计 非常详细电商系统术语 用处不大协同仓+特殊仓Photo Station、MonentsVideo Station" }, { "title": "Vim", "url": "/posts/Vim/", "categories": "其他", "tags": "工具", "date": "2021-12-07 15:28:44 +0800", "snippet": "How to Search in Vim / Vi91 Vim Keyboard Shortcuts to Get Started with Vim:nohlsearch # 关闭search 高亮vjj&amp;gt; # 插入indentset autoindent expandtab tabstop=2 shiftwidth=2Set Indentation Width to 2 or 4 Spaces (or Tab) in Vim" }, { "title": "Docker", "url": "/posts/Docker/", "categories": "容器", "tags": "docker", "date": "2021-12-07 14:14:25 +0800", "snippet": "1. Installsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo# runc not foundyum -y remove runcsudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin# install libseccomp-develyum install libseccomp-develsystemctl enable docker.servicesystemctl start docker.service# 以debug的方式运行dockersudo docker --debug# ubuntucurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;sudo apt-get updateapt-cache policy docker-cesudo apt-get install -y docker-ceConfigurationlive-restore By default, when the Docker daemon terminates, it shuts down running containers. You can configure the daemon so that containers remain running if the daemon becomes unavailable.restart docker 将重启容器，需要设置参数 live-restore{ &quot;live-restore&quot;: true}$ systemctl reload docker2. runlike导出 running container command# centos 8 streamsudo dnf install python3pip3 --versionpip3 install runlikerunlike kopfdocker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ assaflavie/runlike YOUR-CONTAINER3. 镜像3.1. RabbitMQdocker run -d --hostname dev-rabbitmq --name dev-rabbitmq -p 15672:15672 -p 5672:5672 -v ~/docker/rabbitmq:/var/lib/rabbitmq rabbitmq:3-management测试环境下，映射端口 -p 5672:5672http://localhost:8080 访问web UI 管理3.2. redis运行 redis 作为 replicadocker run \\-p 6380:6379/tcp \\--name redis-slave \\-v /myredis:/usr/local/etc/redis \\--privileged=true \\-d redis redis-server /etc/redis/redis.conf \\--appendonly yes参考文章关于 -p stackoverflow为什么在master redis-cli info 看到的slave ip 和 port 不是想要的,参考官方文档replica-announce-ip 5.5.5.5replica-announce-port 12343.3. mysqlHow to fix “mbind: Operation not permitted” in mysql error log让mysql 静默处理异常（线程优先级）service: mysql: image: mysql:8.0.15 # ... cap_add: - SYS_NICE # CAP_SYS_NICEdocker部署mysql 实现远程连接4.networksDocker自身的4种网络工作方式 host：容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。 None：该模式关闭了容器的网络功能。 Container：创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围。 Bridge：此模式会为每一个容器分配、设置IP等，并将容器连接到一个docker0虚拟网桥，通过docker0网桥以及Iptables nat表配置与宿主机通信。查看网络 docker network ls在特定网络运行容器docker run -d --net=my_bridge --name db training/postgres查看容器网络docker inspect --format=&#39;&#39; container_name参考 Compose 中的网络4. DOCKERFILE DOCKERFILE EXPOSE it is just a type of documentation that tells a person who runs the container about the port that needs to be exposed or published to allow communication to the container from outside.作用是告诉运维哪些端口需要被映射摘自非常nice的网站 educba DOCKERFILE COPY只支持linux系统，–chown 可以修改权限 DOCKERFILE BUILDdocker build -t &amp;lt;name of the docker image&amp;gt;:&amp;lt;tag&amp;gt; &amp;lt;path of the Dockerfile&amp;gt; --no-cache --progress plain DOCKERFILE FORMATdocker ps --format &#39;json/table \\t\\t\\t&#39; | jq DOCKERFILE WAIT退出返回0，检查container状态 DOCKERFILE IMAGE HISTORY镜像命令记录 DOCKERFILE RUN–restart no，默认策略，在容器退出时不重启容器 on-failure，在容器非正常退出时（退出状态非0），才会重启容器 on-failure:3，在容器非正常退出时重启容器，最多重启3次 always，在容器退出时总是重启容器 unless-stopped，在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 DOKCERFILE REGISTRYdocker pull ubuntu = docker pull docker.io/library/ubuntudocker pull myregistrydomain:port/foo/bar 指示docker拉取位于myregistrydomain:port的镜像foo/bar在myregistrydomain:port 主机中 registry是一个存储系统,默认的存储驱动程序是本地posix文件系统运行你自己的Registry是与CI/CD系统集成并对其进行补充的绝佳解决方案。在典型的工作流程中，对源版本控制系统的提交将触发在CI系统上的构建，如果构建成功，则将新镜像推送到你的Registry。然后，来自Registry的通知将触发在暂存环境上的部署，或者通知其它系统有一个新镜像可用。 DOCKERFILE systemdsystemd 用于管理许多 Linux 发行版中的 Docker 守护进程sudo systemctl status docker Docker ENTRYPOINTENTRYPOINT [“executable”, “param1”, “param2”, ..]ENTRYPOINT [“nginx”, “-g”, “daemon off;”]ENTRYPOINT command param1 param2ENTRYPOINT exec nginx -g “daemon off;” DOCKER ATTACH附加到正在运行的进程docker attach –sig-proxy=false mynginx docker updatedocker update --restart=always container_name5. 命令$ docker inspect containerId #检查$ docker ps -a #列出所有容器$ docker images ls$ docker container rm/start/stop containerId$ docker exec -it container_name /bin/bash$ docker system df #硬盘使用info$ docker rmi $(docker images --format &#39;:&#39; | grep &#39;imagename&#39;) # prune special image$ docker image prune$ docker container prune$ docker events --since &#39;2023-01-01&#39; --filter container=d7542ad9e70e # 查看事件$ docker rmi $(docker images -a | grep &#39;xxx&#39; | awk &#39;{print $3}&#39;) # 删除特定镜像6. 迁移 &quot;Mounts&quot;: [ { &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;f7723237c4e47579297e81181e4c5763e2fb377d17ae13a879abc154813f05fe&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/f7723237c4e47579297e81181e4c5763e2fb377d17ae13a879abc154813f05fe/_data&quot;, &quot;Destination&quot;: &quot;/var/lib/registry&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; } ]7. Docker Composeinstallsudo curl -L &quot;https://github.com/docker/compose/releases/download/2.29.7/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composedocker-compose --versionbuild: context: ./frontend dockerfile: Dockerfile.prodcontext表示docker文件的上一级相对compose文件的相对路径重建任一servicedocker-compose up -d --no-deps --build &amp;lt;service_name&amp;gt;–no-deps - Don’t start linked services.–build - Build images before starting containers.8. 常见问题 Aspnetcore项目端口问题 environment: - ASPNETCORE_URLS=https://+; - ASPNETCORE_HTTPS_PORT=5001 - ASPNETCORE_Kestrel__Certificates__Default__Password=qazwsx - ASPNETCORE_Kestrel__Certificates__Default__Path=/https/aspnetapp.pfxASPNETCORE_URLS 优先 端口80和443，5001将不起作用，应ASPNETCORE_URLS=https://+:5001，再做端口映射Registry证书外部访问，需要CA机构认证，参考这里,以及Get a certificate ,Use self-signed certificates You have already obtained a certificate from a certificate authority (CA).registry APIopenssl 支持通配符# 分段签发mkdir docker-registry-certscd docker-registry-certsopenssl genrsa -des3 -out registry.lesaunda.com.cn.key 1024# enter phrase for key: lesaunda@registryopenssl req -new -key registry.lesaunda.com.cn.key -out registry.lesaunda.com.cn.csropenssl x509 -req -days 365 -in registry.lesaunda.com.cn.csr -signkey registry.lesaunda.com.cn.key -out registry.lesaunda.com.cn.crt# openssl v1.0.2openssl req \\-newkey rsa:4096 -nodes -sha256 -keyout certs/registry.lesaunda.com.cn.key \\-x509 -days 365 -out certs/registry.lesaunda.com.cn.crt \\-subj &quot;/C=CN/ST=GuangDong/L=GuangZhou/O=Lesaunda/OU=LS/CN=registry.lesaunda.com.cn&quot;# openssl v1.1.1 之后新增 -addext 参考 https://web.archive.org/web/20210103034813/https://www.openssl.org/docs/man1.0.2/man1/openssl-req.html, Centos 7 无法upgrade openssl v1.0.2,在 Centos stream 8 生成证书openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout certs/registry.lesaunda.com.cn.key \\ -addext &quot;subjectAltName = DNS:registry.lesaunda.com.cn,IP: xxx.xxx.xx.xx&quot; \\ -x509 -days 365 -out certs/registry.lesaunda.com.cn.crt# 验证openssl x509 -in /etc/docker/certs.d/registry.xxx.com.cn/ca.crt -noout -subjectopenssl x509 -in /etc/docker/certs.d/registry.xxx.com.cn/ca.crt -noout -textdocker run -d \\ --restart=always \\ --name registry \\ -p 443:443 \\ -v /root/certs:/root/docker-certs \\ -v /mnt/registry:/var/lib/registry \\ -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/root/docker-certs/registry.lesaunda.com.cn.crt \\ -e REGISTRY_HTTP_TLS_KEY=/root/docker-certs/registry.lesaunda.com.cn.key \\ registry:2scp -P 22 /root/certs/registry.lesaunda.com.cn.crt root@172.21.14.207:/etc/docker/certs.d/registry.lesaunda.com.cn/ca.crtscp -P 22 /root/certs/registry.lesaunda.com.cn.crt root@172.21.14.207:/etc/ssl/certs/ca.crt修改/etc/docker/daemon.json{ &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],}systemctl daemon-reloadsystemctl restart docker.servicesystemctl restart containerd复制容器映射新的portdocker stop test01docker commit test01 test02docker run -p 8080:8080 -td test02导出导入参考How to save all Docker images and copy to another machinedocker save $(docker images &quot;172.21.14.206:5000/*&quot; -q) -o mydockersimages.tardocker load -i mydockersimages.tardocker images &quot;172.21.14.206:5000/*&quot; | sed &#39;1d&#39; | awk &#39;{print $1 &quot; &quot; $2 &quot; &quot; $3}&#39; &amp;gt; mydockersimages.listwhile read REPOSITORY TAG IMAGE_IDdo echo &quot;== Tagging $REPOSITORY $TAG $IMAGE_ID ==&quot; docker tag &quot;$IMAGE_ID&quot; &quot;$REPOSITORY:$TAG&quot;done &amp;lt; mydockersimages.list9. 参考exit, and your container is offlinetty,docker -it daemon bashHow to Deploy and Run Redis in Dockerdelete all stopped containers如何获取 Docker 容器的 IP 地址Docker Exec Command With Examples官网docker compose up 部署多个rabbitmq实例持久化runlikedocker events" }, { "title": "Ubuntu Commands", "url": "/posts/Ubuntu-Commands/", "categories": "系统", "tags": "Ubuntu", "date": "2021-12-07 10:53:16 +0800", "snippet": "1. daemonAs explained above, a daemon is a non-interactive program. It runs all the time, and it’s not connected to the terminal. Even when you close the terminal, the operating system will not stop the daemon as it will run in the background.On the other hand, a process will stop when the terminal closes because it is an executing program instance.远程 telnet FTP 不安全，它们在网络上用明文传送口令和数据，中间人攻击 ssh Secure Shell 完全外壳协议 OpenSSH 是SSH的实现 远程桌面 Virtual Network Computing (VNC)ubuntu 设置dns 修改 /etc/netplan/….yamlnetwork: ethernets: enp2s0: dhcp4: false addresses: [172.21.14.xxx/23] gateway4: 172.21.14.x nameservers: addresses: [192.168.1.228,192.168.1.229,8.8.8.8] version: 2sudo netplan apply 安装samba修改 /etc/samba/smb.confinclude = /home/samba/etc/smb.conf.lsd-server-012. 命令2.1. netstatusnetstat -tlnpapt-get install net-Tools端口 53 systemd-resolved为域名系统 (DNS)（包括DNSSEC和DNS over TLS）、多播 DNS (mDNS)和链路本地多播名称解析 (LLMNR)提供解析器服务。端口 631 打印服务link2.2. 其他 sudo = superuser do su = switch user sudo adduser user_name groups id user_name sudo usermod -aG sudo user_name2.3. 修改文件权限查看文件权限 ls -l修改chmod ugo/a+rw filenameu = owner 拥有者g = group 所属组o = other 其他a = all或者chownchown username filename npoc/lscpu 核数 netstat 防火墙 top/htop 监控 rm/mv/cp sourcefile targetfile 删除 移动 复制 cat filename grep “searchstring” wc -l (count line) 2.4. dig Ubuntu Server 安装后没有网络配置sudo systemctl restart system-networkdvim /etc/netplan/*.yamlnetwork: version: 2 renderer: networkd ethernets: enp3s0: dhcp4: truesudo netplan apply3. 参考API查询How to Use the dig Command on LinuxUbuntu 通过 Netplan 配置网络教程What is a Daemon?apt update 和 apt upgrade 有什么区别How To Fix “E: Could not get lock /var/lib/dpkg/lock” Error On Ubuntu合盖不休眠不休眠APT vs APT-GET: What’s the Difference?Getting started with Docker: Running an Ubuntu Imagedocker run image Copy file简单性能监控ls -ltr学习常用的命令 The Directory Tree常用命令" }, { "title": "Redis", "url": "/posts/Redis/", "categories": "缓存", "tags": "redis, 缓存", "date": "2021-12-06 11:56:12 +0800", "snippet": "1. 命令dir /var/lib/redisdbfilename dump.rdbappendonly.aofredis-cli$ auth password$ info$ ping/pong$ set mykey &quot;value&quot;$ get mykey$ CONFIG SET appendonly yes/no 热修改$ flushall 清空$ keys [pattern], keys Qmall*$ scan 0 math Qmall* count 10, scan keys and count 满足条件的keys redis-server sudo /etc/init.d/redis-server start/stop 2. 数据类型 Strings The maximum allowed size of a string value is 512 MB, containing any sequence of characters.Strings are primarily used for caching HTML elements, widgets, and even entire web pages. Lists Redis allows you to associate an ordered sequence of strings to a key such as this linked list of strings.The possibilities offered by linked lists make them an ideal data type for storing real-time data updates, such as social media posts or logs. Hashes A Redis hash stores an unordered mapping of key-value pairs.maintaining a large collection of individual objects such as user data. Sets Redis sets are well suited for tracking one-of-a-kind events such as unique page views or individual IPs. Sorted Sets A single sorted set can keep track and provide ordered leaderboards of player scores in an online competition using either the ZRANGE or ZREVRANGE Redis command. HyperLogLogs Use HyperLogLogs to aggregate and count unique user interactions or queries. Bitmaps (BitStrings) Bitmaps are frequently used to store data that can be represented as a boolean yes/no separation across consecutive keys. 3. ReplicationImportant: A Redis Cluster (i.e a Replication Cluster) with cluster mode disabled has a single node group (e.g a master and one or two replicas) wheres a Redis cluster with cluster mode enabled may consists of two or more node groups (e.g three masters each having slaves or two).在 Redis 复制的基础上（不包括由 Redis Cluster 或 Redis Sentinel 作为附加层提供的高可用性功能），有一个非常易于使用和配置的领导者跟随者（主副本）复制.Every time data safety is important, and replication is used with master configured without persistence, auto restart of instances should be disabled.主从复制，用于无持久化，重启应被禁用4. Redis Sentinel Monitoring. Sentinel constantly checks if your master and replica instances are working as expected. Notification. Sentinel can notify the system administrator, or other computer programs, via an API, that something is wrong with one of the monitored Redis instances. Automatic failover. If a master is not working as expected, Sentinel can start a failover process where a replica is promoted to master, the other additional replicas are reconfigured to use the new master, and the applications using the Redis server are informed about the new address to use when connecting. Configuration provider. Sentinel acts as a source of authority for clients service discovery: clients connect to Sentinels in order to ask for the address of the current Redis master responsible for a given service. If a failover occurs, Sentinels will report the new address.5. 缓存雪崩大面积失效，或缓存服务器宕机，大量的请求打在DB上方案： 过期时间加上随机数,主从+哨兵（sentinel）高可用6. 缓存穿透大量请求缓存或DB中没有的key,绕开缓存，冲击DB方案： 用户鉴权，参数校验 设置没有的key和value（null或者其他文字稍后再查询），设置较短的过期时间 还有redis 自身的布隆过滤器（Bloom filter）7. 缓存击穿大量请求打在某一热点上，当key过期瞬间打在DB上，造成拥堵方案： 热点数据永远不过期对于恶意用户同个IP发出的大量请求攻击，nginx 限制恶意ip(短时间超多请求的拉黑)8. 持久化 RDB redis database snapshot 快照 AOF append-only-file change-log style利用crontab（linux定时程序）定时备份rdb文件到指定目录或云 从rdb备份中恢复 1) stop redis2) redis.conf appendonly no3) 拷贝rdb文件，redis.conf 设置对应的dir和dbfilename，重启redis4) 热修改 config set appendonly yes，此时rdb与aof 一致5) stop redis，redis.conf appendonly yes 9. 参考《我们一起进大厂》系列-缓存雪崩、击穿、穿透Overview Of Redis ArchitectureRedis中文教程Redis Data Types with Commands: Comprehensive Guidethe little redis bookHow to Install Redis on Ubuntu 20.04 / 18.04Open Redis port for remote connectionsHow to Setup Redis Replication (with Cluster-Mode Disabled) in CentOS 8 – Part 1查看redis.conf路径Install and deploy redis using docker (configuration file startup)Redis缓存与SQL数据库的一致性" }, { "title": "api tools", "url": "/posts/api-tools/", "categories": "其他", "tags": "工具", "date": "2021-12-02 17:23:31 +0800", "snippet": "SQLPLSQLtechonthenetoracle official" }, { "title": "Git 命令", "url": "/posts/Git-%E5%91%BD%E4%BB%A4/", "categories": "代码管理", "tags": "git", "date": "2021-12-02 14:27:16 +0800", "snippet": "1. git reset取消暂存文件，但保留文件内容2. git rm删除文件3. git show显示指定提交的元数据和内容更改4. git diff origin/master可以先fetch，然后diff，再merge5. git stashgit stash pop 恢复git stash list 列出所有隐藏的变更集git stash drop 此命令将丢弃最近存放的变更集6. git stash临时存储所有已修改的跟踪文件git ls-files | xargs wc -l计算行数git rebase 变基git checkout devgit rebase master# 放弃缓存区和工作树的更改，不可恢复，谨慎$ git restore --staged --worktree &amp;lt;pathspec&amp;gt;# 将工作树的提交撤回$ git reset &amp;lt;pathspec&amp;gt;# push tags$ git push origin --tags# 生成ssh 公钥$ ssh-keygen -t rsa -b 4096 -C &quot;sk-ecdsa-sha2-nistp256@openssh.com&quot;# 跟踪remote main分支$ git push --set-upstream orign main# proxy$ git config --global http.proxy http://proxyuser:proxypwd@proxy.server.com:8080$ git config --global --unset http.proxy$ git config --global --get http.proxy# 修改文件夹名字大小写$ git mv folder tmpFolder &amp;amp;&amp;amp; mv tmpFolder Folder$ git rm -r --cached .$ git add --all .master 和 dev 都有修改，dev 暂存 修改，重播master修改，再重播自己的修改,dev可以只贡献自己的部分，不用合并git utf-8option 设置 locale = zh_CN, 路径乱码， git config --global core.quotepath false ，不将超过0x80的视为异常，utf8显示文件修改记录git log -p filepathfile history参考Top 20 Git Commands With Examplesuseful website about gitWhat is a Git SSH Key?" }, { "title": "Servers", "url": "/posts/Servers/", "categories": "其他", "tags": "服务器", "date": "2021-12-02 11:08:06 +0800", "snippet": "1.14 autojobs IMasterPicture Loc_TT2BO MGService WQ E_Coupon (communicates with ac(爱橙) webservices(base on WCF) which also exchange data with another one(VMWCF) that created by ourselves) pictures (where label’s pictures store in) platform (release files,such as P1，IMaster and so on)1.206 websites lsmr (物料补给以及试穿) OrderScore (订货评分) SalesTarget_ls (门店个人指标) SalesTarget_jv tf (Traffic货运单) webservices swash 1.26 DTS jobs inventory (T2 T_ATRL_INVENTORY) 0.143 TT backend" }, { "title": "Release Notes", "url": "/posts/Release-Notes/", "categories": "DevOps", "tags": "发布", "date": "2021-12-01 17:41:51 +0800", "snippet": " Use Plain Language: Eliminate technical jargon and overly complex language. Keep It Short: Use descriptions that are concise and quickly digestible. Be Customer-Centric: Make it obvious and easy for users to zero in on what’s of interest to them (e.g., fixes, improvements, new features). Include Relevant Links: Keep release notes to high-level information, but include links to more detailed information (e.g., user guide, video tutorial, etc.). Let Your Brand and Personality Shine: Seize the opportunity to create a meaningful interaction with your user base.Release Notes" }, { "title": "Latency throughput and availability", "url": "/posts/Latency-throughput-and-availability/", "categories": "DevOps", "tags": "Latency, throughput, availability", "date": "2021-11-30 16:24:13 +0800", "snippet": "LatencyLatency is the amount of time in milliseconds (ms) it takes a single message to be delivered.What causes latency Physical distance Complex computation Congestion Too many nodesHow to improve latency Better paths Caching Protocol choice: certain protocols, like HTTP/2, intentionally reduce the amount of protocol overhead associated with a request, and can keep latency lower.ThroughputThroughput is the amount of data that is successfully transmitted through a system in a certain amount of time, measured in bits per second (bps).What causes low throughput Congestion Protocol overhead LatencyHow to improve throughput Increasing bandwidth Improving latency Protocol choiceAvailabilityAvailability is the amount of time that a system is able to respond, that is the ratio of Uptime / (Uptime + Downtime).What causes low availability Hardware failure Software bugs Complex architectures Dependent service outages Request overload Deployment issuesHow to improve availability Failover systems Clustering Backups Geographic redundancy Automatic testing, deployment, and rollbacksLatency, throughput, and availability: system design interview concepts (3 of 9)" }, { "title": "Learn Test From k6", "url": "/posts/Learn-Test-From-k6/", "categories": "DevOps", "tags": "测试", "date": "2021-11-30 14:35:16 +0800", "snippet": "Smoke TestSmoke Test’s role is to verify that your system can handle minimal load, without any problems.You want to run a smoke test to: Verify that your test script doesn’t have errors. Verify that your system doesn’t throw any errors when under minimal load.Load TestLoad Test is primarily concerned with assessing the performance of your system in terms of concurrent users or requests per second.You should run a Load Test to: Assess the current performance of your system under typical and peak load. Make sure you continue to meet the performance standards as you make changes to your system (code and infrastructure).Stress TestStress Test and Spike testing are concerned with assessing the limits of your system and stability under extreme conditions.You typically want to stress test an API or website to determine: How your system will behave under extreme conditions. What the maximum capacity of your system is in terms of users or throughput. The breaking point of your system and its failure mode. If your system will recover without manual intervention after the stress test is over.Spike testingYou want to execute a spike test to determine: How your system will perform under a sudden surge of traffic. If your system will recover once the traffic has subsided.Soak TestSoak Test tells you about reliability and performance of your system over an extended period of time.You typically run this test to: Verify that your system doesn’t suffer from bugs or memory leaks, which result in a crash or restart after several hours of operation. Verify that expected application restarts don’t lose requests. Find bugs related to race-conditions that appear sporadically. Make sure your database doesn’t exhaust the allotted storage space and stops. Make sure your logs don’t exhaust the allotted disk storage. Make sure the external services you depend on don’t stop working after a certain amount of requests are executed.postman测试" }, { "title": "ZT411 打印机帮助手册", "url": "/posts/ZT411-Printer/", "categories": "其他", "tags": "打印", "date": "2021-11-29 11:12:27 +0800", "snippet": "进纸请参考机器盖上的提示，并严格执行设置 如打印内容偏左，可以调整左侧偏移量： 如打印内容教模糊，可以调整打印浓度至20-25之间 如打印内容图片不够清晰或太黑，可以调整图片抖动如何取消所有任务长按机器上的Cancel按钮，直至出现“取消所有打印”后松手即可测试打印 大贴纸模板 小贴纸模板官网official website 驱程下载常见问题显示屏上不显示ZT系列打印机标签计数器（打印X/X)" }, { "title": "A/B Tests", "url": "/posts/Test-Strategies/", "categories": "DevOps", "tags": "测试", "date": "2021-11-25 11:12:27 +0800", "snippet": "A/B Tests" }, { "title": "Infrastructure as Code", "url": "/posts/Infrastructure-as-Code/", "categories": "DevOps", "tags": "Deployment", "date": "2021-11-25 11:12:27 +0800", "snippet": "IaCInfrastructure as Code (IaC) automates the provisioning of infrastructure, enabling your organization to develop, deploy, and scale cloud applications with greater speed, less risk, and reduced cost. When you have identified everything you need, remember: Use version control; everything should be tracked using version control. Code everything; nothing should be done manually. Use code to describe the desired state. Idempotence. Every result from the code you write should always yield the same result, no matter how many times it is executed. Make your code modular. Test, test, test Again: Use version control. Don’t ever forget this.参考：基础设施即代码（IaC）：方法论、办法和优秀实践Infrastructure as CodeHow to use infrastructure as codeWhat is Infrastructure as Code (IaC)?" }, { "title": "Deployment Strategies", "url": "/posts/Depolyments/", "categories": "DevOps", "tags": "Deployment", "date": "2021-11-25 11:12:27 +0800", "snippet": "Rolling DeploymentA rolling deployment is a software release strategy that staggers deployment across multiple phases, which usually include one or more servers performing one or more function within a server cluster. Rather than updating all servers or tiers simultaneously, the organization installs the updated software package on one server or subset of servers at a time. A rolling deployment is used to reduce application downtime and unforeseen consequences or errors in software updates.refer to Rolling DeploymentBlue/Green DeploymentA blue/green deployment is a change management strategy for releasing software code. Blue/green deployments, which may also be referred to as A/B deployments require two identical hardware environments that are configured exactly the same way. While one environment is active and serving end users, the other environment remains idle.Blue/green deployments need two identical sets of hardware, and that hardware carries added costs and overhead without actually adding capacity or improving utilization. Organizations that cannot afford to duplicate hardware configurations may use other strategies such as canary testing or rolling deployments. A canary test deploys new code to a small group of users, while a rolling deployment staggers the rollout of new code across servers.refer to Blue/Green DeploymentCanary DeploymentIn software engineering, canary deployment is the practice of making staged releases. We roll out a software update to a small part of the users first, so they may test it and provide feedback. Once the change is accepted, the update is rolled out to the rest of the users.Canary deployments show us how users interact with application changes in the real world. As in blue-green deployments, the canary strategy offers no-downtime upgrades and easy rollbacks. Unlike blue-green, canary deployments are smoother, and failures have limited impact.refer to Canary Deployment" }, { "title": "Todolist", "url": "/posts/ToDoList/", "categories": "", "tags": "", "date": "2021-11-25 00:00:00 +0800", "snippet": " mermaid kafka event brokener bo Baseline testing service-level agreement (SLA). Stress Testing Load QPSQPS守护进程ACID 规则图片服务- redis订单转换 DbContext migration modelvalidate UML 数据图表工具 hasone withmany readthedocs + github 自动部署开发文档 RSA key RBAC database schema OIDC provider openid connention AP auth provider Options pattern in ASP.NET Core token introspection (内省)api waregatemessage bus 消息总线event bus 事件总线service bus 服务总线message Bus vs message queue后者FIFO，前者不要求敏捷开发微服务之间的通信API Gateway 负责负载均衡、缓存、访问控制、API 计量和监控等任务，并且可以使用 NGINX 有效地实现。该系列的后续文章将介绍API 网关API 网关为每种客户端提供特定的 API。开发人员必须更新 API 网关才能公开每个微服务的端点。更新 API 网关的过程尽可能轻量级很重要服务调用基于微服务的应用程序是一个分布式系统，必须使用进程间通信机制。进程间通信有两种风格。一种选择是使用基于消息传递的异步机制。一些实现使用消息代理，例如 JMS 或 AMQP。其他的，例如 Zeromq，是无代理的，服务直接通信。另一种进程间通信方式是同步机制，例如 HTTP 或 Thrift。系统通常会同时使用异步和同步样式。它甚至可能使用每种样式的多个实现。因此，API 网关需要支持多种通信机制。API 网关与系统中的任何其他服务客户端一样，需要使用系统的服务发现机制：服务器端发现或客户端发现。space-cloud serverless 架构，利用 space-cloud 提供的GraphQL 和 REST APIs分布式锁Zookeeper 统一配置：服务监听 如果服务端改了，其他节点也能跟着同步 统一命名：节点存储多个IP地址 分布式锁：节点创建序号，最小的拿锁，其余监听-1的变化 集群管理负载均衡策略nginx upstream 1. round-robin 2. least-connect 3. ip-hashRaft算法perfmonPhantom Read 幻读分库大数据存储ThriftgRPCOcelot什么是grpcredirect 本质 HTTP Strict Transport Security Protocol (HSTS)The client must support HSTS.UDP 使用尽最大努力交付，即不保证可靠交付ftp协议KONG 网关node 非阻塞I/O交付保证如何熔断之后如何恢复microservice email with queueredis 更新策略https://yunpengn.github.io/blog/2019/05/04/consistent-redis-sql/https://www.manning.com/ 电子书网站https://opensourcelibs.com/ 开源库搜索引擎" }, { "title": "Efcore Relations", "url": "/posts/EFCore-Relations/", "categories": "", "tags": "", "date": "2021-05-24 00:00:00 +0800", "snippet": "title: “EFCore Relations”date: 2021-12-21 15:14:44 +0800categories: [AspnetCore]tags: [efcore]—约定默认情况下，当在某个类型上发现导航属性时，将创建一个关系。 如果当前数据库提供程序无法将其指向的类型映射为标量类型，则该属性被视为导航属性。单个导航属性如果只有一个导航属性，则 WithOne 和 WithMany 会发生无参数重载。 这表示在概念上，关系的另一端有一个引用或集合，但实体类中不包含导航属性。 modelBuilder.Entity&amp;lt;Blog&amp;gt;() .HasMany(b =&amp;gt; b.Posts) .WithOne();导航属性： 在主体和/或从属实体上定义的属性，该属性引用相关实体。集合导航属性： 一个导航属性，其中包含对多个相关实体的引用。引用导航属性： 保存对单个相关实体的引用的导航属性。反向导航属性： 讨论特定导航属性时，此术语是指关系另一端的导航属性。 By convention, if any property has name Id or Id, then it is by default considered as primary key for that EF Core model如果实体有属性命名Id，默认视为主键 ICollection vs IListIList 是 ICollection 的实现，带索引 IEnumerable is read-only You can add and remove items to an ICollection You can do random access (by index) to a List 只允许一个FromBodyDon’t apply [FromBody] to more than one parameter per action method. Once the request stream is read by an input formatter, it’s no longer available to be read again for binding other [FromBody] parameters efcore 导航属性加载 渴望加载 Include/ThenInclude 常用，钻取，EF 会在生成 SQL 时合并连接 显示加载 Single/Collection/Reference 延迟加载 延迟策略 UseLazyLoadingProxies virtual 避免插入导航属性 It seems like your navigation properties have values, please check your navigation property have null reference before to save; EF Core save logic try to save navigation properties if they have value.efcore 插入实体时导航属性也会自动插入，可以通过赋null避免 efcore 无法scaffold view 和 sp Stored procedure and view mapping is not currently supported in ef core scaffolding. You can track open issues respectively at 245 and 827.scaffold -t view_name 即可 oracle fetch first 10 rows only 错误 That syntax isn’t valid until Oracle Database 12c.SELECT * FROM v$version;Oracle Database 11g Release 11.2.0.4.0 - 64bit Production解决方法：x.UseOracleSQLCompatibility(&quot;11&quot;);Loading Related Data[ICollection Vs List in Entity Framework](https://rotadev.com/icollectiont-vs-listt-in-entity-framework-dev/)EFCore 关系" }, { "title": "Asp.net 脚手架生成mvc", "url": "/posts/asp.net-%E8%84%9A%E6%89%8B%E6%9E%B6%E7%94%9F%E6%88%90mvc/", "categories": "", "tags": "", "date": "2021-04-25 00:00:00 +0800", "snippet": "title: “asp.net 脚手架生成mvc”date: 2021-12-21 15:14:05 +0800categories: [AspnetCore]tags: [脚手架]—Tutorial with vs code新建MVC项目dotnet new mvc -auth inditidual -o [projectname]配置数据库数据库使用sql server，添加依赖包dotnet add package Microsoft.EntityFrameworkCore.SqlServer新增appsetting.json属性{ &quot;ConnectionStrings&quot;: { &quot;DefaultConnection&quot;: &quot;Server=(localdb)\\\\mssqllocaldb;Database=Grocery;Trusted_Connection=True;MultipleActiveResultSets=true&quot; }, ...}修改StartUp.cs 文件services.AddDbContext&amp;lt;ApplicationDbContext&amp;gt;(options =&amp;gt; options.UseSqlServer( Configuration.GetConnectionString(&quot;DefaultConnection&quot;)));配置IdentityIdentity默认配置如下，具体可参考Introduction to Identity on ASP.NET Coreservices.AddDefaultIdentity&amp;lt;IdentityUser&amp;gt;(options =&amp;gt; options.SignIn.RequireConfirmedAccount = true) .AddEntityFrameworkStores&amp;lt;ApplicationDbContext&amp;gt;();Identity脚手架,生成auth相关的源文件，razor或者mvc dotnet tool install -g dotnet-aspnet-codegenerator dotnet add package Microsoft.VisualStudio.Web.CodeGeneration.Design dotnet add package Microsoft.EntityFrameworkCore.Design dotnet add package Microsoft.AspNetCore.Identity.EntityFrameworkCore dotnet add package Microsoft.AspNetCore.Identity.UI dotnet add package Microsoft.EntityFrameworkCore.Tools dotnet add package Microsoft.EntityFrameworkCore.SqlServer脚手架 dotnet aspnet-codegenerator identity -dc WebApp1.Data.ApplicationDbContext --files &quot;Account.Register;Account.Login;Account.Logout;Account.RegisterConfirmation&quot;可参考Scaffold Identity in ASP.NET Core projects dotnet ef dbcontext scaffold “Data Source=E40UTF8A;User Id=ST1CNA;Password=ST1CNA;” Oracle.EntityFrameworkCore -t XF_STAFF -f –use-database-names –context-dir Data –output-dir Models -c temp 更新数据库因为默认sqlite，要移除initial migration file，重新生成 dotnet ef migrations removedotnet ef migrations add initialIdentitySchemadotnet ef databse update在Models 文件夹下新增Model, 如Product.cspublic class Product{ public Guid Id { get; set; } public string Name { get; set; }}在DbContext文件中重载 OnModelCreating 方法,fluent-api 构建模型base.OnModelCreating(builder);builder.Entity&amp;lt;Product&amp;gt;(e =&amp;gt;{ e.Property(x =&amp;gt; x.Name).HasMaxLength(200).IsRequired(); e.HasKey(x =&amp;gt; x.Id);});脚手架添加Controller dotnet aspnet-codegenerator controller -name ProductController -async -m Grocery.Models.Product -dc Grocery.Data.ApplicationDbContext -namespace Controllers -outDir Controllers -udl数据模型同步数据库 dotnet ef migrations add Added_Productdotnet ef databse update引用using Microsoft.AspNetCore.Authorization;,在Controller上添加注解[Authorize]运行 dotnet run参考ASP.NET Core Authentication TutorialAuthentication &amp;amp; Authorization in ASP .NET Core 3.1dotnet newLogin with ASP Identity fails every time with “Not Allowed” (even when ‘email’ and ‘username’ have the same value))" }, { "title": "读写分离 CQRS Pattern", "url": "/posts/CQRS%E6%A8%A1%E5%BC%8F/", "categories": "系统框架", "tags": "模式", "date": "2021-03-22 11:12:27 +0800", "snippet": "What is CQRS It states that every method should either be a command that performs an action, or a query that returns data to the caller, but not both.In other words, asking a question should not change the answer. More formally, methods should return a value only if they are referentiallytransparent and hence possess no side effects.Essentially, the Mediator pattern is well suited for CQRS implementationAdvantages of CQRS Highly Scalable Having control over the models in accordance with the type of data operations makes your application highly scalable in the long run. Improved Performance Practically speaking there are always 10 times more Read Operations as compared to the Write Operation. With this pattern you could speed up the performance on your read operations by introducing a cache or NOSQL Db like Redis or Mongo. CQRS pattern will support this usage out of the box, you would not have to break your head trying to implement such a cache mechanism. Secure Parallel Operations Since we have dedicated models per oprtation, there is no possibility of data loss while doing parellel operations. In his CQRS article Martin Fowler warns: “For some situations, this separation can be valuable, but beware that for most systems CQRS adds risky complexity.”对于某些场景，读写分离是值得的，但需要注意，对于大部分系统而言，读写分离增加了崩溃的复杂度。CQRS with MediatR in ASP.NET Core 3.1 – Ultimate Guide" }, { "title": "RabbitMQ", "url": "/posts/RabbitMQ/", "categories": "消息队列", "tags": "queue", "date": "2021-03-22 11:12:27 +0800", "snippet": "安装 通过 Chocolatey 安装（推荐） 通过 RabbitMQ 安装包安装，要求预装Erlang命令使用安装目录下sbin文件夹的rabbitmqctl.bat,也可在开始菜单中找到rabbitmqctl.bat statusrabbitmqctl.bat startrabbitmqctl.bat stopAMQP协议官网 上对协议做出了非常详尽的解释，非常重要！ 消息发布到交易所，通常与邮局或邮箱进行比较。然后，交易所使用称为绑定的规则将 消息副本分发到队列中。然后，代理要么将消息传递给订阅了队列的使用者，要么使用者按需从队列中获取/拉取消息管理登陆后台，端口默认15672，登陆名guest，密码guest概念 exchange 理解为交换机 connection 理解为TCP连接，需要释放的资源 queues 队列，虚拟通道，存储方式分为持久和短暂的，持久的写入内存，主机重启时挥恢复 binding( exchange types) 分4种类别 direct fanout topic headers router-key 路由键 Publishers 发布者 Customers 消费者 vhost 虚拟主机，包含了节点，交换机，队列等 Exchanges are named routing tablesTable entries are called bindingsBindingKey VS routingKeyexchange(交换器)bingdingkey表示exchange与queue的关系routingkey表示message告诉exchange根据exchange type的规则和routingkey路由到哪些queueexchange 通过比较bindingkey和routingkey来路由消息（大前提是 exchange type，如果是fanout，应路由到所有的queue）默认情况下，绑定exchange和queue，routingkey = queuename = bindingkeyExchange 类型direct（直接）: client 可以通过 exchange(type) + routingkey（与bindingkey比较） 消息路由topic（主题）: routingkey 带通配符 * （一个） 或 # （没有或多个）fanout(扇出): 无需设置routingkey, 路由到所有queueheader(头部)：bingding 时通过 arguments （字典类型，其中key = x-match value=all(所有key-value配对)/any（任一key-value配对））配置，publish 时赋值basicProperties.Headers 实现channel信道channel(信道): 因为建立和销毁Tcp连接开销大，所以rabbitmq 通过线程管理信道（信道具有唯一ID，已NIO非阻塞IO方式进行Tcp连接复用，channel-Buffer（缓存区）-Selector（选择器，监听信道））.NET/C# Client API GuideIModel: represents an AMQP 0-9-1 channel, and provides most of the operations (protocol methods)IConnection: represents an AMQP 0-9-1 connectionConnectionFactory: constructs IConnection instancesIBasicConsumer: represents a message consumerCluster集群Building RabbitMQ Clusterrabbitmqctl join_cluster rabbit@rabbiterlang_cookie 要一致RabbitMQ / AMQP：单个队列，同一消息的多个消费者重要的是要了解，在 AMQP 0-9-1 中，消息在消费者之间是负载平衡的。单个队列/多个消费者，每个消费者处理相同的消息 ID 是不可能的。让交换将消息路由到两个单独的队列中确实更好。参考Rabbit官网CAPKafka vs RabbitMQ: What Are the Biggest Differences and Which Should You Learn?Part 4: RabbitMQ Exchanges, routing keys and bindings" }, { "title": "五大设计原则SOLID", "url": "/posts/%E4%BA%94%E5%A4%A7%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99SOLID/", "categories": "系统框架", "tags": "原则", "date": "2021-03-22 11:12:27 +0800", "snippet": "1. 单一职能原则 Single Responsibility SRPa class should only have one responsibility. Furthermore, it should only have one reason to change2. 开放关闭原则 Open for Extension, Closed for Modification OCPclasses should be open for extension, but closed for modification. In doing so, we stop ourselves from modifying existing code and causing potential new bugsOf course, the one exception to the rule is when fixing bugs in existing code.3. 里氏替换原则 Liskov Substitution LSPif class A is a subtype of class B, then we should be able to replace B with A without disrupting the behavior of our program.4. 接口隔离原则 Interface Segregation ISPlarger interfaces should be split into smaller ones. By doing so, we can ensure that implementing classes only need to be concerned about the methods that are of interest to them.5. 依赖倒置原则 Dependency Inversion DIPThe principle of Dependency Inversion refers to the decoupling of software modules. This way, instead of high-level modules depending on low-level modules, both will depend on abstractions.参考Introduction To SOLID PrinciplesA Solid Guide to SOLID Principles" }, { "title": "常用设计模式", "url": "/posts/%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/", "categories": "系统框架", "tags": "模式", "date": "2021-03-22 11:12:27 +0800", "snippet": "工厂方法不同的工厂继承于抽象，调用同一个生成方法，生成不同的产品，此处的产品是单一的，不同于抽象工厂抽象工厂并行层次结构同一个Client，引用不同的工厂，调用同一个生成方法，内部由不同的工厂生成不同的产品组合可以通过新增工厂类来实现新增新的产品组合单例模式关键代码if (_instance == null){ lock (syncLock) { if (_instance == null) { _instance = new LoadBalancer(); } }}适配器模式适配器继承于目标，并重写方法，其中重新组合了其他不同类或框架的方法，不同的适配器可以返回同一类型的目标对象，目标执行方法，执行了不同的组合方法Client端，实例不同的适配器返回同一类型对象，本质上，内部执行不同的对象方法（重写）合成模式允许创建树结构，无论是节点还是分支，都可以被访问无论节点或分支，都继承于组件,根（第一分支）有一个列表，可以存储节点或分支，它们都是组件，然后形成树结构 Client端，实例节点，加入节点或分支，形成树结构外观模式为多个子系统的一组接口提供统一的接口，提供更高级别的接口，使子系统易于使用 Client端，实例高级系统，调用接口方法，本质上，调用了子系统的多个接口组合代理模式代理类和抽象目标之间有相同的行为，但内部实例了真实目标，代理类的行为，其实就是真实目标的行为 Client端，实例代理，调用代理行为，本质上，是调用了真实目标的行为命令模式三个角色：执行者、命令、调用者该模式将请求存储为对象，从而允许客户端执行或回放请求。 Client端，实例命令，引用执行者，实例调用者，引用命令，调用者执行命令，本质上，调用了执行者执行了命令，调用者多次执行，相当于回放请求迭代模式目标内置底层集合，生成迭代器的同时，将集合传入，这样可以隐藏底层集合的结构，又可以操作集合 Client端，实例目标（内部实例了底层集合），添加子项，生成迭代器，执行迭代器行为，如next，first，count 等等观察者模式被观察者添加观察者，执行某项动作时，主动通知观察者 Client端，类似winform 的事件订阅策略模式该模式以对象的形式封装了功能，这允许客户端动态更改算法策略 Client端，实例上下文，如将不同的算法实例传入，可实现动态更改算法仓储和工作单元模式好处： 减少重复代码 业务和操作数据分离 可测试，适合TDD开发问题 工厂方法和抽象工厂之间的区别因为factory方法只是一个方法，所以可以在子类中覆盖它，因此引号的后半部分： …工厂方法模式使用继承，并依赖于子类来处理所需的对象实例化。引用假定此处对象正在调用其自己的工厂方法。因此，唯一可以改变返回值的是子类。抽象工厂是一个具有多种工厂方法的对象。 …使用Abstract Factory模式，一个类通过组合将对象实例化的责任委托给另一个对象…参考仓储(Repository)和工作单元模式(UnitOfWork)Implementing the Repository and Unit of Work Patterns in an ASP.NET MVC Application (9 of 10)ASP.NET Core中的存储库模式–最终指南Repository Pattern C#仓储模式的4个常见错误Repository Pattern (C#)" }, { "title": "jekyll博客网站安装部署", "url": "/posts/jekyll%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/", "categories": "其他", "tags": "博客", "date": "2021-03-16 11:12:27 +0800", "snippet": "介绍jekyll无需再安装数据库,将纯文本转换为静态网站和博客安装按要求在不同的系统安装Ruby、RubyGems、GCC和makewindows系统上最简单的方式是安装RubyInstaller,安装的最后默认勾选ridk install安装完成后以管理员身份运行cmd,输入gem install jekyll bundler安装Jekyll和Bundler,输入jekyll -v检查是否安装成功新建网站jekyll new my-awesome-site比较耗时，等待几分钟，因为要从其他服务器下载文件如出现以下错误，因为Ruby 3.0不再绑定webrick cannot load such file – webrick (LoadError)添加 bundle add webrick即可主题minimal-mistakes官网 有丰富的主题模板，比如minimal-mistakes gem “minimal-mistakes-jekyll”(修改gemfile) bundle(更新绑定) theme: minimal-mistakes-jekyll(修改网站目录下_config.yml)可以下载minimal-mistakes源码，_layout文件夹（渲染模板）和_page文件夹（参数式替换）chirpygithub源码教程 fork 项目 chirpy starter 修改文件 _config.yml url avatar timezone lang 配合 github action 自动部署 修改代码风格复制源码文件夹addon和colors到项目，比如修改github.css 成 github.scss@mixin light-syntax { .... /* --- custom light colors --- */ --highlight-bg-color: #f7f7f7; --highlighter-rouge-color: #2f2f2f; --highlight-lineno-color: #c2c6cc; --inline-code-bg: #f3f3f3; --code-header-bg: #eaeaea; --lang-badge-color: rgb(128 128 128 / 87%); --lang-badge-muted-color: rgb(128 128 128 / 36%); --clipboard-checked-color: #43c743;}支持的语言缩写 这里将addon/syntax.scss文件中的@import &quot;colors/light-syntax&quot; 替换为@import &quot;colors/github&quot;;部署jekyll build 在_sites中生成静态网站，配合nginx挂起，参考Using Jekyll and Nginx&amp;lt;节点 IP&amp;gt;:&amp;lt;节点端口&amp;gt; 会被识别为tag，应改为`&amp;lt;节点 IP&amp;gt;:&amp;lt;节点端口&amp;gt;`" }, { "title": "EF VS Dapper", "url": "/posts/EF-VS-Dapper/", "categories": "ORM框架", "tags": "ef, dapper", "date": "2021-03-15 11:12:27 +0800", "snippet": "EF优势 Code First(Fluent API) 和 EF Designeel） 生成r（Mod数据库 track changes （可追踪变化） 和 SaveChanged 方便保存数据 LINQ(IQuerable) linq语句转sql 反向工程（生成代码和模型） 减少开发时间和成本劣势 无法控制查询 延迟加载Dapper优势 轻量级 参数化查询 通过扩展，可以实现simpleCRUD劣势 类模式 无法跟踪变化aspnetcore with dapper" }, { "title": "依赖注入DI", "url": "/posts/DI-DIP-IOC-IOC-Container/", "categories": "系统框架", "tags": "di, dip, ioc", "date": "2021-03-15 11:12:27 +0800", "snippet": "为了实现松散耦合设计，了解DI、DIP、IOC以及IOC Container。IOC 反转控制原则控制包括对应用程序流的控制，以及对对象创建或从属对象创建和绑定的流的控制,使用Factory模式实现IOC。public class CustomerBusinessLogic{ public CustomerBusinessLogic() { } public string GetCustomerName(int id) { DataAccess _dataAccess = DataAccessFactory.GetDataAccessObj(); return _dataAccess.GetCustomerName(id); }}我们将创建依赖类的对象的控制从一个CustomerBusinessLogic类转换到另一个DataAccessFactory类。DIP 依赖注入原则 高级模块不应依赖于低级模块。两者都应取决于抽象。 抽象不应依赖细节。细节应取决于抽象。public interface ICustomerDataAccess{ string GetCustomerName(int id);}public class CustomerDataAccess: ICustomerDataAccess{ public CustomerDataAccess() { } public string GetCustomerName(int id) { return &quot;Dummy Customer Name&quot;; }}public class DataAccessFactory{ public static ICustomerDataAccess GetCustomerDataAccessObj() { return new CustomerDataAccess(); }}public class CustomerBusinessLogic{ ICustomerDataAccess _custDataAccess; public CustomerBusinessLogic() { _custDataAccess = DataAccessFactory.GetCustomerDataAccessObj(); } public string GetCustomerName(int id) { return _custDataAccess.GetCustomerName(id); }}DI 设计模式依赖注入模式涉及3种类型的类： 客户端类：客户端类（从属类）是依赖于服务类的类。 服务类：服务类（相关性）是为客户端类提供服务的类。 注入器类：注入器类将服务类对象注入到客户端类中。注入方式： 构造函数注入：在构造函数注入中，注入器通过客户端类构造函数提供服务（依赖项）。所述FromServicesAttribute使直接注入到服务的操作方法，而无需使用构造器注入：public IActionResult About([FromServices] IDateTime dateTime){ return Content( $&quot;Current server time: {dateTime.Now}&quot;);} 属性注入：在属性注入（也称为“设置器注入”）中，注入器通过客户端类的公共属性提供依赖项。 方法注入：在这种类型的注入中，客户端类实现一个接口，该接口声明提供依赖项的方法，注入器使用此接口向客户端类提供依赖项。 IOC Container 框架IoC容器（又名DI容器）是用于实现自动依赖项注入的框架。它管理对象的创建及其生命周期，还向类注入依赖项。 Register Resolve(即使未注册，也可在解析时覆盖已注册的类型) Dispose问题生命周期 Transient(短暂的) Scoped（ AddDbContext） Singleton（thread safe and are often used in stateless services）三者之间的区别 Transient objects are always different; a new instance is provided to every controller and every service.在一次请求或调用中，注入的对象在被注入的对象中都是新的 Scoped objects are the same within a request, but different across different requests.在同一次请求中，使用同一个注入的对象 Singleton objects are the same for every object and every request.每次请求或调用都是同一个实例对象关于解析 从范围、短暂中解析单例 从另一个短暂/范围中解析范围实例解析服务 IServiceProvider 注入 ActivatorUtilities （激活）构造函数注入需要一个公共构造函数,但只能存在一个重载，其所有参数都可以通过依赖项注入来实现。参考Martin Fowler:Inversion of Control Containers and the Dependency Injection patternInversion of Control Tutorials" } ]
